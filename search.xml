<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[决策表的知识约简]]></title>
    <url>%2F2019%2F02%2F28%2Fknowledge-reduction-of-decision-table%2F</url>
    <content type="text"><![CDATA[前面讨论了粗糙集理论的基础知识，这一部分将讨论知识表达系统的一种——决策表的知识约简算法。 决策表的基本概念决策表的定义决策表是知识表达系统的一种，关于知识表达系统的定义可以参考上篇blog，这里不再赘述，主要理解一下决策表的不同的特点： $A = C \cup D$，其中C称为条件属性集，每个C中的元素称为C的一个简单属性；D称为决策属性集，且有$C \cap D = \varnothing \, C \ne \varnothing \, D \ne \varnothing$。 当$IND(C) \subseteq IND(D)$时，我们称决策表是相容的，因为不存在条件属性一致但是决策属性不同的样本。 决策规则给定一个决策表$DT = (U,C \cup D,V,f)$，令$X \in U/IND(C) , Y \in U/IND(D) , \forall x \in X , des(X) = \bigcap_{\forall \alpha \in C}(\alpha,\alpha(x))$表示等价类X的描述；$\forall y \in Y , des(Y) = \bigcap_{\forall \beta \in D}(\beta,\beta(Y))$表示等价类Y的描述，则定义： r : des(X) \to des(Y) , X \cap Y \ne \varnothing为从X到Y的决策规则。因为$IND(C) \subseteq IND(D)$，所以我认为上述公式应该满足$X \subseteq Y$。 决策表的属性约简算法盲目删除属性约简算法盲目删除算法的基本思路是：首先任意选择一个条件属性$\alpha_{i} \in C$，如果从决策表中删除该条件属性$\alpha_{i}$使得相对于D的正域没有改变，则说明属性$\alpha_{i}$是C中相对于决策D不必要的，从决策表中删除该属性以及其对应的属性值，然后继续重复上述步骤，直至不能删除其中的任一个元素为止，这时当前的条件属性集就是决策表的一个相对于D的属性约简。 对应的算法流程如下： 输入：$DT = (U,C\cup D,V,f)$ 输出：DT的一个相对约简B，$B \in RED_{D}(C)$ (1) 令B = C (2) $\forall \alpha \in C$，若$\alpha \in B$，则令$Mark(\alpha) = 0$，否则令$Mark(\alpha) = 1$ (3) 任选一个$\alpha \in B$，且$Mark(\alpha) = 0$，令$Mark(\alpha) = 1$，如果有 pos_{IND(B - \{\alpha\})}(D) = pos_{IND(B)}(D)则从B中删除条件属性&alpha;，即$B \Leftarrow B-\{\alpha\}$，转到(2)，否则，转到(4) (4) 若$ \exists \alpha \in B$ 且$ Mark(\alpha) = 0$，转到(3)，否则输出$B \in RED_{D}(C)$，算法结束。 盲目算法虽然可以得到一个相对约简，但是不一定能够得到一个满意的相对约简，即约简结果存在很大的随机性，当然也可以采用搜索策略得到所有可能的约简结果，然后验证是否属于$RED_{D}(C)$，但是搜索往往是一个组合爆炸问题，代价很高，效果并不理想。 基于Pawlak属性重要度的属性约简算法首先讨论一下如何“启发式”的计算条件属性的重要度：给定一个DT，$B \subseteq C , \alpha \in C$，定义 sig(\alpha,B;D) = \frac{|pos_{B\cup \alpha}(D)| - |pos_{B}(D)|}{|U|}为条件属性&alpha;对条件属性集B相对于决策属性D的重要度，因为分母是定值，所以计算的时候经常省略分母。 事实上，我们还可以“启发性”的定义各种各样的属性重要度，它在启发式属性约简算法的构造中占有重要地位，如果重要度函数定义的合理，则可以提高决策表属性约简算法的效率。 对应的算法流程如下： 输入：$DT = (U,C\cup D,V,f)$ 输出：DT的一个相对约简B，$B \in RED_{D}(C)$ (1) 计算C相对于D的核$CORE_{D}(C)$ (2) 令$B = CORE_{D}(C)$，如果$pos_{B}(D) = pos_{C}(D)$，转到(5) (3) 取$\forall c_{i} \in C-B$，计算属性重要度$sig(c_{i},B;D)$，求得$c_{m} = argmax_{c_{i} \in C-B}sig(c_{i},B;D)$，若同时存在多个属性满足最大值，则从中选取一个划分集合最少的属性作为$c_{m}$，令$B = B\cup \{c_{m}\}$ (4) 如果$pos_{B}(D) \ne pos_{C}(D)$，则转到(3)，否则转到(5) (5) 输出 $B \in RED_{D}(C)$，算法结束。 基于差别矩阵的决策表的属性约简算法差别矩阵的定义决策表的差别矩阵：设DT是一个决策表，其中论域U是对象的一个非空有限集合，$U=\{x_{1},x_{2},…,x_{n}\},|U| = n$，则定义： U_{n \times n} = (c_{ij})_{n \times n} = \begin{bmatrix} c_{11} & c_{12} & \cdots & c_{1n} \\ c_{21} & c_{22} & \cdots & c_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ c_{n1} & c_{n2} & \cdots & c_{nn} \\ \end{bmatrix} = \begin{bmatrix} c_{11} & * & \cdots & * \\ c_{21} & c_{22} & \cdots & * \\ \vdots & \vdots & \ddots & \vdots \\ c_{n1} & c_{n2} & \cdots & c_{nn} \\ \end{bmatrix} \\ = \begin{bmatrix} c_{11} & c_{12} & \cdots & c_{1n} \\ * & c_{22} & \cdots & c_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ * & * & \cdots & c_{nn} \\ \end{bmatrix}为决策表的差别矩阵，矩阵元素的取值满足如下关系： c_{ij} = \begin{cases} \{\alpha | (\alpha \in C) \wedge (f_{\alpha}(x_{i}) \ne f_{\alpha}(x_{j}))\} & & f_{D}(x_{i}) \ne f_{D}(x_{j}) \\ \varnothing & & f_{D}(x_{i}) \ne f_{D}(x_{j}) \wedge f_{C}(x_{i}) = f_{C}(x_{j})\\ - & & f_{D}(x_{i}) = f_{D}(x_{j})\\ \end{cases}其中如果决策属性D对应相等的两个对象，我们没有必要研究它们存在的差异，所以对应的差别矩阵值为“$-$”；如果存在决策属性值D不同，但是条件属性C值相同的两个对象，那么该决策表一定是不相容的；只有那些决策属性值不同的对象间的不同条件属性才是我们应该研究的重点。 利用差别矩阵计算核在一个相容决策表中，决策表的相对D核等于该决策表的差别矩阵中所有简单属性(单个属性)元素组成的集合，即： CORE_{D}(C) = \{\alpha | (\alpha \in C) \wedge ((\exists c_{ij} \in M_{n \times n}) \wedge (c_{ij} = \{ \alpha \})) \}证明：因为删除上述公式中的属性&alpha;，就无法正确分类对象$x_{i},x_{j}$，即属性&alpha;在C中是绝对必要的，这是充分条件；如果$CORE_{C}(D)$存在这之外的元素，假设为属性&beta;，删除&beta;不会导致差别矩阵的变化，即无法改变C的对象区分能力，所以&beta;是不必要的，所以不存在这样的&beta;，这是必要条件。 利用差别矩阵计算相对约简$\forall B \subseteq C$，若B满足如下两个条件：(1)$\forall c_{ij} \in M_{n \times n}$，当$ c_{ij} \ne \varnothing , c_{ij} \ne -$时，都有$B \cap c_{ij} \ne \varnothing $，(2)B是相对于D独立的。那么B是决策表的一个相对约简。 证明：由条件(1)可得，属性簇B可以区分所以的决策属性D不同的论域元素$x_{i}$，即有$pos_{IND(B)}(D) \wedge pos_{IND(C)}(D)$成立，又因为$B \subseteq C$，所以必有$pos_{IND(C)}(D) \wedge pos_{IND(B)}(D)$，由此证明得到$pos_{IND(B)}(D) = pos_{IND(C)}(D)$。同时配合条件(2)的独立性，可得B是C上相对于D的一个约简。 最后给出基于Skowron(差别矩阵的提出者)差别矩阵的决策表属性约简算法流程： 输入：$DT = (U,C\cup D,V,f)$ 输出：DT的所有相对约简B，$B \in RED_{D}(C)$ (1) 根据差别矩阵的定义，写出$M_{n \times n}$，因为差别矩阵存在对称性，所以一般写出下(上)矩阵即可 (2) 搜索差别矩阵的所有元素，若没有&varnothing; ，则转到(3)，反之则为不相容决策表，退出算法 (3) 搜索决策表差别矩阵中的所有单属性元素，求并得到$CORE_{D}(C)$ (4) 求出所有包含$CORE_{D}(C)$的可能的属性集合，判断是否满足1、$\forall c_{ij} \in M_{n \times n}(DT)$，当$c_{ij} \ne \varnothing$时，是否有$B \cap c_{ij} \ne \varnothing$，以及2、B是否是独立的。若满足上述两个条件，则令$RED_{D}(C) = RED_{D}(C) \cup \{B\}$ (5) 输出$RED_{D}(C)$，算法结束。 该算法还有一个变种，即基于Skowron差别矩阵和属性选择的决策属性约简算法 输入：$DT = (U,C\cup D,V,f)$ 输出：DT的一个相对约简B，$B \in RED_{D}(C)$ (1) 求差别矩阵$M_{n \times n} = (c_{ij})_{n \times n}$ (2) 计算决策表的相对核$CORE_{D}(C)$，令$B = CORE_{D}(C)$ (3) 对任意的$c_{ij}$，如果$c_{ij} \cup B \ne \varnothing$，则 $c_{ij} = \varnothing$ (4) 对任意的$c_{ij}$，如果都有$c_{ij} = \varnothing$，额转到(6)，否则转到(5) (5) 统计当前差别矩阵中每个属性出现的次数，选取出现次数最多的元素为$\alpha_{m}$，令$B = B \cup \{\alpha_{m}\}$，转到(3) (6) 输出 $B \in RED_{D}(C)$，算法结束。 该变种算法属于一种启发式的思路，即“自下而上”的构造集合B，并在每步中选取最重要的——最有可能属于B的元素加入B，并验证新的集合B是否满足相对约简的条件。 决策表的值约简算法给定一个决策表$DT = (U,C\cup D,V,f)$，经属性约简后得到一个相对约简的关系数据表$T(B) = (U_{B},B\cup D,V,f)$，将表T(B)中的每一个样本视为一条决策规则，这样T(B)就转换为一个决策规则集，在此基础上进行属性值的约简，得到抽象程度更高的决策规则。 下面介绍一个最简单的属性值约简算法，即决策表的盲目删除值约简算法： 输入： $T(B) = (U_{B},B\cup D,V,f)$ 输出： 规则集T(B)关于决策属性D的完全简化的一组规则集(值约简)$R \in \widehat{RED_{D}(C)}$ (1) 设 r_{i}:\bigwedge_{\forall \alpha \in B} (\alpha_{j},\upsilon_{ij}) \to \bigwedge_{\forall \beta \in D}(\beta_{k},\omega_{k}) \\ i = 1,2,\cdots,|U_{B}|; \\ 1 \le j \le |B|; \\ 1 \le k \le |D|; \\ R = \varnothing(2) 对于规则集合${r_{i}| 1 \le i \le |U_{B}|}$中的每条规则$r_{i}$，如果从$r_{i}$中删除$(\alpha_{j},\upsilon_{ij})$，其余的规则也同时删除相同的列，及条件属性$\alpha_{j}$所在的列后，不会产生与$r_{i}$不相容的规则，则称属性$\alpha_{j}$下的值$\upsilon_{ij}$是不必要的，反之则为必要的，不能删除。接着上面的结果，继续删除决策规则$r_{i}$中不同属性下的其他值，直至获得规则$r_{i}$的一个完全约简规则$r_{i}^{}$为止，然后令$R \Leftarrow R \cup \{r_{i}^{}\}$，用上述的方法遍历所有的决策规则 (3) 输出$R \in \widehat{RED_{D}(C)}$ 这种方法得到的结果存在很大的随机性，并不能保证得到的是一个满意的约简。通常需要一些启发式的知识来指导这一过程，比如归纳值约简算法、基于决策矩阵的决策表值约简算法、属性值增量约简算法等。]]></content>
      <tags>
        <tag>rough set theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[粗糙集理论]]></title>
    <url>%2F2019%2F02%2F28%2Frough-set-theory%2F</url>
    <content type="text"><![CDATA[这一部分我们讨论一下粗糙集理论中的基本概念，为后面讨论的知识约简算法做铺垫。 知识与分类粗糙集理论认为，知识直接与真实或抽象世界的不同分类模式联系在一起，即任何客观事物都可以根据知识（事物的不同属性或特征）进行分类，所以我们说知识是具有颗粒性的。 关于知识有如下的定义： 1、论域U的任何一个子集X&sube;U称为论域U的一个概念或者范畴（也叫信息粒）； 2、论域U中任何子集簇（概念簇）称为关于U的（抽象）知识。 粗糙集理论中我们主要讨论那些能够在论域U上形成划分和覆盖的知识，既然划分和等价关系是等价的，所以我们通常用等价关系来表示分类或者知识。 所以，我们讨论的数据集的知识，就是将数据集完成划分的等价关系，而后面讨论的知识约简，也就是对等价关系进行约简,^_^。 知识库给定一个论域U和U上的一簇等价关系S，称二元组K = (U,S)是关于论域U的一个知识库。 不可辨关系给定一个知识库K = (U,S)，若P&sube;S，且P &ne;&empty;，则&and;P（P中所有等价关系的交集）仍然是论域U上的一个等价关系，称为P上的不可辨关系，记为IND(P)，也常简记为P： \forall x \in U,\left [ x \right ]_{p} = \bigcap_{\forall R \in P}^{ } \left [ x \right ]_{R}U/IND(P)便是与等价关系IND(P)相关的知识，IND(P)称为知识库K中关于论域U的P-基本知识，IND(P)的所以等价类也成为基本概念(范畴)。因为可以看到IND(P)是P中所有等价关系的交集，所以可以认为是由P产生的”比P中任一关系更强“的等价关系，对U的划分理论上应该更细。 特殊的，如果Q&in;S，则称Q是关于论域U的Q-初等知识，Q的等价类为知识S的Q初等概念(范畴)。 粗糙集的基本定义集合的下近似和上近似给定知识库K = (U,S)，则对于X&sube;U和论域上的一个等价关系R，定义： X关于R的下近似： \underline{R}(X) = \left \{ x | (\forall x \in U) \in ([x]_{R} \subseteq X) \right \}X关于R的上近似： \overline{R}(X) = \left \{ x | (\forall x \in U) \in ([x]_{R} \in X \neq \varnothing ) \right \}如果X的下近似和上近似相等，那么称X是关于论域U的相对于知识R的R-精确集或R-可定义集，反之则为不可定义集，或者粗糙集。 X的R正域：（latex编辑公式真香^_^） pos_{R} = \underline{R}(X)粗糙集合论的成员关系和经典集合论的“非此即彼”不同，粗糙集合论的成员关系可以表示为：根据知识R，若$x \in \underline{R}(X)$，则x肯定属于集合X；若$x \in \overline{R}(X)$，则x可能属于集合X；若$x \notin \overline{R}(X)$，则x肯定不属于集合X。 知识约简知识的约简与核知识约简有两个基本概念，约简(reduction)和核(core)。由于涉及到知识的独立性，所以我们先介绍知识独立性的定义： 给定一个知识库K = (U,S)，和知识库中的一个等价关系簇$P \subseteq S\,\forall R \in P$，若： IND(P)=IND(P - \left \{R \right \})成立，则称知识R为P不必要的，反之则为必要的。如果对每个$R \in P$，R都为P中必要的，则称P为独立的，反之P为依赖的或者不独立的。且有定理，如果知识P是独立的，$\forall G \subseteq P$，则G也一定是独立的。 知识的约简给定一个知识库K = (U,S)，和知识库上的一簇等价关系$P \subseteq S$，对任意的$G \subseteq P$，若G满足如下要求： (1)G是独立的； (2)IND(G) = IND(P)。 则称G是P的一个简约，记为$G \in RED(P)$，RED(P)表示P的全体简约的集合。这里再回顾一下关系的定义，等价关系实质是对应一种对论域U的划分，所以上述(2)的相等，指的便是得到相同的划分。 知识的核给定一个知识库K =(U,S)和知识库上的一簇等价关系，$P \subseteq S$，对$\forall R \in P$，如果R满足： IND(P-\left\{R\right\}) \ne IND(P)则称R为P中必要的，P中所有的必要的知识组成的集合称为P的核，记为CORE(P)。且有$CORE(P) = \cap RED(P)$。 知识的相对核和相对约简知识的相对约简指的是在不影响一个属性（条件属性）对另一个属性（决策属性）的分类能力的情况下，对条件属性的缩减。 首先给出知识Q相对于知识P的正域的概念： pos_{P}(Q) = \bigcup_{X \in U/Q} \underline{P}(X)实质上，它是论域U中所有根据分类U/P的信息可以准确划分到关系Q的等价类中去的对象集合。 接着给出相对必要性和相对独立的定义： 给定一个知识库K = (U,S)和两个等价关系簇$P\,Q \subseteq S\,\forall R \in P$，若 pos_{IND(P)}(IND(Q)) = pos_{IND(P-\left\{R\right\})}(Q)成立，则称知识R为P中(Q不必要)(连读)的，反之则为P中(Q必要)的。简便起见，一般用$pos_{P}(Q)$代替$pos_{IND(P)}(IND(Q))$。 如果对于每个$R \in P$，R都为P中Q必要的，则称P为Q独立的，反之则是Q依赖的或者Q不独立的。 知识的相对约简给定一个知识库k=(U,S)和知识库上的两个等价关系簇$P,Q \subseteq S$，对任意的$G \subseteq P$，若G满足以下两条： (1)G是Q独立的，即G是P的Q独立子簇； (2)$pos_{G}(Q) = pos_{P}(Q)$ 则称G是P的一个Q约简，记为$G \in RED_{Q}(P)$，其中$RED_{Q}(P)$表示所有的P的Q约简的集合。 知识的相对核给定一个知识库K = (U,S)和知识库上的两个等价关系簇$P\,Q\subseteq S$，对$\forall R \in P$，若R满足： pos_{IND(P-\left\{R\right\})}(IND(Q)) \ne pos_{IND(P)}(IND(Q))则称R为P中的Q必要的，P中所有的Q必要的知识组成的集合称为P的Q核，或P的相对于Q的核，记为$CORE_{Q}(P)$。且有$CORE_{Q}(P) = \cap RED_{Q}(P)$。如果P=Q，则相对核和相对约简的知识(这个知识不是那个知识QAQ)就退化到上一部分的内容了。 粗糙集理论中的知识表示称四元组KRS = (U,A,V,f)是一个知识表达系统，其中， U：论域 A：属性的非空有限集合，可记为$A_{t}$ V：全体属性的值域，$V = \bigcup_{\alpha \in A} V_{\alpha}$，$V_{\alpha}$表示属性&alpha;的值域 f：表示$U\times A\to V$的一个映射，称为信息函数 写到这里，粗糙集理论的内容已经基本和网上的博客，包括wiki一致了，概念也由“关系”、“知识”转变为了“属性”，其实道理都是一致的。 一般地，知识表达系统主要分为两种类型：一类是信息系统，也叫信息表，即不含决策属性的知识表达系统；另一类是决策系统，也叫决策表，即含有决策属性的知识表达系统。因为两种系统的知识约简算法思想是共通的，而且我主要使用的是决策表的约简算法，所以下个blog将会着重讨论决策表的知识约简算法。]]></content>
      <tags>
        <tag>rough set theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典集合理论中的关系]]></title>
    <url>%2F2019%2F02%2F27%2Fthe-relationship-in-classic-aggregate%2F</url>
    <content type="text"><![CDATA[接触粗糙集是因为毕设论文需要用到，但是wiki上和网上的一些博客，包括老师给的参考论文上的理论都有一些出入（其实是不同的东西混在一起），所以就查阅了《粗糙集理论、算法与应用》，对粗糙集理论进行了较为系统的认识。 接着的几篇blog，我将自己在书上看到的和理解的东西整理出来，这一次主要讲一下经典集合中的关系的概念。 了解关系之前，需要先了解如下两个概念： 序偶称由两个元素a1,a2组成的二元有序组(a1,a2)为序偶，如果将概念扩展到n个元素，则称之为n元组。 序偶，包括n元组，和普通集合最大的区别就是前者存在次序，所以两个n元组相等的充要条件是相同次序位置的元素对应相等。 每个具体的二元关系都对应一个以序偶为元素的具有某种特性的集合，这就将函数和集合的概念联系了起来，比如整数1，3，5之间的“小于关系”R对应如下集合： R = {(1,3),(1,5),(3,5)} 反之，给定一个以序偶为元素的集合R，他就确定了一个二元关系，当然，n元关系也可以定义n元组为元素的集合。但这种说法也是有问题的，比如给你上面的R，其实可以得到小于关系以外的关系，比如我拟合一个二次曲线。 集合的笛卡尔积称A X B = {(a,b) | a∈A,b∈B}为集合A和集合B的笛卡尔积，注意以为序偶的次序性，所以集合的笛卡尔积是不满足交换律和结合律，但是可以满足分配律。 下面给出关系的定义： 称A X B 的任一子集为A到B的一个二元关系，记为R，即R&sube;A X B，即关系的实质是一个集合，比如常用的二元关系即为序偶组成的集合，下面的内容如果不加说明，一般指的是二元关系。 给定集合A和集合B，如果元素x∈A和元素y∈B满足关系R，则记为xRy，否则则记为xRy。称集合A是关系R的前域，B为其后域。D(R) = {x|&exist;y, xRy}为关系R的定义域(和函数课程中的定义一致)，R(R) = {y|&exist;x,xRy}是关系R的值域。 等价关系如果集合A上的关系R满足如下条件： 1、自反性：即&forall;x∈A，满足xRx； 2、对称性：即&forall;x,y∈A，如果有xRy，则必有yRx； 3、传递性：即&forall;x,y,z∈A，如果有xRy&and;yRz，则必有xRz。 则称R是集合A上的等价关系。 等价关系看起来要求这么多，那到底和粗糙集理论有什么关系那？先别急着下结论，让我们先来理解一下，什么是等价关系。拿西瓜为例，假设我们判断一个西瓜的好坏会从花纹、颜色、大小三个特征入手，或者说一堆西瓜（即论域A）在我这其实就是3个特征组成的3元组组成的集合。现在我给出如下的等价关系：“西瓜的花纹、颜色、大小均相等”，即R = {(x,y)|x,y∈A &and; x.花纹 = y.花纹 &and;x.颜色 = y.颜色 &and; x.大小 = y.大小}，也就是最常见的等价关系——恒等关系。根据这个关系，我们可以将论域A划分为一系列子集的并集，而这里划分的概念才是粗糙集理论的基础。 商集设R为非空有限集合A上的等价关系，以R的互不相同的所有等价类为元素组成的集合叫做A在R下的商集，记做A/R，即A/R = {[x]R|&forall;x∈A}。 上面提到的等价类有如下定义： 设R是非空有限集合(论域)U上的等价关系，&forall;x∈U，定义 [x]R = {y|yRx}。 商集就是上一部分提到的划分，而划分也有严格的定义（区分于覆盖，可以从划分的概念推出就不赘述了），给定一个论域U和它的非空子集簇π = {A1,A2,…,Am}，如果满足： 1、Ai ≠ ∅ 2、 U = \bigcup_{i=1}^{m}A_{i}3、 Ai&and;Aj = &empty;,i &ne; j 则称集合簇π是U的一个划分，并且有如下定理： 非空有限集合A上的等价关系与集合A的划分是一一对应的。也就是说A/R就是A的一个划分。 经典集合 vs 粗糙集合经典集合只能表达那些具有明确外延的清晰概念，一个对象x是否隶属于某个概念A，必须是非此即彼的，要么x&isin;A，要么x&notin;A，二者必居一。 但是一个粗糙集A是通过它的上、下近似集来表示的，这个概念我们将会在后面的blog讨论，这里我们关于上、下近似集，仅需要知道，它们是使用论域U的一个划分描述A的表达形式。既然我们的描述对象从具体的元素转为了划分，那么一个元素x是否隶属于粗糙集A，自然和x所在的划分与A的关系有关： 如果划分包含于A，即该划分属于A的下近似集，那么x肯定属于概念A；如果划分和A没有交集，即该划分在A的上近似集之外，那么x肯定不属于概念A；如果划分包含于A的上近似集而不包含于下近似集，那么x“可能”属于概念A。 在机器学习任务中，我们经常需要和数据的“特征”打交道，特征的不同则将我们的数据集进行了“划分”，这和我上面举的西瓜的例子想表达的东西是一致的，而粗糙集理论解决的就是只给定数据，如何从其中提取出更为“重要”的特征的问题。]]></content>
      <tags>
        <tag>rough set theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter13 short corridor]]></title>
    <url>%2F2018%2F12%2F28%2FChapter13-short-corridor%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter13/short_corridor.py 使用一个只含3个non-terminal state的简单grid-world问题讨论参数化policy算法的性能。 问题描述 每个reward=-1，第二个state的action是反转的，即left-&gt;right state, right-&gt;left state 引入模块，并给出第一个state的true value计算公式(based on Bellman equation)12345678910111213141516import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdmdef true_value(p): """ True value of the first state Args: p (float): probability of the action 'right'. Returns: True value of the first state. The expression is obtained by manually solving the easy linear system of Bellman equations using known dynamics. """ return (2 * p - 4) / (p * (1 - p)) 创建环境类，实现agent和环境之间的交互123456789101112131415161718192021222324252627282930313233class ShortCorridor: """ Short corridor environment, see Example 13.1 """ def __init__(self): self.reset() def reset(self): self.state = 0 def step(self, go_right): """ Args: go_right (bool): chosen action Returns: tuple of (reward, episode terminated?) """ if self.state == 0 or self.state == 2: if go_right: self.state += 1 else: self.state = max(0, self.state - 1) else: if go_right: self.state -= 1 else: self.state += 1 if self.state == 3: # terminal state return -1, True else: return -1, False 创建agent类，实现参数化policy算法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class ReinforceAgent: """ ReinforceAgent that follows algorithm 'REINFORNCE Monte-Carlo Policy-Gradient Control (episodic)' """ def __init__(self, alpha, gamma): # set values such that initial conditions correspond to left-epsilon greedy self.theta = np.array([-1.47, 1.47]) self.alpha = alpha self.gamma = gamma # first column - left, second - right self.x = np.array([[0, 1], [1, 0]]) self.rewards = [] self.actions = [] def get_pi(self): h = np.dot(self.theta, self.x) t = np.exp(h - np.max(h)) pmf = t / np.sum(t) # never become deterministic, # guarantees episode finish imin = np.argmin(pmf) epsilon = 0.05 if pmf[imin] &lt; epsilon: pmf[:] = 1 - epsilon pmf[imin] = epsilon return pmf def get_p_right(self): # 返回action=right的概率 return self.get_pi()[1] def choose_action(self, reward): if reward is not None: self.rewards.append(reward) pmf = self.get_pi() go_right = np.random.uniform() &lt;= pmf[1] self.actions.append(go_right) return go_right # 此时一次episode完成了，开始进行更新 def episode_end(self, last_reward): self.rewards.append(last_reward) # learn theta G = np.zeros(len(self.rewards)) G[-1] = self.rewards[-1] # 更新顺序是反向的 for i in range(2, len(G) + 1): G[-i] = self.gamma * G[-i + 1] + self.rewards[-i] gamma_pow = 1 for i in range(len(G)): j = 1 if self.actions[i] else 0 pmf = self.get_pi() grad_ln_pi = self.x[j] - np.dot(self.x, pmf) update = self.alpha * gamma_pow * G[i] * grad_ln_pi self.theta += update gamma_pow *= self.gamma self.rewards = [] self.actions = [] 构造使用baseline的agent类，实现approximation policy with baseline算法12345678910111213141516171819202122232425262728293031class ReinforceBaselineAgent(ReinforceAgent): def __init__(self, alpha, gamma, alpha_w): super(ReinforceBaselineAgent, self).__init__(alpha, gamma) self.alpha_w = alpha_w self.w = 0 def episode_end(self, last_reward): self.rewards.append(last_reward) # learn theta G = np.zeros(len(self.rewards)) G[-1] = self.rewards[-1] for i in range(2, len(G) + 1): G[-i] = self.gamma * G[-i + 1] + self.rewards[-i] gamma_pow = 1 for i in range(len(G)): self.w += self.alpha_w * gamma_pow * (G[i] - self.w) j = 1 if self.actions[i] else 0 pmf = self.get_pi() grad_ln_pi = self.x[j] - np.dot(self.x, pmf) update = self.alpha * gamma_pow * (G[i] - self.w) * grad_ln_pi self.theta += update gamma_pow *= self.gamma self.rewards = [] self.actions = [] 完成一个episode的训练12345678910111213141516171819202122def trial(num_episodes, agent_generator): env = ShortCorridor() agent = agent_generator() rewards = np.zeros(num_episodes) for episode_idx in range(num_episodes): rewards_sum = 0 reward = None env.reset() while True: go_right = agent.choose_action(reward) reward, episode_end = env.step(go_right) rewards_sum += reward if episode_end: agent.episode_end(reward) break rewards[episode_idx] = rewards_sum return rewards 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485def example_13_1(): epsilon = 0.05 fig, ax = plt.subplots(1, 1) # 绘制true value of start state，用来和J(Θ)比较 p = np.linspace(0.01, 0.99, 100) y = true_value(p) ax.plot(p, y, color='red') # Find a maximum point, can also be done analytically by taking a derivative imax = np.argmax(y) pmax = p[imax] ymax = y[imax] ax.plot(pmax, ymax, color='green', marker="*", label="optimal point: f(&#123;0:.2f&#125;) = &#123;1:.2f&#125;".format(pmax, ymax)) # Plot points of two epsilon-greedy policies ax.plot(epsilon, true_value(epsilon), color='magenta', marker="o", label="epsilon-greedy left") ax.plot(1 - epsilon, true_value(1 - epsilon), color='blue', marker="o", label="epsilon-greedy right") ax.set_ylabel("Value of the first state") ax.set_xlabel("Probability of the action 'right'") ax.set_title("Short corridor with switched actions") ax.set_ylim(ymin=-105.0, ymax=5) ax.legend() plt.savefig('./example_13_1.png') plt.show()def figure_13_1(): num_trials = 30 num_episodes = 1000 alpha = 2e-4 gamma = 1 rewards = np.zeros((num_trials, num_episodes)) # 使用匿名函数，可以在后续代码改变agent的参数进行比较 agent_generator = lambda : ReinforceAgent(alpha=alpha, gamma=gamma) for i in tqdm(range(num_trials)): reward = trial(num_episodes, agent_generator) rewards[i, :] = reward plt.plot(np.arange(num_episodes) + 1, -11.6 * np.ones(num_episodes), ls='dashed', color='red', label='-11.6') plt.plot(np.arange(num_episodes) + 1, rewards.mean(axis=0), color='blue') plt.ylabel('total reward on episode') plt.xlabel('episode') plt.legend(loc='lower right') plt.savefig('./figure_13_1.png') plt.show()def figure_13_2(): num_trials = 30 num_episodes = 1000 alpha = 2e-4 gamma = 1 agent_generators = [lambda : ReinforceAgent(alpha=alpha, gamma=gamma), lambda : ReinforceBaselineAgent(alpha=alpha, gamma=gamma, alpha_w=alpha*100)] labels = ['Reinforce with baseline', 'Reinforce without baseline'] rewards = np.zeros((len(agent_generators), num_trials, num_episodes)) for agent_index, agent_generator in enumerate(agent_generators): for i in tqdm(range(num_trials)): reward = trial(num_episodes, agent_generator) rewards[agent_index, i, :] = reward plt.plot(np.arange(num_episodes) + 1, -11.6 * np.ones(num_episodes), ls='dashed', color='red', label='-11.6') for i, label in enumerate(labels): plt.plot(np.arange(num_episodes) + 1, rewards[i].mean(axis=0), label=label) plt.ylabel('total reward on episode') plt.xlabel('episode') # 用来设置注解小方框的位置 plt.legend(loc='lower right') plt.savefig('./figure_13_2.png') plt.show()if __name__ == '__main__': example_13_1() figure_13_1() figure_13_2() 100%|██████████| 30/30 [00:23&lt;00:00, 1.18it/s] 100%|██████████| 30/30 [00:23&lt;00:00, 1.26it/s] 100%|██████████| 30/30 [00:29&lt;00:00, 1.09it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter13 Policy Gradient Methods]]></title>
    <url>%2F2018%2F12%2F28%2FChapter13-Policy-Gradient-Methods%2F</url>
    <content type="text"><![CDATA[目前为止讨论的的强化学习算法都是基于value的，即通过学习value来决定policy，这章绕过value这个学习目标，虽然仍然会更新value，但是学习目标是得到一个参数化的policy，来直接进行决策。 Policy Approximation and its Advantagespolicy可以以任意形式参数化，前提是policy：π(a|s,Θ)关于它的参数是可微分的，即∇_θ π(a|s,θ)存在且有限，同时为了保证policy可以提供exploration，π(s|a,Θ)不能是决定的，即∈(0,1), for all s,a,θ.这个section首先讨论离散的action的一种常见的参数化policy方式，以及其对应的优点，关于连续的action会在本章后续section进行讨论。 如果action离散且有限，那么一种常见的参数化方法就是为每个state-action对设置偏爱值：preferences h(s, a, θ) ∈ R，对应的公式和在第二章里使用的preference一致，通过如下的softmax分布来选取对应的偏爱值高的action： 使用基于action preference: h(s|a)有以下3个优点： 1、最直接的优点就是基于action preference的参数化policy更接近于deterministic policy，而前面经常用的基于action value的epsilon-greedy会有一定概率选择随机的action。当然你会考虑是否可以使用基于action value的softmax分布来选择action?这也是不无依据的，wiki上关于softmax在RL的应用有如下公式： 随着训练过程，action value将会收敛，导致基于action value的softmax分布收敛到一系列确定的值。但是注意到这里还有一个 temperature parameter:τ，如果τ过大，会导致softmax分布近似为均匀分布，所以希望τ可以适应action value的变化。一般情况下，在训练中随着action value的收敛，需要不断减少τ的值来增强deterministic，但是减少的方法以及τ的初始值都需要对action value有一定的先验知识，使得实现起来比较困难。但是基于action preference的softmax分布就不存在这样的问题，因为它得到的不是确定的值，而是得到optimal stochastic policy。 2、在讨论第二个优点前，先看一个如下的MDP例子： 第一个和第三个state的action导致的结果是正常的，但是第二个state的action导致的结果是相反的，每个action的reward都是-1，最右边是terminal state。 首先是作者对这个问题的认识： 这个MDP问题是困难的，因为所有的non-terminal state都是相同的；使用epsilon-greedy方法只有两个方法，epsilon-left或者epsilon-right。但是效果很差，如果使用action preference可以得到确定的optimal policy，对应大约0.59的right action，效果优于epsilon-greedy方法。 个人觉得这个说法有问题，因为使用epsilon-greedy方法其实是一个评估(evaluation)问题，得到不同的甚至较差的value都是正常的，毕竟你没做优化啊，但是使用action preference的时候通过梯度优化了偏爱函数h(a|s,Θ)，所以拿优化问题的结果和评估问题的结果对比本身就是有问题的。其次就是state的相同性(identical)，个人觉得random walk问题里的state按照这个说法是不是也都一样的呢？明显总体趋势向右可以达到较优化的结果，而且考虑到第二个state的相反性，结果不应该是完全的右向，所以不太认同这里的一致性。 好了岔远了，现在来讨论第二个优点，即有的问题中最优的policy可能是在多个选择中随机选择的，但是epsilon-greedy不具有这个功能，action preference可以。好吧说实话不太认同。 3、有的问题action preference效果更好。。。感觉除了第一个优点我觉得可能接近deterministic的approximate policy可能会更快收敛，后两个感觉有点片面了。 The Policy Gradient Theorem要使用GD方法训练policy的参数的话，需要确定一个loss function，这里换了一个名字：performance measure，这里使用符号J(Θ)来表示。在这个section我们先讨论episodic问题，这里我们将episode的start state的基于估计policy π_Θ的state value作为J(Θ): 那么如何给出J(Θ)对应的梯度那，首先考虑一下J(Θ)的影响元素，应该是action selections and the distribution of states.前者好理解，就是π(a|s,Θ)嘛，但是state distribution还会受到环境的影响，不能直接给出distribution关于参数Θ的微分。但是我们可以借助如下的Policy Gradient Theorem来避开考虑distribution的微分计算梯度： 正比符号∝在episodic task中表示后者需要乘上episode的平均长度，在continuing task中则直接等价于”=”。上式中的μ(s)可以通过下面的公式理解，可以看到μ(s)不但和policy π有关，同时也受到环境的影响： 关于Policy Gradient Theorem的证明，书上给出了如下公式变换，先看第一部分： 看完之后问题最大的就是最后一步，其中Pr(s-&gt;x,k,π)指的是在policy π的条件下，从state s经过k步转换到state x的概率。其实如果把最后一步拆开看就比较明显了： 太长了我就只写2项吧，是不是就可以理解了那？这里还有关于x∈S和k的两个积分号的交换，当k=0时，令Pr(s-&gt;x,0,π)=0 when x≠s即顺利完成交换。 理解了第一部分第二部分就好办多了，接下来看第二部分的推导： 关于η(s)的替换应该比较清楚了，因为h(s_0) = 1，所以h(s) = 0 when s ≠s_0,可以看到如果是continuing task的话，前面的常数部分=1，如果是episodic task，前面的常数部分等于一个episode的平均长度，因为η(s)代表一个episode中state s出现的平均次数，刚好和上面说的相符。 REINFORCE: Monte Carlo Policy GradientWe are now ready for our first policy-gradient learning algorithm. 8说了，开冲。 对应的更新公式如下： We call this algorithm REINFORCE.因为使用了Gt，所以需要等待一个episode结束才能完成一次更新，属于MC方法。下面给出对应的伪代码，因为使用了所以的样本来完成更新，所以属于batch-gradient算法。同时这里使用了ln函数，一种在数学上很常见的替换。 当π(a|s,Θ)使用线性h(Θ)的softmax函数时，微分项可以写作如下形式： REINFORCE with Baseline可以给policy gradient theorem的action value部分添加一个baseline，公式如下： 原理是因为baseline引入的总和为0，所以总体上并没有改变梯度的值： 因为上一个section使用的是MC方法的REINFORCE方法，所以自然想到使用由MC方法训练得到的w来估计v(St,w)并作为baseline，因为不同的state的value差别可能很大，使用和Gt相适应的baseline，可以有效的降低方差(variance)，下面给出对应的伪代码： Actor–Critic Methods前面使用的MC方法，并没有引入bootstrape，给定的baseline也只是单独的更新，而没有借助到其它的value。事实上通过actor-critic，借助其它value来维护baseline，可以进一步提高训练速度，通过在训练过程中适当的引入bias，可以降低最终的variance，得到全局的优化结果。可以使用如下的更新公式，将one-step的bootstrape引入更新： 给出对应的伪代码： 如果将TD(0)替换为λ-return方法，加入Eligibility trace，就可以将backward view引入更新公式，伪代码如下： Policy Gradient for Continuing Problems前面使用的policy gradient theorem对应continuing task仍然适用，通过添加average reward，可以将episodic task的伪代码迁移到continuing task中使用： 对应的证明如下： 第一部分的推导很常规，公式如下： 在episodic task中，J(Θ)使用的是start state的value，因为这个值不依赖于s，倒也不是说没有关系，但是不是s的函数，即J(Θ)最终是S空间的综合作用。采用一样的想法，continuing task使用r(Θ)作为J(Θ)，即average reward，对应的梯度如下，可以看到结果和episodic task一致，但是仍然应该明白它们对应的J(Θ)的意义是不同的： 这里涉及到continuing task的distribution问题，和episodic task不一样，这里的μ(s)不是离散的不同的值，而是一个在整个S空间上一致的函数，对应的递推公式如下： Policy Parameterization for Continuous Actions参数化的policy方法可以适用于action连续的问题。假设action的选取范围是一个实数范围，对应正态分布的选取概率： 可以将policy函数设置为参数化的p(x)，对应的公式如下： 换句话说，主要是需要参数化对应的p(x)中的方差项和偏差项，一般使用如下的公式： 更新对应的梯度更新公式如下：]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter12 mountain car]]></title>
    <url>%2F2018%2F12%2F27%2FChapter12-mountain-car%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter12/mountain_car.py 使用喜闻乐见的MountainCar来测试Sarsa(λ)算法的性能 因为一直在纠结true online Sarsa(λ)的问题，然后我发现效果惨不忍睹。。。不知道是自己的问题还是不太合适用在这，可以在下面看出来。因为训练速度很慢(全部训练完差不多一下午，中间我还在sao改)，所以就借鉴一下大佬的结果。 问题描述使用强化学习中的经典任务——Mountain Car来测试Sarsa(λ)和true on-line Sarsa(λ)算法的性能，同时也测试了不同的Eligibility trace对Sarsa(λ)算法性能的影响。 Mountain Car的具体描述可以参考例子mountain car 引入模块123456import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom math import floorfrom tqdm import tqdm 下面模块的代码和mountain car一致，建议看之前再熟悉一下Tile Coding的原理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103######################################################################## Following are some utilities for tile coding from Rich.# To make each file self-contained, I copied them from# http://incompleteideas.net/tiles/tiles3.py-remove# with some naming convention changes## Tile coding startsclass IHT: "Structure to handle collisions" def __init__(self, size_val): self.size = size_val self.overfull_count = 0 self.dictionary = &#123;&#125; def count(self): return len(self.dictionary) def full(self): return len(self.dictionary) &gt;= self.size def get_index(self, obj, read_only=False): d = self.dictionary if obj in d: return d[obj] elif read_only: return None size = self.size count = self.count() if count &gt;= size: if self.overfull_count == 0: print('IHT full, starting to allow collisions') self.overfull_count += 1 return hash(obj) % self.size else: d[obj] = count return countdef hash_coords(coordinates, m, read_only=False): if isinstance(m, IHT): return m.get_index(tuple(coordinates), read_only) if isinstance(m, int): return hash(tuple(coordinates)) % m if m is None: return coordinatesdef tiles(iht_or_size, num_tilings, floats, ints=None, read_only=False): """returns num-tilings tile indices corresponding to the floats and ints""" if ints is None: ints = [] qfloats = [floor(f * num_tilings) for f in floats] tiles = [] for tiling in range(num_tilings): tilingX2 = tiling * 2 coords = [tiling] b = tiling for q in qfloats: coords.append((q + b) // num_tilings) b += tilingX2 coords.extend(ints) tiles.append(hash_coords(coords, iht_or_size, read_only)) return tiles# Tile coding ends######################################################################## all possible actionsACTION_REVERSE = -1ACTION_ZERO = 0ACTION_FORWARD = 1# order is importantACTIONS = [ACTION_REVERSE, ACTION_ZERO, ACTION_FORWARD]# bound for position and velocityPOSITION_MIN = -1.2POSITION_MAX = 0.5VELOCITY_MIN = -0.07VELOCITY_MAX = 0.07# discount is always 1.0 in these experimentsDISCOUNT = 1.0# use optimistic initial value, so it's ok to set epsilon to 0EPSILON = 0# maximum steps per episodeSTEP_LIMIT = 5000# take an @action at @position and @velocity# @return: new position, new velocity, reward (always -1)def step(position, velocity, action): new_velocity = velocity + 0.001 * action - 0.0025 * np.cos(3 * position) new_velocity = min(max(VELOCITY_MIN, new_velocity), VELOCITY_MAX) new_position = position + new_velocity new_position = min(max(POSITION_MIN, new_position), POSITION_MAX) reward = -1.0 if new_position == POSITION_MIN: new_velocity = 0.0 return new_position, new_velocity, reward# get action at @position and @velocity based on epsilon greedy policy and @valueFunctiondef get_action(position, velocity, valueFunction): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(ACTIONS) values = [] for action in ACTIONS: values.append(valueFunction.value(position, velocity, action)) return np.argmax(values) - 1 定义3种常见的Eligibility trace，就是更新公式里常见的z12345678910111213141516171819202122232425262728293031323334353637383940414243444546# accumulating trace update rule# @trace: old trace (will be modified)# @activeTiles: current active tile indices# @lam: lambda# @return: new trace for conveniencedef accumulating_trace(trace, active_tiles, lam): trace *= lam * DISCOUNT trace[active_tiles] += 1 return trace# replacing trace update rule# 这种trace只适用于Tile Coding这种的binary feature vector# @trace: old trace (will be modified)# @activeTiles: current active tile indices# @lam: lambda# @return: new trace for conveniencedef replacing_trace(trace, activeTiles, lam): active = np.in1d(np.arange(len(trace)), activeTiles) trace[active] = 1 trace[~active] *= lam * DISCOUNT return trace# replacing trace update rule, 'clearing' means set all tiles corresponding to non-selected actions to 0# @trace: old trace (will be modified)# @activeTiles: current active tile indices# @lam: lambda# @clearingTiles: tiles to be cleared# @return: new trace for conveniencedef replacing_trace_with_clearing(trace, active_tiles, lam, clearing_tiles): active = np.in1d(np.arange(len(trace)), active_tiles) trace[~active] *= lam * DISCOUNT trace[clearing_tiles] = 0 trace[active] = 1 return trace# dutch trace update rule# @trace: old trace (will be modified)# @activeTiles: current active tile indices# @lam: lambda# @alpha: step size for all tiles# @return: new trace for conveniencedef dutch_trace(trace, active_tiles, lam, alpha): coef = 1 - alpha * DISCOUNT * lam * np.sum(trace[active_tiles]) trace *= DISCOUNT * lam trace[active_tiles] += coef return trace 定义Sarsa(λ)算法类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# wrapper class for Sarsa(lambda)class Sarsa: # In this example I use the tiling software instead of implementing standard tiling by myself # One important thing is that tiling is only a map from (state, action) to a series of indices # It doesn't matter whether the indices have meaning, only if this map satisfy some property # View the following webpage for more information # http://incompleteideas.net/sutton/tiles/tiles3.html # @maxSize: the maximum # of indices def __init__(self, step_size, lam, trace_update=accumulating_trace, num_of_tilings=8, max_size=2048): self.max_size = max_size self.num_of_tilings = num_of_tilings self.trace_update = trace_update self.lam = lam self.last_value = 0.0 # divide step size equally to each tiling self.step_size = step_size / num_of_tilings self.hash_table = IHT(max_size) # weight for each tile self.weights = np.zeros(max_size) # trace for each tile self.trace = np.zeros(max_size) # position and velocity needs scaling to satisfy the tile software self.position_scale = self.num_of_tilings / (POSITION_MAX - POSITION_MIN) self.velocity_scale = self.num_of_tilings / (VELOCITY_MAX - VELOCITY_MIN) # get indices of active tiles for given state and action def get_active_tiles(self, position, velocity, action): # I think positionScale * (position - position_min) would be a good normalization. # However positionScale * position_min is a constant, so it's ok to ignore it. active_tiles = tiles(self.hash_table, self.num_of_tilings, [self.position_scale * position, self.velocity_scale * velocity], [action]) return active_tiles # estimate the value of given state and action def value(self, position, velocity, action): if position == POSITION_MAX: return 0.0 active_tiles = self.get_active_tiles(position, velocity, action) return np.sum(self.weights[active_tiles]) # learn with given state, action and target def learn(self, position, velocity, action, target, next_value): active_tiles = self.get_active_tiles(position, velocity, action) estimation = np.sum(self.weights[active_tiles]) delta = target - estimation if self.trace_update == accumulating_trace or self.trace_update == replacing_trace: self.trace_update(self.trace, active_tiles, self.lam) elif self.trace_update == dutch_trace: self.trace_update(self.trace, active_tiles, self.lam, self.step_size) # use true on-line update self.weights += self.step_size * (estimation - self.last_value) * self.trace self.weights[active_tiles] -= self.step_size * (estimation - self.last_value) self.last_value = next_value elif self.trace_update == replacing_trace_with_clearing: clearing_tiles = [] for act in ACTIONS: if act != action: clearing_tiles.extend(self.get_active_tiles(position, velocity, act)) self.trace_update(self.trace, active_tiles, self.lam, clearing_tiles) else: raise Exception('Unexpected Trace Type') self.weights += self.step_size * delta * self.trace # get # of steps to reach the goal under current state value function def cost_to_go(self, position, velocity): costs = [] for action in ACTIONS: costs.append(self.value(position, velocity, action)) return -np.max(costs) agent完成play，并绘制使用不同trace的cost map12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# play Mountain Car for one episode based on given method @evaluator# @return: total steps in this episodedef play(evaluator): position = np.random.uniform(-0.6, -0.4) velocity = 0.0 action = get_action(position, velocity, evaluator) steps = 0 while True: next_position, next_velocity, reward = step(position, velocity, action) next_action = get_action(next_position, next_velocity, evaluator) steps += 1 target = reward + DISCOUNT * evaluator.value(next_position, next_velocity, next_action) next_value = evaluator.value(next_position, next_velocity, next_action) evaluator.learn(position, velocity, action, target,next_value) position = next_position velocity = next_velocity action = next_action if next_position == POSITION_MAX: break if steps &gt;= STEP_LIMIT: print('Step Limit Exceeded!') break return steps# figure 12.10, effect of the lambda and alpha on early performance of Sarsa(lambda)def figure_12_10(): runs = 30 episodes = 50 alphas = np.arange(1, 8) / 4.0 lams = [0.99, 0.95, 0.5, 0] steps = np.zeros((len(lams), len(alphas), runs, episodes)) for lamInd, lam in enumerate(lams): for alphaInd, alpha in enumerate(alphas): for run in tqdm(range(runs)): evaluator = Sarsa(alpha, lam, replacing_trace) for ep in range(episodes): step = play(evaluator) steps[lamInd, alphaInd, run, ep] = step # average over episodes steps = np.mean(steps, axis=3) # average over runs steps = np.mean(steps, axis=2) for lamInd, lam in enumerate(lams): plt.plot(alphas, steps[lamInd, :], label='lambda = %s' % (str(lam))) plt.xlabel('alpha * # of tilings (8)') plt.ylabel('averaged steps per episode') plt.ylim([180, 300]) plt.legend() plt.savefig('./figure_12_10.png') plt.show()# figure 12.11, summary comparision of Sarsa(lambda) algorithms# I use 8 tilings rather than 10 tilingsdef figure_12_11(): # traceTypes = [dutch_trace, replacing_trace, replacing_trace_with_clearing, accumulating_trace] traceTypes = [dutch_trace,replacing_trace] alphas = np.arange(0.2, 1.8, 0.2) episodes = 20 runs = 30 lam = 0.9 rewards = np.zeros((len(traceTypes), len(alphas), runs, episodes)) for traceInd, trace in enumerate(traceTypes): for alphaInd, alpha in enumerate(alphas): for run in tqdm(range(runs)): evaluator = Sarsa(alpha, lam, trace) for ep in range(episodes): if trace == accumulating_trace and alpha &gt; 0.6: steps = STEP_LIMIT else: steps = play(evaluator) rewards[traceInd, alphaInd, run, ep] = -steps # average over episodes rewards = np.mean(rewards, axis=3) # average over runs rewards = np.mean(rewards, axis=2) for traceInd, trace in enumerate(traceTypes): plt.plot(alphas, rewards[traceInd, :], label=trace.__name__) plt.xlabel('alpha * # of tilings (8)') plt.ylabel('averaged rewards pre episode') # plt.ylim([-550, -150]) plt.legend() plt.savefig('./figure_12_11.png') plt.show()figure_12_10()figure_12_11() 100%|██████████| 30/30 [02:07&lt;00:00, 4.19s/it] 100%|██████████| 30/30 [01:49&lt;00:00, 3.72s/it] 100%|██████████| 30/30 [01:42&lt;00:00, 3.45s/it] 100%|██████████| 30/30 [01:37&lt;00:00, 3.13s/it] 100%|██████████| 30/30 [01:44&lt;00:00, 3.31s/it] 100%|██████████| 30/30 [01:42&lt;00:00, 3.17s/it] 100%|██████████| 30/30 [01:40&lt;00:00, 3.37s/it] 100%|██████████| 30/30 [01:36&lt;00:00, 3.14s/it] 100%|██████████| 30/30 [01:36&lt;00:00, 3.36s/it] 100%|██████████| 30/30 [01:07&lt;00:00, 2.24s/it] 100%|██████████| 30/30 [00:57&lt;00:00, 1.90s/it] 100%|██████████| 30/30 [00:53&lt;00:00, 1.77s/it] 100%|██████████| 30/30 [00:49&lt;00:00, 1.63s/it] 100%|██████████| 30/30 [00:46&lt;00:00, 1.56s/it] 100%|██████████| 30/30 [00:45&lt;00:00, 1.45s/it] 100%|██████████| 30/30 [00:45&lt;00:00, 1.54s/it] figure_12_10: figure_12_11: why online Sarsa(λ) so terrible QAQ:]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter12 19-states random-walk]]></title>
    <url>%2F2018%2F12%2F27%2FChapter12-19-states-random-walk%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter12/random_walk.py 使用random walk问题来测试TD(λ)算法的性能 问题描述本例通过将不同类型的的TD(λ)方法应用在Chapter06的random-walk问题中，不过将原来的5-state问题修改为了19-state问题。 引入模块并定义常量1234567891011121314151617181920212223import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm# all statesN_STATES = 19# all states but terminal statesSTATES = np.arange(1, N_STATES + 1)# start from the middle stateSTART_STATE = 10# two terminal states# an action leading to the left terminal state has reward -1# an action leading to the right terminal state has reward 1END_STATES = [0, N_STATES + 1]# true state values from Bellman equationTRUE_VALUE = np.arange(-20, 22, 2) / 20.0TRUE_VALUE[0] = TRUE_VALUE[N_STATES + 1] = 0.0 将TD(λ)算法封装为类，提供接口给agent，用来在交互过程中训练weight基本算法类，只对TD(λ)算法使用的数据进行定义，用来在其他算法类中使用1234567891011121314151617181920212223242526# base class for lambda-based algorithms in this chapter# In this example, we use the simplest linear feature function, state aggregation.# And we use exact 19 groups, so the weights for each group is exact the value for that stateclass ValueFunction: # @rate: lambda, as it's a keyword in python, so I call it rate # @stepSize: alpha, step size for update def __init__(self, rate, step_size): self.rate = rate self.step_size = step_size # 需要考虑terminal state self.weights = np.zeros(N_STATES + 2) # the state value is just the weight def value(self, state): return self.weights[state] # feed the algorithm with new observation # derived class should override this function def learn(self, state, reward): return # initialize some variables at the beginning of each episode # must be called at the very beginning of each episode # derived class should override this function def new_episode(self): return off-line λ-return算法类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# Off-line lambda-return algorithmclass OffLineLambdaReturn(ValueFunction): def __init__(self, rate, step_size): ValueFunction.__init__(self, rate, step_size) # To accelerate learning, set a truncate value for power of lambda self.rate_truncate = 1e-3 def new_episode(self): # initialize the trajectory self.trajectory = [START_STATE] # only need to track the last reward in one episode, as all others are 0 self.reward = 0.0 def learn(self, state, reward): # add the new state to the trajectory self.trajectory.append(state) if state in END_STATES: # start off-line learning once the episode ends self.reward = reward self.T = len(self.trajectory) - 1 self.off_line_learn() # get the n-step return from the given time def n_step_return_from_time(self, n, time): # gamma is always 1 and rewards are zero except for the last reward # the formula can be simplified end_time = min(time + n, self.T) returns = self.value(self.trajectory[end_time]) if end_time == self.T: returns += self.reward return returns # get the lambda-return from the given time def lambda_return_from_time(self, time): returns = 0.0 lambda_power = 1 for n in range(1, self.T - time): returns += lambda_power * self.n_step_return_from_time(n, time) lambda_power *= self.rate if lambda_power &lt; self.rate_truncate: # If the power of lambda has been too small, discard all the following sequences break returns *= 1 - self.rate if lambda_power &gt;= self.rate_truncate: returns += lambda_power * self.reward return returns # perform off-line learning at the end of an episode def off_line_learn(self): for time in range(self.T): # update for each state in the trajectory state = self.trajectory[time] delta = self.lambda_return_from_time(time) - self.value(state) delta *= self.step_size self.weights[state] += delta TD(λ)算法类1234567891011121314151617181920212223# TD(lambda) algorithmclass TemporalDifferenceLambda(ValueFunction): def __init__(self, rate, step_size): ValueFunction.__init__(self, rate, step_size) self.new_episode() def new_episode(self): # initialize the eligibility trace self.eligibility = np.zeros(N_STATES + 2) # initialize the beginning state self.last_state = START_STATE def learn(self, state, reward): # update the eligibility trace and weights self.eligibility *= self.rate # 注意这里使用的self.last_state # 因为提供的参数state实际上是next_state，而self.last_state才是需要更新的current_state， # 这个问题在下面的代码中也存在，需要注意一下 self.eligibility[self.last_state] += 1 delta = reward + self.value(state) - self.value(self.last_state) delta *= self.step_size self.weights += delta * self.eligibility self.last_state = state true on-line TD(λ)算法类12345678910111213141516171819202122232425262728# True online TD(lambda) algorithmclass TrueOnlineTemporalDifferenceLambda(ValueFunction): def __init__(self, rate, step_size): ValueFunction.__init__(self, rate, step_size) def new_episode(self): # initialize the eligibility trace self.eligibility = np.zeros(N_STATES + 2) # initialize the beginning state self.last_state = START_STATE # initialize the old state value # 第一次更新的时候，比较纠结的就是w_t-1的值，如果这样设置， # 说明初始化w_t-1为0 self.old_state_value = 0.0 def learn(self, state, reward): # update the eligibility trace and weights # 公式12.11，仔细看。。。 last_state_value = self.value(self.last_state) state_value = self.value(state) dutch = 1 - self.step_size * self.rate * self.eligibility[self.last_state] self.eligibility *= self.rate self.eligibility[self.last_state] += dutch delta = reward + state_value - last_state_value self.weights += self.step_size * (delta + last_state_value - self.old_state_value) * self.eligibility self.weights[self.last_state] -= self.step_size * (last_state_value - self.old_state_value) self.old_state_value = state_value self.last_state = state 完成一次更新1234567891011121314# 19-state random walkdef random_walk(value_function): value_function.new_episode() state = START_STATE while state not in END_STATES: next_state = state + np.random.choice([-1, 1]) if next_state == 0: reward = -1 elif next_state == N_STATES + 1: reward = 1 else: reward = 0 value_function.learn(next_state, reward) state = next_state 该函数用来计算给定不同的参数的学习算法的均方误差(square value error)12345678910111213141516171819202122232425262728# general plot framework# @valueFunctionGenerator: generate an instance of value function# @runs: specify the number of independent runs# @lambdas: a series of different lambda values# @alphas: sequences of step size for each lambdadef parameter_sweep(value_function_generator, runs, lambdas, alphas): # play for 10 episodes for each run episodes = 10 # track the rms errors errors = [np.zeros(len(alphas_)) for alphas_ in alphas] for run in tqdm(range(runs)): for lambdaIndex, rate in zip(range(len(lambdas)), lambdas): for alphaIndex, alpha in zip(range(len(alphas[lambdaIndex])), alphas[lambdaIndex]): valueFunction = value_function_generator(rate, alpha) for episode in range(episodes): random_walk(valueFunction) stateValues = [valueFunction.value(state) for state in STATES] errors[lambdaIndex][alphaIndex] += np.sqrt(np.mean(np.power(stateValues - TRUE_VALUE[1: -1], 2))) # average over runs and episodes for error in errors: error /= episodes * runs for i in range(len(lambdas)): plt.plot(alphas[i], errors[i], label='lambda = ' + str(lambdas[i])) plt.xlabel('alpha') plt.ylabel('RMS error') plt.legend() 将上述3种算法不同参数下的均方误差绘制成图像123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# Figure 12.3: Off-line lambda-return algorithmdef figure_12_3(): lambdas = [0.0, 0.4, 0.8, 0.9, 0.95, 0.975, 0.99, 1] alphas = [np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), np.arange(0, 0.55, 0.05), np.arange(0, 0.22, 0.02), np.arange(0, 0.11, 0.01)] parameter_sweep(OffLineLambdaReturn, 50, lambdas, alphas) plt.savefig('./figure_12_3.png') plt.show()# Figure 12.6: TD(lambda) algorithmdef figure_12_6(): lambdas = [0.0, 0.4, 0.8, 0.9, 0.95, 0.975, 0.99, 1] alphas = [np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), np.arange(0, 0.99, 0.09), np.arange(0, 0.55, 0.05), np.arange(0, 0.33, 0.03), np.arange(0, 0.22, 0.02), np.arange(0, 0.11, 0.01), np.arange(0, 0.044, 0.004)] parameter_sweep(TemporalDifferenceLambda, 50, lambdas, alphas) plt.savefig('./figure_12_6.png') plt.show()# Figure 12.7: True online TD(lambda) algorithmdef figure_12_8(): lambdas = [0.0, 0.4, 0.8, 0.9, 0.95, 0.975, 0.99, 1] alphas = [np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), np.arange(0, 0.88, 0.08), np.arange(0, 0.44, 0.04), np.arange(0, 0.11, 0.01)] parameter_sweep(TrueOnlineTemporalDifferenceLambda, 50, lambdas, alphas) plt.savefig('./figure_12_8.png') plt.show()figure_12_3()figure_12_6()figure_12_8() 100%|██████████| 50/50 [04:19&lt;00:00, 5.09s/it] 100%|██████████| 50/50 [01:05&lt;00:00, 1.26s/it] 100%|██████████| 50/50 [01:13&lt;00:00, 1.46s/it]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter12 Eligibility Traces]]></title>
    <url>%2F2018%2F12%2F24%2FChapter12-Eligibility-Traces%2F</url>
    <content type="text"><![CDATA[在以往的强化学习方法中我们看到，不管是使用纯粹采样的MC方法，还是基于Bellman equation的DP方法以及bootstrape方法，每个value(state or action)都依赖于其后发生的动作，即属于forward view的算法思想： 通过引入Eligibility trace，我们便能够将forward view的算法转变为backward view的算法。 The λ-return这个算法比较像MC method with approximation，后面的section会看到使用Eligibility trace的MC方法，这两者完全是不同的，所以我认为λ-return方法和TD(λ)不完全一致，两者结合可以得到true on-line算法，但是λ-return仍然是forward view的，而TD(λ)则是将backward view融入了bootstrape方法中的。 回忆一下在n step-TD中使用的G： 如果使用不同的n，并将不同的n对应的G进行加权，就可以得到如下的混合的return，我们称之为λ-return： 可以看到每个G赋给的weight总和=1。 上面的λ-return通式对应于continuing task，对于episodic task，λ-return可以改写为如下公式，仍然保证了weight的总和为1： 如果使用λ-return完成function approximation的weight更新，则使用如下的更新公式，对应的算法称之为off-line λ-return算法： 这个更新公式通过λ将MC方法和n-step的TD方法联系在一起，如果λ趋于1，那么更新公式趋于MC方法，如果λ趋于0，那么更新公式趋于n-step TD方法，准确来说是one-step TD方法，因为受到λ的幂指数影响。 通过random walk例子比较off-line λ-return和n-step TD的性能，例子可以参考19-state Random walk。 目前为止算法仍然是forward view更新的，下一个section将会讨论第一个引入backward view更新的算法TD(λ)。 TD(λ)理解TD(λ)方法，主要要理解Eligibility trace z。z是和w维度相同的向量。w是一个long-memory的变量，也是我们在多个episode中训练的变量，z却是一个short-memory的变量，在每个episode开头将它初始化为0。一开始可能不理解z的作用，可以先看一下z是如何在w的更新中使用的： 首先是z的更新： 然后是z在w更新中的使用： ( _ゝ`) 其中的δ_t是one-step TD error，可以看到z替代了∇v̂(S_t,w_t)的位置，而且z和后者是同维度的，再结合z的更新公式，我们可以分析得到：z用来调整w各个分量更新的权重，如果令z更新公式里的λ=0，那么TD(λ)就和TD(0)方法一致了，但是z引入了“之前”的state的影响，使得更新算法具有了backward的视角： 使用Eligibility trace z更新的伪代码如下： 通过random-walk的例子可以比较TD(λ)和off-line λ-return算法的性能，例子参考19-state Random walk。 关于TD(λ)的收敛性，可以证明在使用linear approximation function的on-policy训练算法的条件下，w收敛到最小VE的理想w的比例范围内： n-step Truncated λ-return Methods在之前提到的λ-return中，因为越往后的state系数很小对应的影响很小，所以可以忽略其影响得到truncated λ-return： 使用1truncated λ-return进行w更新的算法称为TTD(λ)，更新公式如下： TTD(λ)关于truncated λ-return可以使用下面的优化公式计算： 注意δ’的w的下标： 给出backup diagram of truncated TD(λ)： Redoing Updates: The Online λ-return Algorithm上一个section提到的truncated TD(λ)方法可以改进为on-line算法，具体思路如下： 即将h遍历1,2,3，…分别完成不同h的更新，每次更新的结束作为下一次更新的开始，比如h=1更新结果作为h=2的w初始值，这种算法称为on-line λ-return algorithm。 关于on-line和off-line λ-return算法的性能比较，可以参考19-state random-walk True Online TD(λ)通过上一个section，我们可以看到on-line算法表现的更好，但是给出的on-line λ-return算法计算复杂度很高，每次h的迭代量很大。我们发现，如果把更新结果写在一个下三角矩阵中，那么只有对角线上的值是有用的： 使用下面的更新规则直接更新对角线上的w，得到true online TD(λ)算法： 其中的z为dutch trace，使用如下公式更新： true online TD(λ)算法的伪代码如下： 关于Eligibility trace有如下三种： 1、TD(λ)使用的，称为accumulating trace 2、早期的算法中经常使用的，称为replacing trace，仅适用于0-1(binary)的feature vector，如Tile Coding产生的feature，公式如下： 3、true on-line TD(λ)使用的dutch trace，这种Eligibility trace也是目前最常用的。 Dutch Traces in Monte Carlo Learningdutch trace一般在TD learning算法中使用，但是事实上它也可以在MC方法中使用，并带来计算上的优化。 首先看一下基本的MC方法with approximation： 这里关于return G做如下假设：中间过程的reward都是0，只有到达terminal state的action才产生reward，所以G可以视为常数。 一般的MC方法会在0-T之间更新T次上式，但是可以将公式展开得到如下一步更新公式： 其中a_T和z_T可以不依赖G独立的进行增量更新： 这种方法可以获得更好的计算性能，因为可以不用使用额外的空间来存储episode中所有的feature vector，只用维护两个增量更新的变量即可；计算将分布到每一步中去，而不是在episode结束之后才开始。 Sarsa(λ)将上面使用的state value改为action value，即可从TD(λ)算法推广得到Sarsa(λ)算法，直接给出伪代码如下: Sarsa(λ)和n-step Sarsa算法的性能对比可以参考例子Example 12.2: Sarsa(λ) on Mountain Car 同理从true on-line TD(λ)算法推广可以得到on-line Sarsa(λ)算法，伪代码如下： 例子同上。 Stable Off-policy Methods with TracesGTD(λ)GTD算法的Eligibility trace版本，w更新规则： δ： z_t使用accumulating trace： v_t更新公式： GQ(λ)Gradient-TD algorithm for action values with eligibility traces: z_t： v_t的更新和GTD(λ)一致。 HTD(λ)a hybrid state-value algorithm combining aspects of GTD(λ) and TD(λ),it is a strict generalization of TD(λ) to off-policy learning, meaning that if thebehavior policy happens to be the same as the target policy, then HTD(λ) becomes the same as TD(λ), which is not true for GTD(λ). update rule: 可以看到维护了两个Eligibility trace。 Emphatic TD(λ)extension of the one-step Emphatic-TD algorithm from Section 11.8 to eligibility traces(param is Θ):]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter11 counterexample]]></title>
    <url>%2F2018%2F12%2F20%2FChapter11-counterexample%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter11/counterexample.py 通过Baird’s counterexample来分析off-policy的approximation方法的性能。 问题描述这是第11章唯一使用的一个例子，使用了一个MRP如下： 其中虚线dashed action对应从任意state到1-6th state的跳转，实线solid action对应从任意state到7th state的跳转，所有action的reward都是0。off-policy的behavior policy b 选取实线action和虚线action按照概率6/7和1/7，所以由behavior policy产生的next-state的分布是均匀的；target policy π 是只选择solid(实线) action的，所以on-policy 分布更集中于7th state，这里的on-policy分布是指的会产生weight更新的state的分布。 每个state的value都是通过圆圈里的linear function来描述的，如果weight vector是零向量，那么可以精确的推出true value都是0。 通过使用不同的off-policy approximation方法，来讨论off-policy面临的两大挑战，就是和on-policy相比，update target和update distribution的改变，例子里我们可以看到，直接从on-policy推广得到的off-policy，如果没有解决好这两个问题，效果大都差强人意。 引入模块，定义常量123456789101112131415161718192021222324252627282930import numpy as npimport matplotlibimport matplotlib.pyplot as plt%matplotlib inlinefrom tqdm import tqdmfrom mpl_toolkits.mplot3d.axes3d import Axes3D# all states: state 0-5 are upper statesSTATES = np.arange(0, 7)# state 6 is lower stateLOWER_STATE = 6# discount factorDISCOUNT = 0.99# each state is represented by a vector of length 8FEATURE_SIZE = 8FEATURES = np.zeros((len(STATES), FEATURE_SIZE))for i in range(LOWER_STATE): FEATURES[i, i] = 2 FEATURES[i, 7] = 1FEATURES[LOWER_STATE, 6] = 1FEATURES[LOWER_STATE, 7] = 2# all possible actionsDASHED = 0SOLID = 1ACTIONS = [DASHED, SOLID]# reward is always zeroREWARD = 0 定义target policy和behavior policy，并使用step函数来建立agent和environment之间的交互1234567891011121314151617# take @action at @state, return the new statedef step(state, action): if action == SOLID: return LOWER_STATE return np.random.choice(STATES[: LOWER_STATE])# target policydef target_policy(state): return SOLID# behavior policyBEHAVIOR_SOLID_PROBABILITY = 1.0 / 7def behavior_policy(state): if np.random.binomial(1, BEHAVIOR_SOLID_PROBABILITY) == 1: return SOLID return DASHED 使用semi-gradient off-policy TD方法完成一次weight update123456789101112131415161718192021# Semi-gradient off-policy temporal difference# @state: current state# @theta: weight for each component of the feature vector# @alpha: step size# @return: next statedef semi_gradient_off_policy_TD(state, theta, alpha): action = behavior_policy(state) next_state = step(state, action) # get the importance ratio if action == DASHED: rho = 0.0 else: rho = 1.0 / BEHAVIOR_SOLID_PROBABILITY # theta是w1-w8的值，也就是需要学习的weight delta = REWARD + DISCOUNT * np.dot(FEATURES[next_state, :], theta) - \ np.dot(FEATURES[state, :], theta) # ρ×α×δ_t delta *= rho * alpha # derivatives happen to be the same matrix due to the linearity theta += FEATURES[state, :] * delta return next_state 使用semi-gradient DP方法完成一次weight update123456789101112131415161718# Semi-gradient DP# @theta: weight for each component of the feature vector# @alpha: step sizedef semi_gradient_DP(theta, alpha): delta = 0.0 # go through all the states for state in STATES: expected_return = 0.0 # compute bellman error for each state for next_state in STATES: # 使用DP方法，直接使用target policy选择action，所以只选择7th state来完成bellman error的计算 if next_state == LOWER_STATE: expected_return += REWARD + DISCOUNT * np.dot(theta, FEATURES[next_state, :]) bellmanError = expected_return - np.dot(theta, FEATURES[state, :]) # accumulate gradients delta += bellmanError * FEATURES[state, :] # derivatives happen to be the same matrix due to the linearity theta += alpha / len(STATES) * delta 绘制使用semi-gradient off-policy TD和semi-gradient DP方法的weight收敛曲线12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# Figure 11.2(left), semi-gradient off-policy TDdef figure_11_2_left(): # Initialize the theta theta = np.ones(FEATURE_SIZE) theta[6] = 10 alpha = 0.01 steps = 1000 thetas = np.zeros((FEATURE_SIZE, steps)) state = np.random.choice(STATES) for step in tqdm(range(steps)): state = semi_gradient_off_policy_TD(state, theta, alpha) thetas[:, step] = theta for i in range(FEATURE_SIZE): plt.plot(thetas[i, :], label='theta' + str(i + 1)) plt.xlabel('Steps') plt.ylabel('Theta value') plt.title('semi-gradient off-policy TD') plt.legend()# Figure 11.2(right), semi-gradient DPdef figure_11_2_right(): # Initialize the theta theta = np.ones(FEATURE_SIZE) theta[6] = 10 alpha = 0.01 sweeps = 1000 thetas = np.zeros((FEATURE_SIZE, sweeps)) for sweep in tqdm(range(sweeps)): semi_gradient_DP(theta, alpha) thetas[:, sweep] = theta for i in range(FEATURE_SIZE): plt.plot(thetas[i, :], label='theta' + str(i + 1)) plt.xlabel('Sweeps') plt.ylabel('Theta value') plt.title('semi-gradient DP') plt.legend()def figure_11_2(): plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) figure_11_2_left() plt.subplot(2, 1, 2) figure_11_2_right() plt.savefig('./figure_11_2.png') plt.show()# testfigure_11_2() 100%|██████████| 1000/1000 [00:00&lt;00:00, 38275.13it/s] 100%|██████████| 1000/1000 [00:00&lt;00:00, 13016.09it/s] 可见使用semi-gradient off-policy TD和semi-gradient DP方法训练weight均未收敛 使用TDC方法训练1234567891011121314151617181920212223242526272829303132# state distribution for the behavior policySTATE_DISTRIBUTION = np.ones(len(STATES)) / 7# 使用np.diag构造对角矩阵，即矩阵DSTATE_DISTRIBUTION_MAT = np.matrix(np.diag(STATE_DISTRIBUTION))# projection matrix for minimize MSVE# x*inv(x.T*D*x)*(x.T)*D,书P218PROJECTION_MAT = np.matrix(FEATURES) * \ np.linalg.pinv(np.matrix(FEATURES.T) * STATE_DISTRIBUTION_MAT * np.matrix(FEATURES)) * \ np.matrix(FEATURES.T) * \ STATE_DISTRIBUTION_MAT# temporal difference with gradient correction P228# @state: current state# @theta: weight of each component of the feature vector# @weight: auxiliary trace for gradient correction# @alpha: step size of @theta# @beta: step size of @weightdef TDC(state, theta, weight, alpha, beta): action = behavior_policy(state) next_state = step(state, action) # get the importance ratio if action == DASHED: rho = 0.0 else: rho = 1.0 / BEHAVIOR_SOLID_PROBABILITY delta = REWARD + DISCOUNT * np.dot(FEATURES[next_state, :], theta) - \ np.dot(FEATURES[state, :], theta) theta += alpha * rho * (delta * FEATURES[state, :] - DISCOUNT * FEATURES[next_state, :] * np.dot(FEATURES[state, :], weight)) weight += beta * rho * (delta - np.dot(FEATURES[state, :], weight)) * FEATURES[state, :] return next_state 使用expectedTDC训练123456789101112131415161718192021# expected temporal difference with gradient correction# @theta: weight of each component of the feature vector# @weight: auxiliary trace for gradient correction# @alpha: step size of @theta# @beta: step size of @weightdef expected_TDC(theta, weight, alpha, beta): # 可以认为使用了均匀distribution，和第一个DP方法一样，current state并没有真正的改变，即没有agent和environment之间的交互 for state in STATES: # When computing expected update target, if next state is not lower state, importance ratio will be 0, # so we can safely ignore this case and assume next state is always lower state delta = REWARD + DISCOUNT * np.dot(FEATURES[LOWER_STATE, :], theta) - np.dot(FEATURES[state, :], theta) rho = 1 / BEHAVIOR_SOLID_PROBABILITY # Under behavior policy, state distribution is uniform, so the probability for each state is 1.0 / len(STATES) # 这里和sample的TDC的区别主要是前面加上了1/len(STATES)*BEHAVIOR_SOLID_PROBABILITY， # 是为了计算期望值，注意需要考虑x_t和x_t+1的概率，所以使用了条件概率的乘法 expected_update_theta = 1.0 / len(STATES) * BEHAVIOR_SOLID_PROBABILITY * rho * ( delta * FEATURES[state, :] - DISCOUNT * FEATURES[LOWER_STATE, :] * np.dot(weight, FEATURES[state, :])) theta += alpha * expected_update_theta expected_update_weight = 1.0 / len(STATES) * BEHAVIOR_SOLID_PROBABILITY * rho * ( delta - np.dot(weight, FEATURES[state, :])) * FEATURES[state, :] weight += beta * expected_update_weight 使用ETD方法训练1234567891011121314151617181920212223242526272829# interest is 1 for every stateINTEREST = 1# ETD方法也是使用的期望值来训练# expected update of ETD# @theta: weight of each component of the feature vector# @emphasis: current emphasis# @alpha: step size of @theta# @return: expected next emphasisdef expected_emphatic_TD(theta, emphasis, alpha): # we perform synchronous update for both theta and emphasis expected_update = 0 expected_next_emphasis = 0.0 # go through all the states for state in STATES: # compute rho(t-1) if state == LOWER_STATE: rho = 1.0 / BEHAVIOR_SOLID_PROBABILITY else: rho = 0 # update emphasis next_emphasis = DISCOUNT * rho * emphasis + INTEREST expected_next_emphasis += next_emphasis # When computing expected update target, if next state is not lower state, importance ratio will be 0, # so we can safely ignore this case and assume next state is always lower state next_state = LOWER_STATE delta = REWARD + DISCOUNT * np.dot(FEATURES[next_state, :], theta) - np.dot(FEATURES[state, :], theta) expected_update += 1.0 / len(STATES) * BEHAVIOR_SOLID_PROBABILITY * next_emphasis * 1 / BEHAVIOR_SOLID_PROBABILITY * delta * FEATURES[state, :] theta += alpha * expected_update return expected_next_emphasis / len(STATES) 计算RMSVE和RMSPBE123456789101112131415# compute RMSVE for a value function parameterized by @theta# true value function is always 0 in this exampledef compute_RMSVE(theta): return np.sqrt(np.dot(np.power(np.dot(FEATURES, theta), 2), STATE_DISTRIBUTION))# compute RMSPBE for a value function parameterized by @theta# true value function is always 0 in this exampledef compute_RMSPBE(theta): bellman_error = np.zeros(len(STATES)) for state in STATES: for next_state in STATES: if next_state == LOWER_STATE: bellman_error[state] += REWARD + DISCOUNT * np.dot(theta, FEATURES[next_state, :]) - np.dot(theta, FEATURES[state, :]) bellman_error = np.dot(np.asarray(PROJECTION_MAT), bellman_error) return np.sqrt(np.dot(np.power(bellman_error, 2), STATE_DISTRIBUTION)) 绘制图像比较TDC和expected TDC性能1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# Figure 11.6(left), temporal difference with gradient correctiondef figure_11_6_left(): # Initialize the theta theta = np.ones(FEATURE_SIZE) theta[6] = 10 weight = np.zeros(FEATURE_SIZE) alpha = 0.005 beta = 0.05 steps = 1000 thetas = np.zeros((FEATURE_SIZE, steps)) RMSVE = np.zeros(steps) RMSPBE = np.zeros(steps) state = np.random.choice(STATES) for step in tqdm(range(steps)): state = TDC(state, theta, weight, alpha, beta) thetas[:, step] = theta RMSVE[step] = compute_RMSVE(theta) RMSPBE[step] = compute_RMSPBE(theta) for i in range(FEATURE_SIZE): plt.plot(thetas[i, :], label='theta' + str(i + 1)) plt.plot(RMSVE, label='RMSVE') plt.plot(RMSPBE, label='RMSPBE') plt.xlabel('Steps') plt.title('TDC') plt.legend()# Figure 11.6(right), expected temporal difference with gradient correctiondef figure_11_6_right(): # Initialize the theta theta = np.ones(FEATURE_SIZE) theta[6] = 10 weight = np.zeros(FEATURE_SIZE) alpha = 0.005 beta = 0.05 sweeps = 1000 thetas = np.zeros((FEATURE_SIZE, sweeps)) RMSVE = np.zeros(sweeps) RMSPBE = np.zeros(sweeps) for sweep in tqdm(range(sweeps)): expected_TDC(theta, weight, alpha, beta) thetas[:, sweep] = theta RMSVE[sweep] = compute_RMSVE(theta) RMSPBE[sweep] = compute_RMSPBE(theta) for i in range(FEATURE_SIZE): plt.plot(thetas[i, :], label='theta' + str(i + 1)) plt.plot(RMSVE, label='RMSVE') plt.plot(RMSPBE, label='RMSPBE') plt.xlabel('Sweeps') plt.title('Expected TDC') plt.legend()def figure_11_6(): plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) figure_11_6_left() plt.subplot(2, 1, 2) figure_11_6_right() plt.savefig('./figure_11_6.png') plt.show() figure_11_6() 100%|██████████| 1000/1000 [00:00&lt;00:00, 10149.88it/s] 100%|██████████| 1000/1000 [00:00&lt;00:00, 4183.01it/s] 可以看到虽然最后结果收敛到了PBE为0，但是VE仍然是2，即和w=(0,0,0,0,0,0,0,0)的true weight仍然有差距。 绘制图像测试ETD算法性能12345678910111213141516171819202122232425262728# Figure 11.7, expected ETDdef figure_11_7(): # Initialize the theta theta = np.ones(FEATURE_SIZE) theta[6] = 10 alpha = 0.03 sweeps = 1000 thetas = np.zeros((FEATURE_SIZE, sweeps)) RMSVE = np.zeros(sweeps) emphasis = 0.0 for sweep in tqdm(range(sweeps)): emphasis = expected_emphatic_TD(theta, emphasis, alpha) thetas[:, sweep] = theta RMSVE[sweep] = compute_RMSVE(theta) for i in range(FEATURE_SIZE): plt.plot(thetas[i, :], label='theta' + str(i + 1)) plt.plot(RMSVE, label='RMSVE') plt.xlabel('Sweeps') plt.title('emphatic TD') plt.legend() plt.savefig('./figure_11_7.png') plt.show()figure_11_7() 100%|██████████| 1000/1000 [00:00&lt;00:00, 12319.59it/s] 使用ETD方法训练时使用的期望值，可以看到最后VE收敛到了0，但是实际应用时，即实际sample造成的方差会很大，所以这种方法理论收敛但是实际一般不能收敛。]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter11 Off-policy Methods with Approximation]]></title>
    <url>%2F2018%2F12%2F19%2FChapter11-Off-policy-Methods-with-Approximation%2F</url>
    <content type="text"><![CDATA[这章在第9章 On-policy Prediction with Approximation和第10章 On-policy Control with Approximation的基础上，讨论off-policy的approximation方法，主要讲解了off-policy with approximation的预测方法。在tubular方法里，从on-policy方法推广到off-policy方法基本是无痛的，收敛性也可以得到保障，但是这一章会讨论更多关于收敛性和可学习性(Learnable)的话题，理解这些不但可以帮助我们更好的理解off-policy的approximation方法，也可以更好的理解强化学习近似函数训练的本质。 首先提出off-policy的两个挑战，其实是相对于on-policy算法遇到的一些问题。第一个是target of update，因为off-policy方法使用behavior policy产生的数据更新target policy，所以target in update rule需要做一些改动，这个工作是通过重要采样因子完成的；第二个是distribution of updates，这里的distribution该怎么理解？完成更新所使用的state的分布？老实说我并没有get到this，我能理解的只是，一些显而易见的梯度下降的损失函数，比如BE(Bellman Error)，可能不能保证训练结果，也就是weight的收敛，需要考虑其他的最小化目标。那么我对于distribution of update可以这样理解吗，就是最终学习到的weight会导致value function的分布更趋于哪种，是趋于BE最小，还是RE最小，还是PBE最小等等。换句话说，distribution和target，感觉是approximation function method和tabular method的本质区别，前者更新weight，更新的的是整体state(or action) value的分布，或者函数形式，而后者更新的只是某个具体的value的值。所以两个挑战，只是不同方法的不同表现形式吧？ -----------------------------------------我是分割线( _ゝ`)----------------------------------------- 关于上面提到的distribution of update，我好像懂了。。。因为结合section:Examples of Off-policy Divergence我发现，作者认为on-policy更新时候使用的state，即更新使用的state分布，可以保证收敛，但是因为off-policy的更新取决于behavior policy和target policy，所以自然使用的更新state分布和on-policy不一样，所以导致了例子(就那两个state的短例子)的off-policy方法训练的发散。而且Rich Sutton大佬也说了，如果将counterexample中的DP方法的更新distribution改为on-policy的分布，即使用异步DP，结果就收敛了： If we alter just the distribution of DP updates in Baird’s counterexample, from the uniform distribution to the on-policy distribution (which generally requires asynchronous updating), then convergence is guaranteed to a solution with error bounded by (9.14).....The example shows that even the simplest combination of bootstrapping and function approximation can be unstable if the updates are not done according to the on-policy distribution. 所以一开始我对distribution理解看来是错了( _ゝ`)，不过感觉思考的感觉挺有意思就留着吧。。。 Semi-gradient Methods这一部分将前面使用过的off-policy方法推广到approximation function上，主要以semi-gradient的形式，这也是在本书中使用的最多的weight update方法，但是需要注意有的方法可能不会收敛，具体的收敛性证明和改进会在后面的section讨论。 在off-policy方法中，经常使用重要采样因子importance sampling来调整从behavior policy得到的数据，用于target policy的训练，但是approximation function中使用的采样因子形式和在第5章使用的形式不太一样，因为这里使用的semi-gradient方法使用的bootstrape(自举)形式，所以相应的采样因子包含的乘积项也比较少: 和on-policy的update rule相比，off-policy的update rule很相似了： ( _ゝ`) 其中的δ_t和on-policy的一致，分为discount setting和average reward setting两种： 不过也有例外的，比如expected-Sarsa算法就不使用采样因子： 原因很简单，我们更新q(St,At,w)，使用的next action value是使用target policy π distribution的expected值，所以不需要采样因子进行调整。 接着给出了off-policy的n-step Sarsa方法，虽然原文中说是expected sarsa的，但是参考第7章 n-step Off-policy Learning by Importance Sampling感觉说是n-step Sarsa的off-policy更合适： 当然在第7章还提到一个不需要采样因子的n-step off-policy方法，即tree-backup algorithm： Examples of Off-policy Divergence这一部分给出了本章第一个反例，就是直接使用on-policy的approximation方法，只仿照tabular method方法改变target of update，而不考虑distribution of update的改变，得到的训练结果是发散的。而且通过后续section的学习，可以看到VE(semi-gradient TD(0))和BE(semi-gradient DP)作为损失函数，是不可学习的(Non-Learnable)。 首先给出一个包含2个state的MDP的片段，记住这只是一个完整的MDP的一部分，并不是全部，来帮助理解: 圆圈内的数值代表了估计value值，1st state只有一个action，就是转向2nd state，with reward=0。因为是off-policy update，那么设想这样一种完全可能发生的情形： 使用semi-gradient TD(0)来分析，设α=0.1，w的初始值设定为10，那么w更新一次之后是10+0.1*(20-10)=11； 但是因为采样因子可能为0，即behavior policy采取了target policy不可能采取的action，经过了其他一些train后，w的值并没有改变，这时train又重新来到了上图中的state转换； 于是重复的更新发生了第2次，这次w是11+0.1*(22-11)=12.1； 以此类推，继续若干次训练，我们可以看到w值会向无穷的方向发散。 这种情况在off-policy中发生是很常见的，但是on-policy却不存在这样的问题，因为在state转换中，总会存在next state导致w下降，因为有episodic的设定和discount的设定来保证训练的收敛。我们可以在一个完整的例子中看到这个问题的确会发生：Baird’s counterexample with semi-gradient TD(0) or semi-gradient DP 看来，不遵循on-policy的update分布，哪怕最简单的bootstraping method with function approximation都收敛不了。 还有，貌似如果behavior policy和target policy很相似的话(例子里的b和π也差的太离谱了吧QAQ)，使用Q-Learning可以保证收敛？？？据说实验表示可以，但是还没有理论证明。 书中还提到了两种方法来避免不稳定性和发散发生，都没给出具体的例子，而且效果貌似也( _ゝ`)，还是省省脑子看后面的已经证明可用的方法吧。 The Deadly Triad这个标题：死亡三巨头。看后的我是( _ゝ`)的。 通过上面的例子总结得到，下面三个老哥在一起用，就有可能出现不稳定和发散的危险： 而且貌似去掉一种方法就可以避免发散)_(눈_눈」∠)_。那么去掉哪个那？ function approximation肯定不能去掉，靠这个来估计大型state空间问题那，至少也要用个linear function approximation那。 去掉bootstrape当然可行，就是放弃了(小声：大量的)计算的性能( _ゝ`)，还有data efficiency，因为直接使用MC方法可以看到会受到一些不好的数据(噪声)的影响，因为毕竟是一致采样的，但是bootstrape方法引入了自举，之前学习到的value estimate可以在一定程度上衰减那些噪点的影响，一般会通过提高n-step方法中的n来降低bootstrape的使用，但是这个方法也是比较让人难以割舍的。 最后考虑off-policy，大部分问题使用on-policy就可以很好的解决了，那是不是意味着可以取代了off-policy那？本书中不涉及这部分知识，但是现在有并行学习的方法，想要并行学习多个target policy，那当然要用off-policy方法了。 Linear Value-function Geometry这一部分引入了估计估计函数和true value之间的不同误差表示，是后面几部分讨论可学习性和有效off-policy学习算法的基础。 首先考虑一种简单的情况，假设有这样的state空间：S={s1,s2,s3}，对应的function approximation使用2个参数来完成估计：w={w1,w2}，那么估计value可以看做一个三维空间中的点，而对应的估计参数可以看做一个二维平面上的点，如下图所示： ( _ゝ`) 但是图上的符号表示远不止两个空间的表示，这就是接下来要讲的逼近误差的问题。 如果我们假设真实的value function v_π不能用两个分量的w表示，那么就没办法将v_π映射到w的子平面上，那我们就需要了解到，估计函数v_w和目标v_π之间的“误差”有多大？ 首先直觉考虑欧几里得距离，即使用value空间中两个value function v1和v2之间距离v1-v2如何？明显不太合适，因为前面[第九章]在讨论VE的时候就说明了，误差对不同state的敏感度是不同的，所以我们可以使用类似的思路，引入μ(s)~[0,1]来表示误差对不同state的敏感度，并使用如下公式来度量两个value function的距离： 对于每个value function v，找到它对应的w子空间中距离最近的估计函数可以被定义为一种映射操作(projection operation)，并可以通过符号Π来表示如下： 同时对应linear方法，映射操作可以通过矩阵变换的方法实现： 上面的使用映射方法得到估计函数v_w的方法可以通过MC方法渐进训练得到，但是TD方法提出了一种新的方法来考虑误差： 首先考虑自举方法的理论基础——Bellman等式： 如果将右边的v_π换为估计值v_w，然后等式左右作差来得到Bellman error: 可以看到Bellman error是前面提到的TD error的期望值。处理所有state产生的Bellman error，需要下面给出的均方Bellman Error: 通过最小化BE可以在子空间上得到一个使得BE最小的点，但是通过上图可以看到，minBE得到的子空间的点和minVE不是一个点，关于minBE的off-policy训练方法将在后续两个section讨论。 Bellman error是通过Bellman operate得到的，Bellman operate可以定义为如下变换： 但是Bellman operate通过会将w平面的估计函数v_w变换到w平面之外，即无法通过w的linear形式表示。通过DP的方法(without approximation function)来进行Bellman operate时，就像图中的灰色箭头一样，最终Bellman operate会导致v收敛到v_π，即： 但是如果通过function approximation来完成Bellman operate，需要在每次operate之后将w子空间之外的v重新映射到w子平面内，才能重新进行下一次operate，这种Bellman operate+project的方法同样会产生损失函数，我们称之为均方映射Bellman Error(PBE)，公式如下： linear近似方法最小化PBE过程中总是可以得到PBE=0的估计函数，即最后可以收敛到第九章 the section of Linear Methods提到的w_TD。但是这种训练方法在off-policy下并不总是稳定的，后续section将会讨论具体的保证收敛的训练算法。 Stochastic Gradient Descent in the Bellman Error这个section和下一个section将会讨论使用基于BE的梯度下降算法的可学习性。 因为上个section提到，Bellman error是TD error的expected，所以先考虑一个naive的例子，就是使用TD error做损失函数，one-step TD error公式如下： 对应的损失函数TDE格式如下： μ(s)是基于behavior policy的distribution。 对应的w更新公式如下： 这个公式和本章第一次使用的例子的更新公式除了最后一个因子都是一样的，那个式子是通过on-policy最小化VE的目标直接搬过来的，并且已经证明不稳定和发散。但是这个公式是可以保证收敛的，我们称之为naive residual-gradient方法。 虽然在on-policy下，naive residual-gradient稳定收敛，但是收敛结果却不那么令人满意，换句话说，收敛得到的TDE最小的v_w，并不是true value： Minimizing the TDE is naive; by penalizing all TD errors it achieves something more like temporal smoothing than accurate prediction. 下面这个例子解释了这个问题，考虑给定的MDP如下（虽然这会说这个可能不太应景，就是上个section、这个section、下个section，都是基于on-policy讨论的，因为要先讨论可以准确收敛的损失函数）： A等概率跳转到B和C，相应的reward都在图上，灰色方块代表terminal state。那么true value很明显了，通过Bellman Error可以得到v(A)=0.5,v(B)=1,v(C)=0。但是TDE最小的角度考虑，结果却应该是A=1/2，B=3/4,C=1/4，而true value并没有获得最下的TDE。 但是Bellman error是TD error的expected啊，像上面的问题，如果采用期望的方式，得到的true value的Bellman error应该是0（因为TD error一正一负，平方之后再想加就都是正的了，但是Bellman error是先加再平方，一正一负直接抵消了，具体可以比较一下BE和TDE的表达式，还有就是因为markdown我不会打出上划线，那里有那里没有上划线，即均方，就看自己理解吧( _ゝ`) ）。所以将上述naive residual-gradient方法改为expected形式，即将所有sample的值改为expected形式，我们称之为residual-gradient algorithm： 有一个问题，就是关于ρt，为什么从期望符号里面出来就没有v(St,w)前面就没有了？不过考虑到是on-policy情况下评估可学习性，本来给这个更新公式就没什么意义。然后是关于这个更新公式的实际操作，因为SGD嘛，肯定是拿随机抽样来更新的，那不就是和naive版本一样了吗？不过还是有不同的，因为是使用sample值来代替上面式子的最终的公式的expected的，但是有两个expected，所以需要使用不同的sample值，有两种方法： 1、在实际的确定性环境中，采的两次样肯定是一样的，所以直接用一样的sample 2、在模拟的环境中，可以采一次样，然后撤回，再重新采一次样 但是讲了这么多都是废话啊，因为residual-gradient algorithm至少有3点令人不满： 1、收敛慢，毕竟要采样两次 2、residual-gradient algorithm仍然会收敛到错误的value 3、BE是不具有可学习性的 The Bellman Error is Not Learnable这个section讨论了可学习性(learnable)的概念。以往的机器学习中讨论可学习性，是指可以通过指数数量的例子学习得到多项式表达。强化学习中拓展了这个概念，把学习范围扩展到了任意数量的例子。如果一个强化学习中学习的变量可以在给定环境内部结构知识的情况下计算，但不能从观察到的特征向量、动作和奖励序列中计算或估计，就称之为不可学习的。BE事实上就是不可学习的，参考下面的例子： 每个state的两个action都是等概率的，我们可以看到两个不同的MRP产生了相同的reward序列，即随机的0,2序列，所以说state的数目是一个不可学习的对象。 同时可以证明VE也是不可学习的，因为如果使用VE来梯度下降，另γ=0，得到的两个MRP的VE不同，通过同样的reward序列学习到了不同的VE，所以VE也是不可学习的。但是不太一样的是，虽然两个MRP的VE不同，但是学习到的value却是最优的！ 为了解释这个问题，考虑另外一个比较常见的误差，即均方Return error，这个量明显是learnable的。可以看到RE和VE之间存在如下关系： 所以可以总结出来这些不同的优化目标之间的关系： 关于上图有如下解释： 但是相应的BE确实不可学习的，我们讨论的另外的学习目标，如PBE和TDE，可以通过数据决定，即是可学习的。它们之间的关系可以用下图解释： Gradient-TD Methods上面讨论了那么多在off-policy中失效的算法，同时讨论了更多关于强化学习的细节，可以帮助更好的理解问题吧，因为毕竟off-policy的方法现在还是属于开发课题，并没有被很好地解决。 首先考虑使用矩阵形式来表示PBE： 为了使用梯度下降，需要求出PBE的梯度： 接着给出相应的expected形式： 可以看到表达式是3个expected的乘积，其中1st和3rd的因子是相关的，如果直接采样并应用到这两个式子中，就会导致和naive residual-gradient一样，产生有差估计。 另一种想法是分别估计3个因式，但是计算资源消耗太大，尤其是矩阵存储和求逆运算。可以改进：估计两个因子，对剩下一个因子采样，比如可以对后两个因子估计，对第一个进行采样，这样复杂度会从O(d^3)-&gt;O(d^2)。 我们将后两个因子的乘积估计设为v： 并使用如下公式更新v，复杂度为O(d): 并使用1st因子的采样代替expected，得到最简单的w更新公式如下： 这个算法称为GTD2，并可以通过在对后两项替换成v之前，做一些变换来获得改进： 这个改进版本的算法称为TDC。老实说这个公式我也不太理解，ρt感觉也是少了一个啊，为什么作者公式里吸收ρt的操作总是不给个提示？ 例子可以参考counterexample Emphatic-TD MethodsThe one-step emphatic-TD algorithm for learning episodic state values is defined by: I_t,the interest, being arbitrary and M_t, the emphasis, being initialized to M_t−1 = 0. 例子可以参考counterexample 这种方法和TDC完全不一样，原理和第九章 Looking Deeper at On-policy Learning: Interest and Emphasis的内容比较相符。 值得提出的一点是，通过Baird’s counterexample可以看到，TDC方法收敛到PBE为0的optimal value，但是却没有使得VE为0，而是为2，说明这种方法仍然存在偏差(bias)；ETD方法的VE达到0了，但是使用的是expected的值，如果采样sample的话，结果就不会收敛，所以这种方法是一种理论可行但是实际往往难以收敛的方法。 下一部分也就是本章最后一部分会进一步讨论off-policy的variance问题，我们可以看到本章一直使用的例子Baird’s counterexample本身给定的behavior policy和target policy就在一定程度上影响了off-policy算法的性能，可以说是比较糟糕的设定了。 Reducing Variance相较on-policy算法，off-policy算法本身就具有更大的方差(variance)，因为behavior policy和target policy之间不可避免的会存在不同，而这种不同就导致了，学习算法使用的数据和目标policy本身就有不相关性，如果你通过做饭的技巧来学习飙车，能行吗？ 关于这种不相关性，第五章提出的importance sampling ratio可以说是贯穿了本书的off-policy算法，但是正是这个参数会引入较大的方差。我们知道重要采样因子的期望值是1，但是事实上它的实际值却是分布的相当发散，可能会很大也可能会很小。考虑ρ很大的情况下，ρ是和step-size α相乘的，所以这会导致在GD算法中使用了很大的一步梯度下降，这对于GD算法来说可能是致命的，因为过大的步进会导致GD算法陷入不稳定。但是如果将α设置的过小，又会导致训练速度的减慢，所以解决variance问题一种可以考虑的途径就是如果自适应的调整步进α。 以下列举出一些在以往的章节使用的减少variance的方法： 1、第五章使用的weighted importance sampling，但是在function approximation中使用may be a challenge。 2、第七章使用的Tree Backup Algorithm，通过TB算法可以不使用采样因子，这种方法已经被利用到off-policy中开发出了稳定的算法。 3、可以通过behavior policy部分的定义出target policy，这样可以不至于得到过大的采样因子。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter10 access control]]></title>
    <url>%2F2018%2F12%2F15%2FChapter10-access-control%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter10/access_control.py 使用访问控制的例子来测试continuing tasks下使用average reward setting训练action value function approximation的效果。 问题描述有一个永远不会空的customer队列，每位customer都有对应的优先级，每次队列首的customer要求访问server的时候，server可以选择不接受，此时reward=0，也可以选择接受，此时reward根据customer的优先级决定，有1,2,4,8四种，如果server接受了customer的访问请求，那么这台server就判定为busy，不能再接受其余customer的访问请求，每次step每台server都有概率从busy变为free状态；如果没有server自然就会拒绝customer的访问请求。不管队首的customer是否成功访问到server了，本次操作之后customer就从队列剔除，轮到下一位customer来访问server，每个队首的customer的优先级是随机分布的。 这个问题的state由两部分组成：当前可用的server数量和队首的customer的优先级，action对应有接受和拒绝两种。可以使用tiling code来构造特征。 引入模块并定义常量1234567891011121314151617181920212223242526272829303132333435import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdmfrom mpl_toolkits.mplot3d.axes3d import Axes3Dfrom math import floorimport seaborn as sns# possible prioritiesPRIORITIES = np.arange(0, 4)# reward for each priorityREWARDS = np.power(2, np.arange(0, 4))# possible actionsREJECT = 0ACCEPT = 1ACTIONS = [REJECT, ACCEPT]# total number of serversNUM_OF_SERVERS = 10# at each time step, a busy server will be free with probability 0.06PROBABILITY_FREE = 0.06# w的学习率# step size for learning state-action valueALPHA = 0.01# average reward的学习率# step size for learning average rewardBETA = 0.01# probability for explorationEPSILON = 0.1 使用了9.5的Tiling Coding来将(s,a)转换成feature，这里没有使用custom的Tiling Coding算法，使用了Richard S. Sutton的tiling-code software1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859######################################################################## Following are some utilities for tile coding from Rich.# To make each file self-contained, I copied them from# http://incompleteideas.net/tiles/tiles3.py-remove# with some naming convention changes## Tile coding startsclass IHT: "Structure to handle collisions" def __init__(self, size_val): self.size = size_val self.overfull_count = 0 self.dictionary = &#123;&#125; def count(self): return len(self.dictionary) def full(self): return len(self.dictionary) &gt;= self.size def get_index(self, obj, read_only=False): d = self.dictionary if obj in d: return d[obj] elif read_only: return None size = self.size count = self.count() if count &gt;= size: if self.overfull_count == 0: print('IHT full, starting to allow collisions') self.overfull_count += 1 return hash(obj) % self.size else: d[obj] = count return countdef hash_coords(coordinates, m, read_only=False): if isinstance(m, IHT): return m.get_index(tuple(coordinates), read_only) if isinstance(m, int): return hash(tuple(coordinates)) % m if m is None: return coordinatesdef tiles(iht_or_size, num_tilings, floats, ints=None, read_only=False): """returns num-tilings tile indices corresponding to the floats and ints""" if ints is None: ints = [] qfloats = [floor(f * num_tilings) for f in floats] tiles = [] for tiling in range(num_tilings): tilingX2 = tiling * 2 coords = [tiling] b = tiling for q in qfloats: coords.append((q + b) // num_tilings) b += tilingX2 coords.extend(ints) tiles.append(hash_coords(coords, iht_or_size, read_only)) return tiles# Tile coding ends####################################################################### 使用Tiling Coding构造特征并建立linear value function1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# a wrapper class for differential semi-gradient Sarsa state-action functionclass ValueFunction: # In this example I use the tiling software instead of implementing standard tiling by myself # One important thing is that tiling is only a map from (state, action) to a series of indices # It doesn't matter whether the indices have meaning, only if this map satisfy some property # View the following webpage for more information # http://incompleteideas.net/sutton/tiles/tiles3.html # @alpha: step size for learning state-action value # @beta: step size for learning average reward def __init__(self, num_of_tilings, alpha=ALPHA, beta=BETA): self.num_of_tilings = num_of_tilings self.max_size = 2048 self.hash_table = IHT(self.max_size) self.weights = np.zeros(self.max_size) # state features needs scaling to satisfy the tile software self.server_scale = self.num_of_tilings / float(NUM_OF_SERVERS) # 这个地方len(PRIORITIES)-1是因为优先级最大值是3，最小值是0，max-min是3 # 注意分母始终要是state分量的max-min self.priority_scale = self.num_of_tilings / float(len(PRIORITIES) - 1) self.average_reward = 0.0 # divide step size equally to each tiling self.alpha = alpha / self.num_of_tilings self.beta = beta # get indices of active tiles for given state and action def get_active_tiles(self, free_servers, priority, action): active_tiles = tiles(self.hash_table, self.num_of_tilings, [self.server_scale * free_servers, self.priority_scale * priority], [action]) return active_tiles # estimate the value of given state and action without subtracting average def value(self, free_servers, priority, action): active_tiles = self.get_active_tiles(free_servers, priority, action) return np.sum(self.weights[active_tiles]) # estimate the value of given state without subtracting average def state_value(self, free_servers, priority): values = [self.value(free_servers, priority, action) for action in ACTIONS] # if no free server, can't accept if free_servers == 0: return values[REJECT] return np.max(values) # learn with given sequence def learn(self, free_servers, priority, action, new_free_servers, new_priority, new_action, reward): active_tiles = self.get_active_tiles(free_servers, priority, action) estimation = np.sum(self.weights[active_tiles]) # estimation = self.value(free_servers,priority,action) delta = reward - self.average_reward + self.value(new_free_servers, new_priority, new_action) - estimation # update average reward self.average_reward += self.beta * delta delta *= self.alpha for active_tile in active_tiles: self.weights[active_tile] += delta 通过建立的optimal policy来指导agent和environment进行交互123456789101112131415161718192021# get action based on epsilon greedy policy and @valueFunction# 返回使用epsilon-policy得到actiondef get_action(free_servers, priority, value_function): # if no free server, can't accept if free_servers == 0: return REJECT if np.random.binomial(1, EPSILON) == 1: return np.random.choice(ACTIONS) values = [value_function.value(free_servers, priority, action) for action in ACTIONS] return np.random.choice([action_ for action_, value_ in enumerate(values) if value_ == np.max(values)])# take an action# 返回交互结果：free_servers，队列中下一个customer的优先级，action得到的rewarddef take_action(free_servers, priority, action): if free_servers &gt; 0 and action == ACCEPT: free_servers -= 1 reward = REWARDS[priority] * action # some busy servers may become free busy_servers = NUM_OF_SERVERS - free_servers free_servers += np.random.binomial(busy_servers, PROBABILITY_FREE) return free_servers, np.random.choice(PRIORITIES), reward 使用differential semi-gradient Sarsa(0)进行训练1234567891011121314151617181920212223# differential semi-gradient Sarsa# @valueFunction: state value function to learn# @maxSteps: step limit in the continuing taskdef differential_semi_gradient_sarsa(value_function, max_steps): current_free_servers = NUM_OF_SERVERS current_priority = np.random.choice(PRIORITIES) current_action = get_action(current_free_servers, current_priority, value_function) # track the hit for each number of free servers # 0～11 freq = np.zeros(NUM_OF_SERVERS + 1) for _ in tqdm(range(max_steps)): freq[current_free_servers] += 1 new_free_servers, new_priority, reward = take_action(current_free_servers, current_priority, current_action) new_action = get_action(new_free_servers, new_priority, value_function) value_function.learn(current_free_servers, current_priority, current_action, new_free_servers, new_priority, new_action, reward) current_free_servers = new_free_servers current_priority = new_priority current_action = new_action print('asymptotic average reward: ',value_function.average_reward) print('Frequency of number of free servers:') print(freq / max_steps) 绘制图表表现differential value和policy123456789101112131415161718192021222324252627282930313233343536373839# Figure 10.5, Differential semi-gradient Sarsa on the access-control queuing taskdef figure_10_5(): max_steps = int(2e6) # use tile coding with 8 tilings num_of_tilings = 8 value_function = ValueFunction(num_of_tilings) differential_semi_gradient_sarsa(value_function, max_steps) values = np.zeros((len(PRIORITIES), NUM_OF_SERVERS + 1)) for priority in PRIORITIES: for free_servers in range(NUM_OF_SERVERS + 1): values[priority, free_servers] = value_function.state_value(free_servers, priority) fig = plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) for priority in PRIORITIES: plt.plot(range(NUM_OF_SERVERS + 1), values[priority, :], label='priority %d' % (REWARDS[priority])) plt.xlabel('Number of free servers') plt.ylabel('Differential value of best action') plt.legend() ax = fig.add_subplot(2, 1, 2) policy = np.zeros((len(PRIORITIES), NUM_OF_SERVERS + 1)) for priority in PRIORITIES: for free_servers in range(NUM_OF_SERVERS + 1): values = [value_function.value(free_servers, priority, action) for action in ACTIONS] if free_servers == 0: policy[priority, free_servers] = REJECT else: policy[priority, free_servers] = np.argmax(values) fig = sns.heatmap(policy, cmap="YlGnBu", ax=ax, xticklabels=range(NUM_OF_SERVERS + 1), yticklabels=PRIORITIES) fig.set_title('Policy (0 Reject, 1 Accept)') fig.set_xlabel('Number of free servers') fig.set_ylabel('Priority') plt.savefig('./figure_10_5.png') plt.show()figure_10_5() 100%|██████████| 2000000/2000000 [05:48&lt;00:00, 5742.25it/s] asymptotic average reward: 2.7830264537085903 Frequency of number of free servers: [1.219660e-01 2.275715e-01 2.717895e-01 2.138460e-01 1.125485e-01 4.154650e-02 9.331000e-03 1.276000e-03 1.150000e-04 8.000000e-06 2.000000e-06]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter10 mountain car]]></title>
    <url>%2F2018%2F12%2F15%2FChapter10-mountain-car%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter10/mountain_car.py 使用强化学习中经典的例子:mountain car来测试semi-gradient Sarsa算法的性能。 问题描述Mountain Car是强化学习中一个比较经典的例子：一个动力不足的小车想爬上右边的goal，但是他的油门抵不过重力作用，所以它为了冲上右边的高峰，唯一的选择是先到左边坡上，然后想办法往右边冲，其实就是在两边摆，通过自己有限的动能累计来达到goal。 小车的位移和速度由以下的公式决定： bound操作保证了-1.2≤x_t≤0.5 and -0.07≤xdot_t≤0.07，当x到达左边界，速度xdot_t重置为0，当x到达右边界，小车到达goal，结束。每个episode都从x_t∈[-0.6,-0.4]和xdot_t=0开始。action有3种：反向发动=-1，不发动=0，正向发动=1，对应的reward都是-1。 引入模块并定义了常量12345678910111213141516171819202122232425import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdmfrom mpl_toolkits.mplot3d.axes3d import Axes3D# 向下取整from math import floor# all possible actionsACTION_REVERSE = -1ACTION_ZERO = 0ACTION_FORWARD = 1# order is importantACTIONS = [ACTION_REVERSE, ACTION_ZERO, ACTION_FORWARD]# bound for position and velocityPOSITION_MIN = -1.2POSITION_MAX = 0.5VELOCITY_MIN = -0.07VELOCITY_MAX = 0.07# use optimistic initial value, so it's ok to set epsilon to 0# 因为整体的优化value基本是负值，所以缺省为0的初始化value是乐观的，使用完全greedy policy仍然可以保证exploringEPSILON = 0 使用了9.5的Tiling Coding来将(s,a)转换成feature，这里没有使用custom的Tiling Coding算法，使用了Richard S. Sutton的tiling-code software123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960######################################################################## Following are some utilities for tile coding from Rich.# To make each file self-contained, I copied them from# http://incompleteideas.net/tiles/tiles3.py-remove# with some naming convention changes## Tile coding starts# index hash tableclass IHT: "Structure to handle collisions" def __init__(self, size_val): self.size = size_val self.overfull_count = 0 self.dictionary = &#123;&#125; def count(self): return len(self.dictionary) def full(self): return len(self.dictionary) &gt;= self.size def get_index(self, obj, read_only=False): d = self.dictionary if obj in d: return d[obj] elif read_only: return None size = self.size count = self.count() if count &gt;= size: if self.overfull_count == 0: print('IHT full, starting to allow collisions') self.overfull_count += 1 return hash(obj) % self.size else: d[obj] = count return countdef hash_coords(coordinates, m, read_only=False): if isinstance(m, IHT): return m.get_index(tuple(coordinates), read_only) if isinstance(m, int): return hash(tuple(coordinates)) % m if m is None: return coordinatesdef tiles(iht_or_size, num_tilings, floats, ints=None, read_only=False): """returns num-tilings tile indices corresponding to the floats and ints""" if ints is None: ints = [] qfloats = [floor(f * num_tilings) for f in floats] tiles = [] for tiling in range(num_tilings): tilingX2 = tiling * 2 coords = [tiling] b = tiling for q in qfloats: coords.append((q + b) // num_tilings) b += tilingX2 coords.extend(ints) tiles.append(hash_coords(coords, iht_or_size, read_only)) return tiles# Tile coding ends####################################################################### step函数完成agent和环境的交互1234567891011# take an @action at @position and @velocity# @return: new position, new velocity, reward (always -1)def step(position, velocity, action): new_velocity = velocity + 0.001 * action - 0.0025 * np.cos(3 * position) new_velocity = min(max(VELOCITY_MIN, new_velocity), VELOCITY_MAX) new_position = position + new_velocity new_position = min(max(POSITION_MIN, new_position), POSITION_MAX) reward = -1.0 if new_position == POSITION_MIN: new_velocity = 0.0 return new_position, new_velocity, reward 定义value function approximation，使用Tiling Code构造特征12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# wrapper class for state action value functionclass ValueFunction: # In this example I use the tiling software instead of implementing standard tiling by myself # One important thing is that tiling is only a map from (state, action) to a series of indices # It doesn't matter whether the indices have meaning, only if this map satisfy some property # View the following webpage for more information # http://incompleteideas.net/sutton/tiles/tiles3.html # @max_size: the maximum # of indices def __init__(self, step_size, num_of_tilings=8, max_size=2048): self.max_size = max_size self.num_of_tilings = num_of_tilings # divide step size equally to each tiling # 将α按照tiling的数量衰减 self.step_size = step_size / num_of_tilings self.hash_table = IHT(max_size) # weight for each tile self.weights = np.zeros(max_size) # position and velocity needs scaling to satisfy the tile software self.position_scale = self.num_of_tilings / (POSITION_MAX - POSITION_MIN) self.velocity_scale = self.num_of_tilings / (VELOCITY_MAX - VELOCITY_MIN) # get indices of active tiles for given state and action def get_active_tiles(self, position, velocity, action): # 调整scale是为了让给定的state的(s1,s2,...,sn)可以将单位转换为1，即和tile的尺寸一致，这样才能得到覆盖的tile的indice # 不需要考虑绝对值，所以可以做如下近似计算： # I think positionScale * (position - position_min) would be a good normalization. # However positionScale * position_min is a constant, so it's ok to ignore it. active_tiles = tiles(self.hash_table, self.num_of_tilings, [self.position_scale * position, self.velocity_scale * velocity], [action]) return active_tiles # estimate the value of given state and action def value(self, position, velocity, action): if position == POSITION_MAX: return 0.0 active_tiles = self.get_active_tiles(position, velocity, action) return np.sum(self.weights[active_tiles]) # learn with given state, action and target def learn(self, position, velocity, action, target): active_tiles = self.get_active_tiles(position, velocity, action) estimation = np.sum(self.weights[active_tiles]) delta = self.step_size * (target - estimation) for active_tile in active_tiles: self.weights[active_tile] += delta # get # of steps to reach the goal under current state value function # 用来绘制cost-to-go-map def cost_to_go(self, position, velocity): costs = [] for action in ACTIONS: costs.append(self.value(position, velocity, action)) return -np.max(costs) 使用epsilon-greedy policy选择action12345678# get action at @position and @velocity based on epsilon greedy policy and @valueFunctiondef get_action(position, velocity, value_function): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(ACTIONS) values = [] for action in ACTIONS: values.append(value_function.value(position, velocity, action)) return np.random.choice([action_ for action_, value_ in enumerate(values) if value_ == np.max(values)]) - 1 使用n-step semi-gradient Sarsa方法训练control policy123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# semi-gradient n-step Sarsa# @valueFunction: state value function to learn# @n: # of stepsdef semi_gradient_n_step_sarsa(value_function, n=1): # start at a random position around the bottom of the valley current_position = np.random.uniform(-0.6, -0.4) # initial velocity is 0 current_velocity = 0.0 # get initial action current_action = get_action(current_position, current_velocity, value_function) # track previous position, velocity, action and reward positions = [current_position] velocities = [current_velocity] actions = [current_action] rewards = [0.0] # track the time time = 0 # the length of this episode T = float('inf') while True: # go to next time step time += 1 if time &lt; T: # take current action and go to the new state new_postion, new_velocity, reward = step(current_position, current_velocity, current_action) # choose new action new_action = get_action(new_postion, new_velocity, value_function) # track new state and action positions.append(new_postion) velocities.append(new_velocity) actions.append(new_action) rewards.append(reward) if new_postion == POSITION_MAX: T = time # get the time of the state to update update_time = time - n if update_time &gt;= 0: returns = 0.0 # calculate corresponding rewards for t in range(update_time + 1, min(T, update_time + n) + 1): returns += rewards[t] # add estimated state action value to the return if update_time + n &lt;= T: returns += value_function.value(positions[update_time + n], velocities[update_time + n], actions[update_time + n]) # update the state value function if positions[update_time] != POSITION_MAX: value_function.learn(positions[update_time], velocities[update_time], actions[update_time], returns) if update_time == T - 1: break current_position = new_postion current_velocity = new_velocity current_action = new_action return time 绘制semi-gradient Sarsa(0)训练过程形成的cost图像123456789101112131415161718192021222324252627282930313233343536373839404142# print learned cost to godef print_cost(value_function, episode, ax): grid_size = 40 positions = np.linspace(POSITION_MIN, POSITION_MAX, grid_size) # positionStep = (POSITION_MAX - POSITION_MIN) / grid_size # positions = np.arange(POSITION_MIN, POSITION_MAX + positionStep, positionStep) # velocityStep = (VELOCITY_MAX - VELOCITY_MIN) / grid_size # velocities = np.arange(VELOCITY_MIN, VELOCITY_MAX + velocityStep, velocityStep) velocities = np.linspace(VELOCITY_MIN, VELOCITY_MAX, grid_size) axis_x = [] axis_y = [] axis_z = [] for position in positions: for velocity in velocities: axis_x.append(position) axis_y.append(velocity) axis_z.append(value_function.cost_to_go(position, velocity)) ax.scatter(axis_x, axis_y, axis_z) ax.set_xlabel('Position') ax.set_ylabel('Velocity') ax.set_zlabel('Cost to go') ax.set_title('Episode %d' % (episode + 1))# Figure 10.1, cost to go in a single rundef figure_10_1(): episodes = 9000 plot_episodes = [0, 99, episodes - 1] fig = plt.figure(figsize=(40, 10)) axes = [fig.add_subplot(1, len(plot_episodes), i+1, projection='3d') for i in range(len(plot_episodes))] num_of_tilings = 8 alpha = 0.3 value_function = ValueFunction(alpha, num_of_tilings) for ep in tqdm(range(episodes)): semi_gradient_n_step_sarsa(value_function) if ep in plot_episodes: print_cost(value_function, ep, axes[plot_episodes.index(ep)]) plt.savefig('./figure_10_1.png') plt.show()figure_10_1() 100%|██████████| 9000/9000 [03:37&lt;00:00, 41.37it/s] semi-gradient Sarsa(0)在不同α值下的性能12345678910111213141516171819202122232425262728# Figure 10.2, semi-gradient Sarsa with different alphasdef figure_10_2(): runs = 10 episodes = 500 num_of_tilings = 8 alphas = [0.1, 0.2, 0.5] steps = np.zeros((len(alphas), episodes)) for run in range(runs): value_functions = [ValueFunction(alpha, num_of_tilings) for alpha in alphas] for index in range(len(value_functions)): for episode in tqdm(range(episodes)): step = semi_gradient_n_step_sarsa(value_functions[index]) steps[index, episode] += step steps /= runs for i in range(0, len(alphas)): plt.plot(steps[i], label='alpha = '+str(alphas[i])+'/'+str(num_of_tilings)) plt.xlabel('Episode') plt.ylabel('Steps per episode') plt.yscale('log') plt.legend() plt.savefig('./figure_10_2.png') plt.show()figure_10_2() 100%|██████████| 500/500 [00:20&lt;00:00, 23.95it/s] 100%|██████████| 500/500 [00:17&lt;00:00, 39.39it/s] 100%|██████████| 500/500 [00:15&lt;00:00, 39.90it/s] 100%|██████████| 500/500 [00:21&lt;00:00, 23.78it/s] 100%|██████████| 500/500 [00:17&lt;00:00, 28.72it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 38.00it/s] 100%|██████████| 500/500 [00:22&lt;00:00, 22.57it/s] 100%|██████████| 500/500 [00:18&lt;00:00, 26.99it/s] 100%|██████████| 500/500 [00:15&lt;00:00, 32.92it/s] 100%|██████████| 500/500 [00:21&lt;00:00, 23.45it/s] 100%|██████████| 500/500 [00:18&lt;00:00, 27.60it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 33.99it/s] 100%|██████████| 500/500 [00:20&lt;00:00, 36.76it/s] 100%|██████████| 500/500 [00:17&lt;00:00, 29.05it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 33.48it/s] 100%|██████████| 500/500 [00:20&lt;00:00, 38.90it/s] 100%|██████████| 500/500 [00:17&lt;00:00, 28.14it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 33.89it/s] 100%|██████████| 500/500 [00:20&lt;00:00, 24.13it/s] 100%|██████████| 500/500 [00:17&lt;00:00, 28.94it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 34.23it/s] 100%|██████████| 500/500 [00:20&lt;00:00, 23.97it/s] 100%|██████████| 500/500 [00:17&lt;00:00, 29.15it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 42.99it/s] 100%|██████████| 500/500 [00:20&lt;00:00, 35.01it/s] 100%|██████████| 500/500 [00:17&lt;00:00, 29.30it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 34.86it/s] 100%|██████████| 500/500 [00:20&lt;00:00, 24.11it/s] 100%|██████████| 500/500 [00:17&lt;00:00, 28.32it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 33.42it/s] 比较Sarsa(0)和 n-step Sarsa的性能1234567891011121314151617181920212223242526272829# Figure 10.3, one-step semi-gradient Sarsa vs multi-step semi-gradient Sarsadef figure_10_3(): runs = 10 episodes = 500 num_of_tilings = 8 alphas = [0.5, 0.3] n_steps = [1, 8] steps = np.zeros((len(alphas), episodes)) for run in range(runs): value_functions = [ValueFunction(alpha, num_of_tilings) for alpha in alphas] for index in range(len(value_functions)): for episode in tqdm(range(episodes)): step = semi_gradient_n_step_sarsa(value_functions[index], n_steps[index]) steps[index, episode] += step steps /= runs for i in range(0, len(alphas)): plt.plot(steps[i], label='n = %.01f' % (n_steps[i])) plt.xlabel('Episode') plt.ylabel('Steps per episode') plt.yscale('log') plt.legend() plt.savefig('./figure_10_3.png') plt.show() figure_10_3() 100%|██████████| 500/500 [00:14&lt;00:00, 34.02it/s] 100%|██████████| 500/500 [00:12&lt;00:00, 39.23it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 34.70it/s] 100%|██████████| 500/500 [00:13&lt;00:00, 36.34it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 33.99it/s] 100%|██████████| 500/500 [00:15&lt;00:00, 38.11it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 33.94it/s] 100%|██████████| 500/500 [00:12&lt;00:00, 41.27it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 33.72it/s] 100%|██████████| 500/500 [00:12&lt;00:00, 40.05it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 34.96it/s] 100%|██████████| 500/500 [00:12&lt;00:00, 39.89it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 34.15it/s] 100%|██████████| 500/500 [00:12&lt;00:00, 46.37it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 33.76it/s] 100%|██████████| 500/500 [00:12&lt;00:00, 40.73it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 34.70it/s] 100%|██████████| 500/500 [00:12&lt;00:00, 41.34it/s] 100%|██████████| 500/500 [00:14&lt;00:00, 42.82it/s] 100%|██████████| 500/500 [00:12&lt;00:00, 47.91it/s] 比较α值和bootstrape的n值对semi-gradient Sarsa方法性能的影响123456789101112131415161718192021222324252627282930313233343536# Figure 10.4, effect of alpha and n on multi-step semi-gradient Sarsadef figure_10_4(): alphas = np.arange(0.25, 1.75, 0.25) n_steps = np.power(2, np.arange(0, 5)) episodes = 50 runs = 5 max_steps = 300 steps = np.zeros((len(n_steps), len(alphas))) for run in range(runs): for n_step_index, n_step in enumerate(n_steps): for alpha_index, alpha in enumerate(alphas): if (n_step == 8 and alpha &gt; 1) or \ (n_step == 16 and alpha &gt; 0.75): # In these cases it won't converge, so ignore them steps[n_step_index, alpha_index] += max_steps * episodes continue value_function = ValueFunction(alpha) for episode in tqdm(range(episodes)): step = semi_gradient_n_step_sarsa(value_function, n_step) steps[n_step_index, alpha_index] += step # average over independent runs and episodes steps /= runs * episodes for i in range(0, len(n_steps)): plt.plot(alphas, steps[i, :], label='n = '+str(n_steps[i])) plt.xlabel('alpha * number of tilings(8)') plt.ylabel('Steps per episode') plt.ylim([220, max_steps]) plt.legend() plt.savefig('./figure_10_4.png') plt.show() figure_10_4() 100%|██████████| 50/50 [00:04&lt;00:00, 12.42it/s] 100%|██████████| 50/50 [00:03&lt;00:00, 24.05it/s] 100%|██████████| 50/50 [00:02&lt;00:00, 18.91it/s] 100%|██████████| 50/50 [00:02&lt;00:00, 18.59it/s] 100%|██████████| 50/50 [00:02&lt;00:00, 19.42it/s] 100%|██████████| 50/50 [00:02&lt;00:00, 19.35it/s] 100%|██████████| 50/50 [00:03&lt;00:00, 13.55it/s] 100%|██████████| 50/50 [00:02&lt;00:00, 18.42it/s] 100%|██████████| 50/50 [00:02&lt;00:00, 19.95it/s] 100%|██████████| 50/50 [00:02&lt;00:00, 20.90it/s] 100%|██████████| 50/50 [00:02&lt;00:00, 20.82it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter10 On-policy Control with Approximation]]></title>
    <url>%2F2018%2F12%2F15%2FChapter10-On-policy-Control-with-Approximation%2F</url>
    <content type="text"><![CDATA[上一章讲了value function approximation的评估问题，那么将固定的policy转换为需要优化的optimal policy，再加上policy improvement就可以完成控制问题。本章使用on-policy GPI的模式来考虑基于approximation的on-policy control问题。 Episodic Semi-gradient Control因为上一章使用的都是state value approximation，所以首先给出action value approximation更新的公式，其实很类似的： 然后给出最常用的on-policy 控制方法——Sarsa的one-step形式的weight更新公式： 接着需要考虑的就是policy-improvement和action-selection问题了。如果action是连续的，或者可选的action空间比较大，这样的问题现在还在研究中；如果action的选择是从有限的离散空间的，那么可以使用以往的选择方法，比如最常用的epsilon-greedy方法。对应的伪代码如下： 这里有一个强化学习的基本例子mountain car，可以帮助理解approximation的on-policy control问题。 n-step Semi-gradient Sarsa没什么悬念，这里接着给出了n-step的 Sarsa算法，首先看一下对应的weight更新公式： Average Reward: A New Problem Setting for Continuing Tasks这个section引入了适用于MDP问题的第三个经典设置，前两个分别是episodic和discount，第三个是average reward，这个设置和discount一样都是针对continuing问题的，即MDP问题没有start state也没有terminal state，下面给出average reward r(π)的定义： 关于极限收敛的条件，需要满足如下假设： 可以看到expectation是基于一系列先前的action，A0,A1,…,A_{t-1}，这些action都是基于policy π的。关于policy π，有一个重要的指标就是基于π的稳定state分布μ_π(s): r(π)极限收敛的假设是，上述state分布μ_π(s)必须独立于S0。这表明MDP无论从哪里开始，或者有什么先前的交互发生在agent和environment之间，这都只具有临时的作用，从长远来看的话，只有policy π才能决定稳定状态下每个state的分布。 这种假设称为ergodicity，即各态历经性。 我关于average reward是这样理解的：因为average reward的收敛是基于任意start都可以达到distribution最终收敛的假设的，所以如果训练中引入average reward的更新，如果训练r(π)收敛的话，也就保证了continuing task的收敛，这种思路和discount的引入一个道理，都是使得原本无穷的训练任务可以达到收敛。 接着引入使用average reward的return，即differential return： 根据differential return建立起来的value function称为differential value function，只需要在原来的更新公式上引入一些小的改动即可得到differential value function，下面给出引入average reward的TD errors: 使用同样的weight更新公式即可，但是需要同时更新average reward的估计，下面给出differential的semi-gradient Sarsa(0)算法伪代码： 关于differential semi-gradient Sarsa(0)的例子可以参考Example 10.2: An Access-Control Queuing Task Deprecating the Discounted Setting首先证明了，如果在continuing tasks中引入discount因子，得到的平均average reward和undiscount效果是一样的： 可以看到引入discount因子不过是给average reward加了一个比例因子，对于得到optimal的average reward没有什么影响。至于为什么不能使用discount因子，书中讲的比较模糊，给出的观点是discount setting不能满足policy improvement的理论，以至于在Sarsa的策略提升的内容上，虽然使用了epsilon-greedy的policy，仍然不能保证这种提升是对全体state的提升。 关于不满足policy improvement理论这点不难理解，因为之前在tabular method的时候，每次提升只改变一个state的policy决策，这种policy提升并没有影响到别的policy，所以整体policy是提升的；但是使用function approximation的policy improvement，因为每次更新是改变的参数，换句话说，每次提升改变了所以state的policy决策，这就不能使用4.2的理论来解释了。 但是这个问题是function approximation的共性，episodic setting和average reward setting也同样存在啊。。。所以只能说function approximation的控制方法目前来说还是缺乏理论基础的。 n-step Differential Semi-gradient Sarsa给出使用n-step bootstrape的Sarsa return和error: 给出使用n-step bootstrape提升的semi-gradient Sarsa方法伪代码:]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter09 square_wave]]></title>
    <url>%2F2018%2F12%2F12%2FChapter09-square-wave%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter09/square_wave.py 使用Coarse Coding方法构造feature来比较不同参数对近似函数性能的影响 问题描述这个例子是书上的Example 9.3: Coarseness of Coarse Coding： 使用Coarse Coding的方法建立近似函数去近似一个方波函数，即将方波函数的随机采样作为U_t来使用，通过修改区间之间的间隔、尺寸等参数来比较不同参数对Coarse Coding特征的泛化特性的影响。 9.5讲的几个feature构造方法，并不只是用于强化学习，在函数拟合也是可以适用的，即回归问题上也是适用的。话说本来value function approximation就是监督学习的思想。。。所以本质不要搞混了。 引入模块12345import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm 定义区间类12345678910111213141516171819# wrapper class for an interval# readability is more important than efficiency, so I won't use many tricksclass Interval: # [@left, @right) def __init__(self, left, right): self.left = left self.right = right # whether a point is in this interval def contain(self, x): return self.left &lt;= x &lt; self.right # length of this interval def size(self): return self.right - self.left# domain of the square wave, [0, 2)DOMAIN = Interval(0.0, 2.0) 定义需要估计的方波波形，以及随机抽样函数123456789101112131415# square wave functiondef square_wave(x): if 0.5 &lt; x &lt; 1.5: return 1 return 0# get @n samples randomly from the square wave# 返回长度为n的抽样序列def sample(n): samples = [] for i in range(0, n): x = np.random.uniform(DOMAIN.left, DOMAIN.right) y = square_wave(x) samples.append([x, y]) return samples 定义Coarse Coding类来建立近似函数和更新方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546# wrapper class for value functionclass ValueFunction: # @domain: domain of this function, an instance of Interval # @alpha: basic step size for one update def __init__(self, feature_width, domain=DOMAIN, alpha=0.2, num_of_features=50): self.feature_width = feature_width self.num_of_featrues = num_of_features self.features = [] self.alpha = alpha self.domain = domain # 选择了一种方法来放置feature，也就是区间Interval # there are many ways to place those feature windows, # following is just one possible way # num_of)feature需要大于1 step = (domain.size() - feature_width) / (num_of_features - 1) left = domain.left for i in range(0, num_of_features - 1): self.features.append(Interval(left, left + feature_width)) left += step self.features.append(Interval(left, domain.right)) # initialize weight for each feature self.weights = np.zeros(num_of_features) # for point @x, return the indices of corresponding feature windows def get_active_features(self, x): active_features = [] for i in range(0, len(self.features)): if self.features[i].contain(x): active_features.append(i) return active_features # estimate the value for point @x def value(self, x): active_features = self.get_active_features(x) # 所有active-feature的weight的总和 return np.sum(self.weights[active_features]) # update weights given sample of point @x # @delta: y - x，这里的delta并没有包含step-size，总感觉这个代码和1000-state不是一个作者。。。 def update(self, delta, x): active_features = self.get_active_features(x) delta *= self.alpha / len(active_features) for index in active_features: self.weights[index] += delta 训练并绘制图表，比较不同参数的性能，这里只修改了样本数量和feature的width12345678910111213141516171819202122232425262728# train @value_function with a set of samples @samplesdef approximate(samples, value_function): for x, y in samples: delta = y - value_function.value(x) value_function.update(delta, x)# Figure 9.8def figure_9_8(): num_of_samples = [10, 40, 160, 640, 2560, 10240] feature_widths = [0.2, 0.4, 1.0] plt.figure(figsize=(30, 20)) axis_x = np.arange(DOMAIN.left, DOMAIN.right, 0.02) for index, num_of_sample in enumerate(num_of_samples): print(num_of_sample, 'samples') samples = sample(num_of_sample) value_functions = [ValueFunction(feature_width) for feature_width in feature_widths] plt.subplot(2, 3, index + 1) plt.title('%d samples' % (num_of_sample)) for value_function in value_functions: approximate(samples, value_function) values = [value_function.value(x) for x in axis_x] plt.plot(axis_x, values, label='feature width %.01f' % (value_function.feature_width)) plt.legend() plt.savefig('./figure_9_8.png') plt.show() figure_9_8() 10 samples 40 samples 160 samples 640 samples 2560 samples 10240 samples 可以看到feature-width对训练结果影响很大，width大对应board feature，泛化范围广，曲线较平坦；width小对应narrow feature，泛化范围窄，曲线毛刺比较多。总体的渐进效果影响不大，但是对具体state的泛化影响就比较大了。]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter09 1000-state Random Walk]]></title>
    <url>%2F2018%2F12%2F08%2FChapter09-1000-state-Random-Walk%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter09/random_walk.py 通过1000-state MRP的例子比较了linear近似函数的不同feature构造方法对应的value function approximation强化学习算法性能 问题描述这个问题是Examples 6.2 random-walk的扩展: (1)状态数扩展到了1000，状态序列[1,1000]，开始状态是500 (2)State transitions are from the current state to one of the 100 neighboring states to its left, or to one of the 100 neighboring states to its right, all with equal probability. (3)if the current state is near an edge, then there may be fewer than 100 neighbors on that side of it. In this case, all the probability that would have gone into those missing neighbors goes into the probability of terminating on that side (thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right). (4)在左边达到terminal-state的reward是-1，在右边达到terminal-state的reward是+1 引入模块并定义常量1234567891011121314151617181920212223242526import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm# # of states except for terminal statesN_STATES = 1000# all states expect terminal stateSTATES = np.arange(1, N_STATES + 1)# start from a central stateSTART_STATE = 500# terminal statesEND_STATES = [0, N_STATES + 1]# possible actionsACTION_LEFT = -1ACTION_RIGHT = 1ACTIONS = [ACTION_LEFT, ACTION_RIGHT]# maximum stride for an actionSTEP_RANGE = 100 定义函数计算真实的state-value，结果是近似直线，只有在直线两端有非线性部分12345678910111213141516171819202122232425262728293031def compute_true_value(): # true state value, just a promising guess true_value = np.arange(-1001, 1003, 2) / 1001.0 # Dynamic programming to find the true state values, based on the promising guess above # Assume all rewards are 0, given that we have already given value -1 and 1 to terminal states while True: # deep copy old_value = np.copy(true_value) for state in STATES: true_value[state] = 0 for action in ACTIONS: for step in range(1, STEP_RANGE + 1): step *= action next_state = state + step next_state = max(min(next_state, N_STATES + 1), 0) # asynchronous update for faster convergence true_value[state] += 1.0 / (2 * STEP_RANGE) * true_value[next_state] error = np.sum(np.abs(old_value - true_value)) if error &lt; 1e-2: break # correct the state value for terminal states to 0 # 注意这里左右的terminal-state的true_value都是0，第六章里的代码为了计算Monte-Carlo方法方便把true_value[-1]设置成了1 # 虽然结果来说无伤大雅，因为如果terminal-state value=1，那么可以将reward都设置为0；如果将terminal-state value设置为0 # 那么需要引入非零的reward # 但是terminal-state value到底是0还是不是0，计算可以随意，概念上看来是没有明确规定了 true_value[0] = true_value[-1] = 0 return true_valuetrue_value = compute_true_value() 定义step函数来模拟单步交互，定义get_action函数使用随机policy选择action12345678910111213141516171819# take an @action at @state, return new state and reward for this transitiondef step(state, action): step = np.random.randint(1, STEP_RANGE + 1) step *= action state += step state = max(min(state, N_STATES + 1), 0) if state == 0: reward = -1 elif state == N_STATES + 1: reward = 1 else: reward = 0 return state, reward# get an action, following random policydef get_action(): if np.random.binomial(1, 0.5) == 1: return 1 return -1 定义State Aggregation的function approximation1234567891011121314151617181920212223# a wrapper class for aggregation value functionclass ValueFunction: # @num_of_groups: # of aggregations def __init__(self, num_of_groups): self.num_of_groups = num_of_groups self.group_size = N_STATES // num_of_groups # w self.params = np.zeros(num_of_groups) # get the value of @state def value(self, state): if state in END_STATES: return 0 group_index = (state - 1) // self.group_size return self.params[group_index] # update parameters # @delta: step size * (target - old estimation) # @state: state of current sample def update(self, delta, state): group_index = (state - 1) // self.group_size self.params[group_index] += delta 定义使用近似函数的Monte-Carlo方法来训练value function123456789101112131415161718192021222324252627# gradient Monte Carlo algorithm# @value_function: an instance of class ValueFunction# @alpha: step size# @distribution: array to store the distribution statisticsdef gradient_monte_carlo(value_function, alpha, distribution=None): state = START_STATE trajectory = [state] # We assume gamma = 1, so return is just the same as the latest reward reward = 0.0 # 模拟一个episode，得到state序列 while state not in END_STATES: action = get_action() next_state, reward = step(state, action) trajectory.append(next_state) state = next_state # Gradient update for each state in this trajectory # 因为每一步reward都是0，只有达到terminal-state才会产生非零reward，所以G_t = reward # 这个地方的梯度更新和公式(9.7)稍微有点出入，主要是由于这里的function approximation采用了状态聚合的形式 # 即维度为10的w对应了10个state group，函数求导的时候，只有state对应的group对应的w分量才会更新，并且这里将导数设置成了 # 0-1函数，所以这样的梯度下降公式可以简化为每次只更新一个对应的w分量 for state in trajectory[:-1]: delta = alpha * (reward - value_function.value(state)) value_function.update(delta, state) if distribution is not None: distribution[state] += 1 绘制图像，比较使用SGD的MC方法的预测value function和true_value的区别，以及state的分布12345678910111213141516171819202122232425262728293031323334# Figure 9.1, gradient Monte Carlo algorithmdef figure_9_1(true_value): episodes = int(1e5) alpha = 2e-5 # we have 10 aggregations in this example, each has 100 states value_function = ValueFunction(10) distribution = np.zeros(N_STATES + 2) for ep in tqdm(range(episodes)): gradient_monte_carlo(value_function, alpha, distribution) distribution /= np.sum(distribution) state_values = [value_function.value(i) for i in STATES] plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) plt.plot(STATES, state_values, label='Approximate MC value') plt.plot(STATES, true_value[1: -1], label='True value') plt.xlabel('State') plt.ylabel('Value') plt.legend() plt.subplot(2, 1, 2) plt.plot(STATES, distribution[1: -1], label='State distribution') plt.xlabel('State') plt.ylabel('Distribution') plt.legend() plt.savefig('./figure_9_1.png') plt.show() # testfigure_9_1(true_value) 100%|██████████| 100000/100000 [01:26&lt;00:00, 1156.28it/s] 可以通过distribution解释更多关于function approximation的细节。最明显的就是estimate value的左下角和右上角，左下角的值基本都比true_value大，右上角的基本都比true_value小。因为在一个state group中，相对的distribution占得越多的state对聚合value的更新贡献越大。可以看到distribution越往中间值越大，但是越靠中间的group内的states的相对distribution就比较接近了，所以estimate value在靠中间的state上基本是均匀分布在true_value附近的。但是两头的state因为group内的相对distribution差别比较大，所以就出现了相对true_value的明显偏差。 定义使用近似函数的n-step TD方法训练，使用semi-gradient1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# semi-gradient n-step TD algorithm# @valueFunction: an instance of class ValueFunction# @n: # of steps# @alpha: step sizedef semi_gradient_temporal_difference(value_function, n, alpha): # initial starting state state = START_STATE # arrays to store states and rewards for an episode # space isn't a major consideration, so I didn't use the mod trick # mod trick指的是只存储n+1个state，通过观察P169给出的伪代码，更新始终只使用了n个reward以及存储的index(mod n+1之前)最大的 # state，所以可以采用循环队列的方法来存储reward和state序列， # 但是我觉得应该只用mod n就够了，以伪代码的符号为例来解释： # 当t=n-1时，τ=0，进入第二部分循环，此时已经存储S1-Sn，共n个state以及n个reward # 第二部分循环只使用了R1-Rn和Sn，所以长度n的循环队列理论上就够了 states = [state] rewards = [0] # track the time time = 0 # the length of this episode T = float('inf') while True: # go to next time step time += 1 if time &lt; T: # choose an action randomly action = get_action() next_state, reward = step(state, action) # store new state and new reward states.append(next_state) rewards.append(reward) if next_state in END_STATES: T = time # get the time of the state to update update_time = time - n if update_time &gt;= 0: returns = 0.0 # calculate corresponding rewards for t in range(update_time + 1, min(T, update_time + n) + 1): returns += rewards[t] # add state value to the return if update_time + n &lt;= T: returns += value_function.value(states[update_time + n]) state_to_update = states[update_time] # update the value function # update the w，but not value table if not state_to_update in END_STATES: delta = alpha * (returns - value_function.value(state_to_update)) value_function.update(delta, state_to_update) if update_time == T - 1: break state = next_state 绘制图表，观察n-step TD方法的效果，以及不同n取值对rms error的影响123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# semi-gradient TD on 1000-state random walk，使用TD(0)方法def figure_9_2_left(true_value): episodes = int(1e5) alpha = 2e-4 value_function = ValueFunction(10) for ep in tqdm(range(episodes)): semi_gradient_temporal_difference(value_function, 1, alpha) stateValues = [value_function.value(i) for i in STATES] plt.plot(STATES, stateValues, label='Approximate TD value') plt.plot(STATES, true_value[1: -1], label='True value') plt.xlabel('State') plt.ylabel('Value') plt.legend()# different alphas and steps for semi-gradient TDdef figure_9_2_right(true_value): # all possible steps steps = np.power(2, np.arange(0, 10)) # all possible alphas alphas = np.arange(0, 1.1, 0.1) # each run has 10 episodes episodes = 10 # perform 100 independent runs runs = 100 # track the errors for each (step, alpha) combination errors = np.zeros((len(steps), len(alphas))) for run in tqdm(range(runs)): for step_ind, step in zip(range(len(steps)), steps): for alpha_ind, alpha in zip(range(len(alphas)), alphas): # we have 20 aggregations in this example value_function = ValueFunction(20) for ep in range(0, episodes): semi_gradient_temporal_difference(value_function, step, alpha) # calculate the RMS error state_value = np.asarray([value_function.value(i) for i in STATES]) errors[step_ind, alpha_ind] += np.sqrt(np.sum(np.power(state_value - true_value[1: -1], 2)) / N_STATES) # take average errors /= episodes * runs # truncate the error for i in range(len(steps)): plt.plot(alphas, errors[i, :], label='n = ' + str(steps[i])) plt.xlabel('alpha') plt.ylabel('RMS error') plt.ylim([0.25, 0.55]) plt.legend()def figure_9_2(true_value): plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) figure_9_2_left(true_value) plt.subplot(2, 1, 2) figure_9_2_right(true_value) plt.savefig('./figure_9_2.png') plt.show()# testfigure_9_2(true_value) 100%|██████████| 100000/100000 [01:32&lt;00:00, 1077.20it/s] 100%|██████████| 100/100 [04:50&lt;00:00, 2.87s/it] 下面几个例子分别使用了不同的特征模型(Feature Construction)来完成VFA(value function approximation)，分别对应了 9.5的polynomial / Fourier基函数模型、Tilings-Code模型，使用的强化学习方法是n-step TD方法Tile Coding特征的linear function approximation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# a wrapper class for tile coding value functionclass TilingsValueFunction: # @num_of_tilings: # of tilings # @tileWidth: each tiling has several tiles, this parameter specifies the width of each tile # @tilingOffset: specifies how tilings are put together def __init__(self, numOfTilings, tileWidth, tilingOffset): self.numOfTilings = numOfTilings self.tileWidth = tileWidth self.tilingOffset = tilingOffset # To make sure that each sate is covered by same number of tiles, # we need one more tile for each tiling # self.tilingSize 指的是每个tiling包含的tile的数量，+1是为了保证tiling空间大于state空间，保证每个 # state都被相同数量的tile覆盖 self.tilingSize = N_STATES // tileWidth + 1 # weight for each tile self.params = np.zeros((self.numOfTilings, self.tilingSize)) # For performance, only track the starting position for each tiling # 只使用tiling的起始点和对应偏移来表示tiling # As we have one more tile for each tiling, the starting position will be negative self.tilings = np.arange(-tileWidth + 1, 0, tilingOffset) # get the value of @state def value(self, state): stateValue = 0.0 # go through all the tilings for tilingIndex in range(0, len(self.tilings)): # find the active tile in current tiling tileIndex = (state - self.tilings[tilingIndex]) // self.tileWidth # estimate value是所有tile对应的weight的求和，可以认为是state聚合的拓展，具有更强的泛化能力 stateValue += self.params[tilingIndex, tileIndex] return stateValue # update parameters # @delta: step size * (target - old estimation) # @state: state of current sample def update(self, delta, state): # each state is covered by same number of tilings # so the delta should be divided equally into each tiling (tile) # 这个地方对delta的缩放主要是为了针对tiling codeing缩放step-size α，书上有对应的解释P176 Figure 9.10 delta /= self.numOfTilings # go through all the tilings for tilingIndex in range(0, len(self.tilings)): # find the active tile in current tiling tileIndex = (state - self.tilings[tilingIndex]) // self.tileWidth self.params[tilingIndex, tileIndex] += delta # Figure 9.10, it will take quite a whiledef figure_9_10(true_value): # My machine can only afford one run, thus the curve isn't so smooth runs = 1 # number of episodes episodes = 5000 num_of_tilings = 50 # each tile will cover 200 states tile_width = 200 # how to put so many tilings tiling_offset = 4 labels = ['tile coding (50 tilings)', 'state aggregation (one tiling)'] # track errors for each episode errors = np.zeros((len(labels), episodes)) for run in range(runs): # initialize value functions for multiple tilings and single tiling value_functions = [TilingsValueFunction(num_of_tilings, tile_width, tiling_offset), ValueFunction(N_STATES // tile_width)] for i in range(len(value_functions)): for episode in tqdm(range(episodes)): # I use a changing alpha according to the episode instead of a small fixed alpha # With a small fixed alpha, I don't think 5000 episodes is enough for so many # parameters in multiple tilings. # The asymptotic performance for single tiling stays unchanged under a changing alpha, # however the asymptotic performance for multiple tilings improves significantly # 递减的alpha使得训练在一开始更重视当前target，加快训练速度 # 后期的训练更重视经验，有效收敛 alpha = 1.0 / (episode + 1) # gradient Monte Carlo algorithm gradient_monte_carlo(value_functions[i], alpha) # get state values under current value function state_values = [value_functions[i].value(state) for state in STATES] # get the root-mean-squared error errors[i][episode] += np.sqrt(np.mean(np.power(true_value[1: -1] - state_values, 2))) # average over independent runs errors /= runs for i in range(0, len(labels)): plt.plot(errors[i], label=labels[i]) plt.xlabel('Episodes') plt.ylabel('RMSVE') plt.legend() plt.savefig('./figure_9_10.png') plt.show() figure_9_10(true_value) 100%|██████████| 5000/5000 [03:53&lt;00:00, 21.37it/s] 100%|██████████| 5000/5000 [00:09&lt;00:00, 527.11it/s] Polynomial / Fourier -Based value function 特征的linear function approximation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# a wrapper class for polynomial / Fourier -based value functionPOLYNOMIAL_BASES = 0FOURIER_BASES = 1class BasesValueFunction: # @order: # of bases, each function also has one more constant parameter (called bias in machine learning) # @type: polynomial bases or Fourier bases def __init__(self, order, type): self.order = order self.weights = np.zeros(order + 1) # set up bases function self.bases = [] if type == POLYNOMIAL_BASES: for i in range(0, order + 1): self.bases.append(lambda s, i=i: pow(s, i)) elif type == FOURIER_BASES: for i in range(0, order + 1): self.bases.append(lambda s, i=i: np.cos(i * np.pi * s)) # get the value of @state def value(self, state): # map the state space into [0, 1] state /= float(N_STATES) # get the feature vector feature = np.asarray([func(state) for func in self.bases]) return np.dot(self.weights, feature) def update(self, delta, state): # map the state space into [0, 1] state /= float(N_STATES) # get derivative value derivative_value = np.asarray([func(state) for func in self.bases]) self.weights += delta * derivative_value # Figure 9.5, Fourier basis and polynomialsdef figure_9_5(true_value): # my machine can only afford 1 run runs = 1 episodes = 5000 # # of bases orders = [5, 10, 20] alphas = [1e-4, 5e-5] labels = [['polynomial basis'] * 3, ['fourier basis'] * 3] # track errors for each episode errors = np.zeros((len(alphas), len(orders), episodes)) for run in range(runs): for i in range(len(orders)): value_functions = [BasesValueFunction(orders[i], POLYNOMIAL_BASES), BasesValueFunction(orders[i], FOURIER_BASES)] for j in range(len(value_functions)): for episode in tqdm(range(episodes)): # gradient Monte Carlo algorithm gradient_monte_carlo(value_functions[j], alphas[j]) # get state values under current value function state_values = [value_functions[j].value(state) for state in STATES] # get the root-mean-squared error errors[j, i, episode] += np.sqrt(np.mean(np.power(true_value[1: -1] - state_values, 2))) # average over independent runs errors /= runs for i in range(len(alphas)): for j in range(len(orders)): plt.plot(errors[i, j, :], label='%s order = %d' % (labels[i][j], orders[j])) plt.xlabel('Episodes') plt.ylabel('RMSVE') plt.legend() plt.savefig('./figure_9_5.png') plt.show() figure_9_5(true_value) 100%|██████████| 5000/5000 [01:18&lt;00:00, 63.43it/s] 100%|██████████| 5000/5000 [02:05&lt;00:00, 38.65it/s] 100%|██████████| 5000/5000 [01:36&lt;00:00, 51.71it/s] 100%|██████████| 5000/5000 [02:54&lt;00:00, 27.98it/s] 100%|██████████| 5000/5000 [02:06&lt;00:00, 39.42it/s] 100%|██████████| 5000/5000 [04:38&lt;00:00, 18.22it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter09 On-policy Prediction with Approximation]]></title>
    <url>%2F2018%2F12%2F08%2FChapter09-On-policy-Prediction-with-Approximation%2F</url>
    <content type="text"><![CDATA[之前在第一部分(Chapter01~Chapter08)讲的value function建立，是一种表格的方法(tabular method)，不管是state-value还是action-value，最终的收敛结果都是以有限的、准确的列表来存储的。这种方法是理解强化学习的基础方法，但是随着state space的扩大，传统的精确方法便被计算资源和计算时间所限制，这时，学习目标便从准确的value table变为了学习近似的value function。function approximation是value function的近似形式，之所以称之为function，是因为所有的value都是通过一个带参数的函数来近似的，函数参数是通过一部分的学习数据得到，用来在整个state space上推广。 这种思路比较像监督学习(supervised learning)。但是因为强化学习问题所引入的nonstationarity, bootstrapping, and delayed targets等特性，reinforcement learning with function approximation相较supervised learning还有很多新的问题需要讨论。本章主要讲了function approximation在on-policy的预测问题中的应用。 Value-function Approximationvalue-function approximation就是使用s和一组向量w来表示state的estimate value：f(s,w) value-function approximation有别于表格方法的参数化值函数表示方法，通过s-&gt;g作为训练样本，最小化训练误差来训练参数，通过有限的样本来拟合state空间上的value值。 We use these methods for value prediction simply by passing to them the state -&gt;goal of each update as a training example. We then interpret the approximate function they produce as an estimated value function. 这个函数理论上可以使用任意的监督学习算法：线性模型、决策树、神经网络等。但是针对强化学习问题，对监督学习算法提出以下要求： (1)可以随着agent和环境进行交互进行on-line学习，通过可以处理增量形式获取的数据 (2)算法可以适应变化的目标函数，因为强化学习任务经常遇到nonstationary的问题，比如基于GPI进行的control任务，target policy π经常会发生变化。 The Prediction Objective (VE)基于function approximation的预测的目标是尽量减少estimate value和true value之间的误差，这里引入Mean Squared Value Error作为预测目标： μ(s)是引入的误差权重，在on-policy的预测问题中称为on-policy distribution。在连续任务(continuing tasks)中，该分布是基于policy π的固定分布。 on-policy distribution是通过以下两步公式计算得到： h(s)表示一个episode从s开始的概率，η(s) 表示一个episode出现s的概率 μ(s)是η(s)在|S|上的归一化值，μ(s)越大，则说明s出现的概率越大，s引入的value估计误差影响越大。 最小化VE并不能保证得到全局最优的function approximation，除了像简单的线性拟合函数等，但是对于复杂的监督学习模型，比如决策树，神经网络等，可能达到局部最优，也可能不会收敛。 Stochastic-gradient and Semi-gradient Methods使用梯度下降的方法来训练得到最优的w，以及得到最优的function approximation。 通过对VE关于w求导，并使用梯度下降的思路来训练得到VE最小的w： 这个地方开始就和普通的监督学习问题不同了。考虑之前在监督学习中学习梯度下降的时候，训练时把train data直接带入下降公式训练参数即可。但是在强化学习任务中有一个问题，就是true value是未知的！那么怎么开始梯度下降？ 考虑我们在第三章学过的一个公式： 如果U_t是无偏的，即E[U_t|S_t = s] = v_π(S_t)，那么梯度下降的结果会收敛到局部最优值。 下面给出梯度下降(使用SGD)的Monte Carlo方法的伪代码： 值得注意的是，如果使用DP方法或者TD方法等bootstrapping方法，上面的更新是有问题的，因为bootstrapping方法中U_t包含了后续状态的estimate，所以此时的U_t就也是w的函数了，那么这个梯度下降公式只考虑了w改变对VE的影响，却忽略了w改变对update target的影响，这种梯度下降方法称为semi-gradient方法。 尽管semi-gradient方法不能像上面提到的gradient方法那样稳定的收敛，但是在某些特定条件下(such as the linear case discussed in the next section.)这种方法更受偏爱，因为semi-gradient方法不用等待episode完成，更新速度也更快，可以说是on-line learning的，因为更新是随着agent和环境交互同时进行的，所以可以适用于continuing problem。 下面给出一种典型的semi-gradient方法，semi-gradient TD(0)方法的伪代码： 关于gradient的例子，可以参考Example 9.1: State Aggregation on the 1000-state Random Walk，注意这里采用了状态聚合的SGD方法。 Linear Methodsthis section给出了一种常见的function approximation形式——linear形式： 关于semi gradient TD(0)方法的收敛性证明，可以证明这种方法收敛到w_{TD}的不动点(fixed point): 同时渐进误差VE可以达到最小VE误差的一个扩大边界范围： 给出了semi-gradient的TD(n)方法的伪代码，因为采样是on-policy的，所以和TD(n)的表格方法预测代码结构基本相似： 关于使用自举(bootstrapping)的状态聚合semi-gradient TD方法的渐进性能，以及不同n和α对VE的影响，可以参考Example 9.2: Bootstrapping on the 1000-state Random Walk Feature Construction for Linear Methods这一部分是本章的重点，讲了如何从给定的state构造相应的feature。直接使用state的原始数字特征，比如一个多维度的S={s1,s2,…,sn}来做特征的话，并不能有效的学习到整个|S|的value function的特性，选择合适的特征可以使得学习到的value function更有效的泛化在整个state空间上。 Polynomials基函数构造特征 多项式构造feature结构简单，并可以自然的考虑到不同维度state数值的联系，但是在大多数的强化学习任务中效果并不是很好。 Fourier Basis基函数构造特征使用傅里叶级数表示特征的方法广泛的适用于强化学习任务，因为只要一个函数是给定的，那么它一定存在傅里叶级数展开，当然前提需要满足狄利克雷条件，一般强化学习任务构造的函数都满足这个条件。 对于有界的函数，我们可以令三角函数的基本周期τ对应区间长度，并只取区间内的三角函数的值来作为基函数。 更进一步，如果将τ设置为两倍的区间长度，那么可以只使用cos函数来作为基函数，然后只使用(0,τ/2)上的基函数来做特征。 下面给出一个1维state的Fourier基函数： 同理，多维的state构造Fourier基函数如下所示： Coarse Coding(粗糙编码)使用不同的state空间范围来(如1维空间对应随机长度的区间，2维空间对应随机大小和位置的圆)作为特征，对于state空间上任一点，如果落在f_n对应的空间内，则对应feature则置1，反正置0。这就是Coarse Coding的基本原理。 以二维空间为例，不同的圆的形状、尺寸、分布密度，都对所构造的粗糙编码特征的泛化性能造成影响： 以使用黑色阴影内的白色state来更新value function为例： 在narrow分布下，包含state的circle比较密集，所以泛化的范围比较小； 在broad分布下，包含state的circle覆盖范围比较大，所以泛化的范围比较大； 在Asymmetric分布下，因为circle的形状比较窄长，所以泛化会沿着特定方向。 关于Coarse Coding的例子，可以参考Example 9.3: Coarseness of Coarse Coding Tile CodingTile Coding是Coarse Coding的的一种类型，这种方法通过将每个feature的感受野(receptive fields)划分为很多tile，如果训练使用的state落在对应的tile内，则对其进行更新。 使用随机的offset可以使得泛化更加均匀，如果使用统一的offset，泛化效果就会沿着对角线，泛化结果会更差一点： Radial Basis FunctionsRBF也是Coarse Coding的一种，但是这种feature构造方法并不是二值化的，而是采用了(0,1)之间的特征： 使用RBF特征产生的value function approximation更加平滑，并且是可微分的。 Selecting Step-Size Parameters Manually这里给出关于学习步长α的直观认识： (1)递减的α能有效收敛，但是收敛速度比较慢 (2)设置α过大，使得最近的样本占得比重提高，极限情况下α=1，这时经验值的比重为0，直接采用当前的样本值。直接消除value估计函数和样本的误差，既不利于在整个样本上达到整体误差最小，甚至有可能引起收敛曲线振荡；也不利于估计函数在状态空间上的泛化。 (3)在tabular方法中，可以近似认为，α=1/τ代表每个state的估计值是最近τ个样本的平均值。虽然在value function approximation中这种表示不太准确，但是在样本特征的长度变化不大下，有以下的经验公式，可以近似保证被最近τ个experience的平均值更新： Nonlinear Function Approximation: Artificial Neural Networks人工神经网络当然也可以作为近似value function，这部分主要介绍了ANN，并没有介绍具体的DeepRL，所以就不在赘述了。 Least-Squares TD不使用梯度下降的方法，根据TD方法收敛的不动点w_TD和A矩阵以及b矩阵的关系，直接on-policy更新w_TD，注意这里是直接置更新，不是增量更新。 这种方法训练速度快，但是相应的计算成本高，尤其是求逆运算，直接求逆复杂度为O(d^3)，使用了Sherman-Morrison formula优化可以达到O(d^2)，但是和梯度下降的O(d)相比仍然很高了。 使用LSTD方法训练的伪代码如下： Memory-based Function Approximationlazy learning，类似监督学习中的knn，通过存储一系列样本，然后预测s的时候，取出和s相似度高的来预测，大概分为以下几种算法： (1)nearest neighbor method:返回存储中和s最近的样本的value (2)weighted average:返回最近的几个example的加权平均，权重是随着距离递增递减的 (3)locally weighted regression:通过参数拟合方法，使得拟合结果符合附近state整体的函数形状，即估计值可以最小化VE误差，估计之后会丢弃估计值，这是lazy learning方法的通性，即不存储估计结果。 相较于参数学习方法，memory-based方法可以提供更快的估计方法，而且估计结果随着训练数据的提升可以显著提升。因为on-policy采样的结果，可以避免全局近似，使得估计更专注于有价值的state集合。 但是memory-based算法性能同时也受到存储容量和搜索算法的影响，这些都有一些对应的加速算法。 Kernel-based Function Approximation核函数(Kernel)是描述两个state相关度的函数，核函数可以用于memory-based方法来选取用于估计的样本state对应的weight，对应的memory-based方法为Kernel regression： 核函数描述了两个state之间的相关度，换句话说，核函数也可以表示一个state泛化到另一个state的能力。比如前面在tile Coding中的图片，state对应的深度即表示了中心state对其的泛化能力。事实上，核函数可以描述所有线性近似函数的泛化能力。 一个典型的核函数是Gaussian radial basis function (RBF) used in RBF function approximation。在memory-based方法中，RBF的中心是存储的样本，将落在state空间中的待估计s对应的RBF值作为样本weight来完成估计。 通过前面构造的feature同样可以重新构成Kernel: 这种形式的Kernel regression可以在相同训练数据情况下和参数估计价值函数达到相同的效果。 这种方法给我们一个启示：feature空间可能很大，或者说直接构造feature比较麻烦，为何不直接从数据中构造Kernel那？SVM方法就是一个很好的例子。对应很多强化学习问题来说，从feature内积得到Kernel可能是困难的，但是所有问题都是可以直接构造Kernel的，这种情况下构造Kernel然后使用Kernel regression会比构造feature使用参数近似函数方法更有效，这就是机器学习中常使用的kernel trick Looking Deeper at On-policy Learning: Interest and Emphasis本章到这里，我一直有一个问题，就是在VE的section，计算VE的时候，每个state的estimate都有一个对应的weight，即on-policy distribution，但是在后面的算法中并没有体现这个参数的作用。这一部分算是解决了我的一个疑问，就是不同的state训练的结果对w的影响应该是不同的，即引入了interest和emphasis的概念。 interest表示训练算法对一个state的感兴趣程度，是一个0-1的非负数。 emphasis即计算w增量时的参数，下面给出带emphasis的semi-gradient TD(n)更新公式: 对应的emphasis更新公式： 这个更新公式同时也涵盖了Monte Carlo的情况，令n=T-t，G_{t:t+n}=G_t，M_t=I_t]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter08 Planning and Learning with Tabular Methods]]></title>
    <url>%2F2018%2F12%2F07%2FChapter08-Planning-and-Learning-with-Tabular-Methods%2F</url>
    <content type="text"><![CDATA[这章给出了关于model-free方法和model-based方法的一个统一观点，即将两者的思路结合起来解决问题： model-based 方法训练采用基于model进行planning的方法，通过向后迭代(backup)来更新value function；而model-free 方法训练采用从环境抽样(sampling)来进行value function的learning，具体也是使用后续的state(action)-value来backup当前value function。两者之间同时有很多相似点，比如都会update value function，都采用backup的策略update等。可见两者的区别主要在于model的维护与否，当然这就是结合起来两种方法的训练策略。 Models and Planning我觉得这里理解sample model就够了，sample model也是本章剩下的部分使用的planning方法的基础——从采样得到随机model进行planning。 首先理解distribution model和sample model之间的关系： Given a state and an action, a model produces a prediction of the resultant next state and next reward. distribution model: produce a description of all possibilities and probabilities for next state and next reward. sample model: produce just one next state and action by sampling before. distribution model就是在第四章使用的MDP，sample model就是利用采样得到next state，然后利用这些采样经验组成的model。 但是不管哪种模型，更新value function的方法都是一样的，即通过planning的方法来得到优化(optimal)的policy。但是从planning得到policy的具体算法还分为两类： state-space planning：通过产生模拟的经验，通过模拟action让一个state转换到另一个state，来更新value-function从而得到优化的policy plan-space planning：通过在plans的空间中寻找优化的plan，通过从一个plan切换到另一个plan，从而更新value-function，得到优化的policy 后者在随机序列决策问题(stochastic sequential decision problems)中应用起来比较困难，所以一般都是采用前者。 state-space planning的优化流程图： 接着给出使用sample model更新value function的伪代码，注意我们一般不会只采用模型来更新(Priority Sweeping除外)，所以这个算法会在本章后续部分和Q-Learning等model-free方法一起来完成更新: Dyna: Integrating Planning, Acting, and Learning这一部分提出一个重要的算法:Dyna-Q。在我们讨论这个算法之前，先来看一下model planning是如何融入之前的model-free算法的，即indirect reinforcement learning方法是如何提升value function的update的： 可以看到通过实时的经验(experience)，学习产生了model，然后通过planning也对value function进行了更新，这种方法叫做on-line planning。 好了，我们接着来看一下Dyna算法的通用体系： 右边部分阐述了model-planning的工作原理，首先需要通过实际的经验来学习得到一个model，然后剩下的和上面提到的state-space plan一致：通过model模拟experience，然后进行backup。 最后给出Dyna-Q的算法伪代码： 这里有一个使用不同的model-planning step的示例：Example 8.1 Dyna Maze When the Model Is Wrong如果我们正在学习的任务，它所对应的environment是变化的，那么我们学习到的model很有可能和environment之间是有差别，这一部分来探讨一下如果模型出错，Dyna方法的性能。 这里引入一种Dyna-Q的优化算法：Dyna-Q+。说优化，其实主要是Q+方法引入了一种启发式exploration：通过给sample得到的model中的state-action对添加了一个timestamp。当选择action的时候，那些current_time-timestamp值比较大的state-action说明这种state-action已经很久没有选中了，那么我们模型中关于这对state-action的动态特性很有可能已经过时了，即此时模型有可能已经和实际environment有偏差了，所以需要给这些state-action的reward附加补偿：r+k*sqrt(current_time-timestamp)。这种启发式的exploration可以有效的帮助policy的学习过程跳出局部optimal policy，同时对于变化的environment适应性更强。 关于Dyna-Q和Dyna-Q+在变化的environment中的性能，可以参考Example 8.2 Blocking Maze Example 8.3 Shortcut Maze Prioritized Sweeping可以看到前面建立model的时候，是把episode随机产生的state-action”喂”(feed)进model，同理从model模拟环境进行planning的时候，也是随机产生state-action的。那么我们不禁思考：可不可以优先更新那些“更新效果更好”的state-action？ 答案是肯定的。因为有的state-action可能在实际的优化路径中基本不会出现，花费资源来更新这些state-action，对其他有用的state-action或者说整体所有state-action对的更新和策略的优化基本是0收益的。所以这部分提出了一种Prioritized Sweeping的方法，维护一个优先队列，只有那些更新target足够大的state-action才会添加入队列，同时队列也是按照更新概率来排序的，即abs(target) = priority。 同时Prioritized Sweeping算法的value function更新是采用backward focusing方法的。即更新的时候是利用当前Q来更新前导previous-Q。这是一种启发式的思路(P137)： Only an update along a transition into the state just prior to the goal, or from it, will change any values. So search might be usefully focused by working backward from goal states. Of course, we do not really want to use any methods specific to the idea of “goal state.” In general, we want to work back not just from goal states but from any state whose value has changed. 理解了算法的基本原理，下面给出相应的伪代码： 注意Prioritized Sweeping算法并没有在sample阶段使用Q-Learning或者其他model-free算法来更新，原因上面也讲了，因为更新随机采样得到的state-action会引入不必要的value更新，浪费计算资源。 关于Prioritized Sweeping的例子Example 8.4 Prioritized Sweeping on Mazes Expected vs Sample Updates可以通过三个问题将目前我们已经学过的或者提到过的强化学习算法分为7类(理论上应该可以分为8=2^3类，但是有一种分类并没有对应具体的算法)： (1)更新的action-value还是state-action (2)估计value使用的是target-policy还是greedy policy (3)更新使用的是expected还是sample update 对应的具体算法如下: expected update使用的是bellman equation： sample update使用的是n-step的更新公式，以Q-Learning为例： 在value估计过程中，两种方法的误差性能，可以参考Figure 8.8 Trajectory Sampling在采样更新中，不可避免需要讨论采样方法。可以像第四章的DP方法那样，从第一个state一直顺序更新到terminal state，即完整的更新整个set of state；也可以像第六章的Temporal Difference方法那样，以Sarsa方法为例，使用policy来选择并更新action-value。 后者即这部分要讲的trajectory sampling抽样算法。trajectory sampling的思路和优先扫描(Prioritized Sweeping)的一致，都是考虑到均匀更新会浪费大量的计算资源，只按照一定的策略选择更新value，属于启发式的更新策略。 关于均匀抽样和trajectory sampling的算法性能比较，可以参考Figure 8.9 Real-time Dynamic Programmingreal-time DP可以看做DP算法的trajectory sampling升级版：RTDP updates the values of states visited in actual or simulated trajectories by means of expected tabular value-iteration updates. RTDP是一种典型的异步DP算法，异步DP算法在第四章提到的： Asynchronous DP algorithms are not organized in terms of systematic sweeps of the state set; they update state values in any order whatsoever, using whatever values of other states happen to be available. RTDP算法适用于一类stochastic optimal path problems(随机最优路径问题)，这类问题满足以下4个性质： (1)每一个goal state的初始value都是0 (2)there exists at least one policy that guarantees that a goal state will be reached with probability one from any start state(懂意思但是翻译不好@_@) (3)所有从非goal state进行的state转换，得到的reward都是负值 (4)所有的state的初始value都要不小于最终的optimal value，最简单的方法就是将initial value都设置为0 这种问题优化目的不是为了找到最高的reward总和，而是为了得到最少的cost。优化policy，使得达到goal state的负值reward和最小，即最小化cost。 和传统DP相比，RTDP收敛所需的update次数更少，因为RTDP是随着value function达到最优时，选择action的policy也同时达到最优。 但是传统的DP方法需要等待value function更新误差小于一定值才会停止更新，其实在value function停止更新前，policy其实已经达到最优了，但是如果附加程序来检查policy是否达到最优会需要相当多的额外计算。通过这点也可以看出RTDP的收敛是快于传统DP的。 Planning at Decision Time这里提出了一种使用planning的新方法，即在给定current_state时，利用当前学习得到的model来预测并实施action，这种方法叫做planning at decision time。 这章前面的内容提到的算法，都是在后台运行planning算法，来提升整体的value function训练性能。关于两种算法，一般根据不同的情形有不同的用法： 如果对时间要求不是很高，比如棋类游戏，每一步都有几秒或者几分钟时间来思考，那么适用planning at decision time可以在这段时间向前plan几十步，选择一个合适的action，而且因为实际的state-action对很多，所以出现重复的概率比较低，相应的资源浪费也就没那么严重； 如果对时间要求比较高，那么最好在后台运行planning算法来优化policy，使得整体的value function得到提升。 Heuristic Search使用decision time planning的state-space planning方法中，最常见的就属Heuristic Search(启发式搜索)了。 启发式搜索可以看做一个多步的greedy policy，启发式搜索算法的工作流程： 每当遇到一个状态，就以该状态为根节点，所有可能的后续state和action为leaf来创建一棵树，然后使用backup的方法计算每个节点的value，并greedy的选择action。当一个状态的planning结束之后，丢弃所有的备份值。 启发式搜索的优点主要是由decision time带来的，使得policy可以集中资源来考虑当前状态下的决策，比如在棋类游戏中，这种方法可以从当前状态开始存储更多的未来状态情况，可以更加有效的利用内存。 Rollout Algorithmsrollout algorithms是一类针对backgammon的算法，该算法是基于Monte Carlo的decision time planning算法，通过在当前state下进行Monte Carlo模拟采样来进行state的update，当然并不保留backup的值。 Rollout算法的目标是基于当前状态和给定的rollout-policy来进行planning，做出决策之后则抛弃使用的估计值，即计算得到的backup。Rollout算法只是为了在给定的rollout-policy基础上进行提升以及决策，并不是为了学习到optimal policy，所以Rollout算法严格来说并不能算是learning algorithms。 书上对Rollout Algorithms的总结比较合适： We do not ordinarily think of rollout algorithms as learning algorithms because they do not maintain long-term memories of values or policies.However, these algorithms take advantage of some of the features of reinforcement learning that we have emphasized in this book. As instances of Monte Carlo control, they estimate action values by averaging the returns of a collection of sample trajectories, in this case trajectories of simulated interactions with a sample model of the environment. In this way they are like reinforcement learning algorithms in avoiding the exhaustive sweeps of dynamic programming by trajectory sampling, and in avoiding the need for distribution models by relying on sample, instead of expected, updates. Finally, rollout algorithms take advantage of the policy improvement property by acting greedily with respect to the estimated action values.(我们通常不把rollout算法看作学习算法，因为它们不保持对值或策略的长期记忆。然而，这些算法利用了我们在本书中强调的强化学习的一些特征。作为蒙特卡罗控制的实例，它们通过平均一组样本轨迹的reward来估计动作值，在这种情况下，是模拟与环境样本模型的交互的轨迹。通过这种方式，它们就像强化学习算法一样，通过trajectory sampling避免了进行动态规划的exhaustive sweeps，并且通过依赖样本而不是期望更新来避免对分布模型的需求。最后，展开算法利用策略改进特性，对估计的动作值greedy地操作。) Monte Carlo Tree SearchMonte Carlo Tree Search是最近出现的比较成功的decision-planning算法(PS:花了4个小标题来阐述decision time planning算法，为什么不给个例子呢( _ゝ`))。 MCTS算法是基于Rollout算法的，但是通过累计通过Monte Carlo方法仿真得到的value estimate来更好的提升rollout-policy。 算法基本上分为一下四步： 第一步是Selection，就是在树中找到一个最好的值得探索的节点，一般策略是先选择未被探索的子节点，如果都探索过就选择UCB值最大的子节点。第二步是Expansion，就是在前面选中的子节点中走一步创建一个新的子节点，一般策略是随机自行一个操作并且这个操作不能与前面的子节点重复。第三步是Simulation，就是在前面新Expansion出来的节点开始模拟游戏，直到到达游戏结束状态，这样可以收到到这个expansion出来的节点的得分是多少。第四步是Backpropagation，就是把前面expansion出来的节点得分反馈到前面所有父节点中，更新这些节点的quality value和visit times，方便后面计算UCB值。 这里的decision time planning算法只是做了一个简单的介绍，大致介绍了一下基本思路，如果需要深入还需要阅读相关的论文。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter08 trajectory_sampling]]></title>
    <url>%2F2018%2F12%2F06%2FChapter08-trajectory-sampling%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter08/trajectory_sampling.py 通过一个MDP的例子比较了均匀采样和on-policy采样的性能 问题描述问题建立在一个MDP上，首先假设一个MDP有n个state，其中[0,n)是一般状态，n是terminal-state，每个状态有两个action=0;1。每个action都用epsilon的概率使state跳转到terminal-state；如果没有跳转到terminal-state，action会导致state等概率的跳转到b个branch对应的state上，同时每个branch对应的state也是|S|上等概率的，所以MDP示意图如下： 引入模块并定义常量1234567891011121314151617import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm# 2 actionsACTIONS = [0, 1]# each transition has a probability to terminate with 0TERMINATION_PROB = 0.1# maximum expected updatesMAX_STEPS = 20000# epsilon greedy for behavior policyEPSILON = 0.1 定义argmax函数，返回value中最大值的任一个索引1234# break tie randomlydef argmax(value): max_q = np.max(value) return np.random.choice([a for a, q in enumerate(value) if q == max_q]) 定义Task类，用来模拟agent和环境的交互1234567891011121314151617181920212223242526class Task(): # @n_states: number of non-terminal states # @b: branch # Each episode starts with state 0, and state n_states is a terminal state def __init__(self, n_states, b): self.n_states = n_states self.b = b # transition matrix, each state-action pair leads to b possible states # 返回size尺寸的range(n_states)内的随机数 # 这里根据self.transition来选定next_state这样理解的： # 首先选定state和action，因为每个action都有b种分支branch的选择，每种branch又随机对应一种|S|中的state # 所以在step函数里根据state和action选择next_state的时候，需要先random选择一个branch，然后这个branch对应的 # state再由self.transition给出 self.transition = np.random.randint(n_states, size=(n_states, len(ACTIONS), b)) # it is not clear how to set the reward, I use a unit normal distribution here # reward is determined by (s, a, s') self.reward = np.random.randn(n_states, len(ACTIONS), b) def step(self, state, action): if np.random.rand() &lt; TERMINATION_PROB: # 直接跳转到terminal状态，并给reward=0 return self.n_states, 0 next = np.random.randint(self.b) return self.transition[state, action, next], self.reward[state, action, next] 计算使用基于value function的greedy policy得到的平均episode-reward123456789101112131415# Evaluate the value of the start state for the greedy policy# derived from @q under the MDP @taskdef evaluate_pi(q, task): # use Monte Carlo method to estimate the state value runs = 1000 returns = [] for r in range(runs): rewards = 0 state = 0 while state &lt; task.n_states: action = argmax(q[state]) state, r = task.step(state, action) rewards += r returns.append(rewards) return np.mean(returns) 使用expected update更新value function，使用一致采样123456789101112131415161718192021222324252627# perform expected update from a uniform state-action distribution of the MDP @task# evaluate the learned q value every @eval_interval steps# 根据给定的评估间隔，在expected update中穿插着reward的评估，来对更新算法进行评估def uniform(task, eval_interval): performance = [] q = np.zeros((task.n_states, 2)) for step in tqdm(range(MAX_STEPS)): # //表示整除 # 这里可以看到所有的state-action都会被sample，只要MAX_STEPS足够大 state = step // len(ACTIONS) % task.n_states action = step % len(ACTIONS) # next_states是一个维度为(b,)的array next_states = task.transition[state, action] # task.reward[state,action]是一个维度为(b,)的array # q[next_states,:]是维度为(b,2)的array,np.max操作之后就是(b,)维度了 # 这个更新公式是expected update的，P141 # 这里还有一个细节，就是真实的概率应该是(1-TERMINATION_PROB)/b，但是np.mean里隐含了1/b q[state, action] = (1 - TERMINATION_PROB) * np.mean( task.reward[state, action] + np.max(q[next_states, :], axis=1)) if step % eval_interval == 0: # 如果到评估间隔了，则进行一次greedy policy更新 v_pi = evaluate_pi(q, task) performance.append([step, v_pi]) return zip(*performance) 使用基于on-policy的采样方法进行value function的更新123456789101112131415161718192021222324252627282930# perform expected update from an on-policy distribution of the MDP @task# evaluate the learned q value every @eval_interval stepsdef on_policy(task, eval_interval): performance = [] q = np.zeros((task.n_states, 2)) # 可以看到两种更新方法都是从state=0开始训练的 state = 0 # 可以看到采样的时候没有使用uniform的均匀采样，而是根据value function来生成episode，并紧随着state的迭代来更新 for step in tqdm(range(MAX_STEPS)): if np.random.rand() &lt; EPSILON: action = np.random.choice(ACTIONS) else: action = argmax(q[state]) next_state, _ = task.step(state, action) # value function的更新和uniform()的更新是一样的 next_states = task.transition[state, action] q[state, action] = (1 - TERMINATION_PROB) * np.mean( task.reward[state, action] + np.max(q[next_states, :], axis=1)) if next_state == task.n_states: next_state = 0 state = next_state if step % eval_interval == 0: v_pi = evaluate_pi(q, task) performance.append([step, v_pi]) return zip(*performance) 通过图像来比较两种采样方法的性能123456789101112131415161718192021222324252627282930313233343536def figure_8_9(): num_states = [1000, 10000] branch = [1, 3, 10] methods = [on_policy, uniform] # average accross 30 tasks # woc这总共要跑2*3*2*30=360个max_steps，太变态了。。。 # n_tasks = 30 n_tasks = 5 # number of evaluation points x_ticks = 100 plt.figure(figsize=(10, 20)) for i, n in enumerate(num_states): plt.subplot(2, 1, i+1) for b in branch: tasks = [Task(n, b) for _ in range(n_tasks)] for method in methods: value = [] for task in tasks: steps, v = method(task, MAX_STEPS / x_ticks) value.append(v) value = np.mean(np.asarray(value), axis=0) plt.plot(steps, value, label='b = %d, %s' % (b, method.__name__)) plt.title('%d states' % (n)) plt.ylabel('value of start state') plt.legend() plt.subplot(2, 1, 2) plt.xlabel('computation time, in expected updates') plt.savefig('./figure_8_9.png') plt.show()figure_8_9() 100%|██████████| 20000/20000 [00:23&lt;00:00, 863.09it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 882.61it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 873.29it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 884.96it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 870.28it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 888.02it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 891.23it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 883.12it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 885.09it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 864.50it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 833.70it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 847.89it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 872.53it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 879.40it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 868.80it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 906.87it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 876.36it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 890.05it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 888.76it/s] 100%|██████████| 20000/20000 [00:21&lt;00:00, 910.71it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 874.98it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 896.89it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 884.20it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 904.25it/s] 100%|██████████| 20000/20000 [00:21&lt;00:00, 924.97it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 881.82it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 904.37it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 896.47it/s] 100%|██████████| 20000/20000 [00:21&lt;00:00, 909.28it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 884.27it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 857.58it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 861.50it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 858.42it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 876.96it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 872.85it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 900.74it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 889.23it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 896.29it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 891.87it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 879.51it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 854.93it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 853.38it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 851.27it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 839.56it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 868.91it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 858.23it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 869.78it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 883.71it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 894.70it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 886.71it/s] 100%|██████████| 20000/20000 [00:23&lt;00:00, 868.78it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 882.62it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 887.52it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 881.84it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 878.90it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 902.32it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 901.14it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 882.30it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 872.38it/s] 100%|██████████| 20000/20000 [00:22&lt;00:00, 894.51it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter08 expectation vs sample]]></title>
    <url>%2F2018%2F12%2F06%2FChapter08-expectation-vs-sample%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter08/expectation_vs_sample.py 通过一个简单的示例表现了使用expected 和sample update训练产生的相对误差 引入模块12345import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm 计算next state的sample和expect的均方误差1234567891011121314151617181920# for figure 8.8, run a simulation of 2 * @b stepsdef b_steps(b): # set the value of the next b states # it is not clear how to set this # distribution 是维度为b的正态分布array distribution = np.random.randn(b) # true value of the current state true_v = np.mean(distribution) samples = [] errors = [] # sample 2b steps for t in range(2 * b): v = np.random.choice(distribution) samples.append(v) errors.append(np.abs(np.mean(samples) - true_v)) return errors 绘制图线，表征均方误差随抽样次数的变化1234567891011121314151617181920def figure_8_8(): runs = 100 branch = [2, 10, 100, 1000] for b in branch: errors = np.zeros((runs, 2 * b)) for r in tqdm(np.arange(runs)): errors[r] = b_steps(b) errors = errors.mean(axis=0) x_axis = (np.arange(len(errors)) + 1) / float(b) plt.plot(x_axis, errors, label='b = %d' % (b)) plt.xlabel('number of computations') plt.xticks([0, 1.0, 2.0], ['0', 'b', '2b']) plt.ylabel('RMS error') plt.legend() plt.savefig('./figure_8_8.png') plt.show()figure_8_8() 100%|██████████| 100/100 [00:00&lt;00:00, 9047.05it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 2282.52it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 176.76it/s] 100%|██████████| 100/100 [00:20&lt;00:00, 4.89it/s] 通过这个示例，可以看到expected update可以避免偏差，sample相应的误差就凸显出来了，but no free lunch，获得更好的训练效果自然需要付出相应的计算资源消耗。]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter08 maze]]></title>
    <url>%2F2018%2F12%2F05%2FChapter08-maze%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter08/maze.py 通过maze问题帮助对8.1-8.4的内容有一个更好的理解^_^ Dyna-Q：8.2 Dyna-Q+：8.3 Prioritized Sweeping：8.4 引入模块1234567import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdmimport heapqfrom copy import deepcopy 定义迷宫类maze，实现算法和环境的交互方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# A wrapper class for a maze, containing all the information about the maze.# Basically it's initialized to DynaMaze by default, however it can be easily adapted# to other mazeclass Maze: def __init__(self): # maze width self.WORLD_WIDTH = 9 # maze height self.WORLD_HEIGHT = 6 # all possible actions self.ACTION_UP = 0 self.ACTION_DOWN = 1 self.ACTION_LEFT = 2 self.ACTION_RIGHT = 3 self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT] # start state self.START_STATE = [2, 0] # goal state self.GOAL_STATES = [[0, 8]] # all obstacles self.obstacles = [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]] self.old_obstacles = None self.new_obstacles = None # time to change obstacles # 改变障碍物的时刻 self.obstacle_switch_time = None # initial state action pair values # self.stateActionValues = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions))) # the size of q value # q(s,a)的size=(height,width,actions) self.q_size = (self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions)) # max steps self.max_steps = float('inf') # track the resolution for this maze self.resolution = 1 # extend a state to a higher resolution maze # @state: state in lower resoultion maze # @factor: extension factor, one state will become factor^2 states after extension # 把一个给定的state拓展到以(state[0],state[1])和(state[0]+factor,state[1]+factor)为对角线的正方形states区域 # 提高了原来某个点的分辨率，该函数用在extend_maze函数里，用来提高迷宫的分辨率 def extend_state(self, state, factor): new_state = [state[0] * factor, state[1] * factor] new_states = [] for i in range(0, factor): for j in range(0, factor): new_states.append([new_state[0] + i, new_state[1] + j]) return new_states # extend a state into higher resolution # one state in original maze will become @factor^2 states in @return new maze # 拓展了整个maze的分辨率 def extend_maze(self, factor): new_maze = Maze() new_maze.WORLD_WIDTH = self.WORLD_WIDTH * factor new_maze.WORLD_HEIGHT = self.WORLD_HEIGHT * factor new_maze.START_STATE = [self.START_STATE[0] * factor, self.START_STATE[1] * factor] # 把goal_state拓展到一个以factor为缩放因子的正方形区域中 new_maze.GOAL_STATES = self.extend_state(self.GOAL_STATES[0], factor) new_maze.obstacles = [] for state in self.obstacles: new_maze.obstacles.extend(self.extend_state(state, factor)) new_maze.q_size = (new_maze.WORLD_HEIGHT, new_maze.WORLD_WIDTH, len(new_maze.actions)) # new_maze.stateActionValues = np.zeros((new_maze.WORLD_HEIGHT, new_maze.WORLD_WIDTH, len(new_maze.actions))) new_maze.resolution = factor return new_maze # take @action in @state # @return: [new state, reward] # 根据给定的state-action对，返回next-state和对应的reward def step(self, state, action): x, y = state if action == self.ACTION_UP: x = max(x - 1, 0) elif action == self.ACTION_DOWN: x = min(x + 1, self.WORLD_HEIGHT - 1) elif action == self.ACTION_LEFT: y = max(y - 1, 0) elif action == self.ACTION_RIGHT: y = min(y + 1, self.WORLD_WIDTH - 1) if [x, y] in self.obstacles: x, y = state if [x, y] in self.GOAL_STATES: reward = 1.0 else: reward = 0.0 return [x, y], reward dyna算法的参数类1234567891011121314151617181920212223242526# a wrapper class for parameters of dyna algorithmsclass DynaParams: def __init__(self): # discount self.gamma = 0.95 # probability for exploration self.epsilon = 0.1 # step size self.alpha = 0.1 # weight for elapsed time self.time_weight = 0 # n-step planning self.planning_steps = 5 # average over several independent runs self.runs = 10 # algorithm names self.methods = ['Dyna-Q', 'Dyna-Q+'] # threshold for priority queue self.theta = 0 根据epsilon-greedy policy选择action1234567# choose an action based on epsilon-greedy algorithmdef choose_action(state, q_value, maze, dyna_params): if np.random.binomial(1, dyna_params.epsilon) == 1: return np.random.choice(maze.actions) else: values = q_value[state[0], state[1], :] return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)]) 建立模型类，使用一般的模型建立方法，也是Dyna-Q算法的model建立方法1234567891011121314151617181920212223242526272829303132# Trivial model for planning in Dyna-Qclass TrivialModel: # @rand: an instance of np.random.RandomState for sampling def __init__(self, rand=np.random): # self.model的键值是state，对应的值是dict：action:[next_state,reward]，其中state是tuple格式，next_state是list格式 self.model = dict() self.rand = rand # feed the model with previous experience def feed(self, state, action, next_state, reward): state = deepcopy(state) next_state = deepcopy(next_state) # 必须把state(type=list)转换为tuple，因为dictionary的键值只能是不变值:tuple、string、数值型，list不可以。 if tuple(state) not in self.model.keys(): self.model[tuple(state)] = dict() self.model[tuple(state)][action] = [list(next_state), reward] # randomly sample from previous experience # 从建立起来的model中随机采样得到state-action-next_state-reward序列 def sample(self): # choice函数有两种：python模块rand对应的random.choice和模块numpy对应的np.random.choice # 默认参数下两者效果一样的，都是从给定序列中随机取出一个元素，但是np.random.choice()不能从string取出元素 # random.choice可以操作list，tuple，string state_index = self.rand.choice(range(len(self.model.keys()))) # state取出上一步随机选中的state index对应的state state = list(self.model)[state_index] action_index = self.rand.choice(range(len(self.model[state].keys()))) action = list(self.model[state])[action_index] next_state, reward = self.model[state][action] state = deepcopy(state) next_state = deepcopy(next_state) return list(state), action, list(next_state), reward 建立模型类，使用time-based模型建立方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# Time-based model for planning in Dyna-Q+class TimeModel: # @maze: the maze instance. Indeed it's not very reasonable to give access to maze to the model. # @timeWeight: also called kappa, the weight for elapsed time in sampling reward, it need to be small # @rand: an instance of np.random.RandomState for sampling def __init__(self, maze, time_weight=1e-4, rand=np.random): self.rand = rand self.model = dict() # track the total time self.time = 0 self.time_weight = time_weight self.maze = maze # feed the model with previous experience def feed(self, state, action, next_state, reward): state = deepcopy(state) next_state = deepcopy(next_state) self.time += 1 if tuple(state) not in self.model.keys(): self.model[tuple(state)] = dict() # 这里可以看到Dyna-Q+方法的model还考虑了未出现的action # Actions that had never been tried before from a state were allowed to be considered in the planning step for action_ in self.maze.actions: if action_ != action: # Such actions would lead back to the same state with a reward of zero # Notice that the minimum time stamp is 1 instead of 0 self.model[tuple(state)][action_] = [list(state), 0, 1] self.model[tuple(state)][action] = [list(next_state), reward, self.time] # randomly sample from previous experience def sample(self): state_index = self.rand.choice(range(len(self.model.keys()))) state = list(self.model)[state_index] action_index = self.rand.choice(range(len(self.model[state].keys()))) action = list(self.model[state])[action_index] next_state, reward, time = self.model[state][action] # adjust reward with elapsed time since last vist # 这个补偿的解释在P136页， # 因为self.time-time越大，说明这个state-action对已经很久没被选择过了，这种方法给予model一种启发式的探索 # 因为如果这对state-action已经很久没被选中了，那么它的动态性很有可能已经改变，那么我们当前的model保有的还是 # 它的旧的动态性dynamic，说明模型也有可能已经不正确了，所以给予一个相应的补偿奖励帮助选中这对state-action reward += self.time_weight * np.sqrt(self.time - time) state = deepcopy(state) next_state = deepcopy(next_state) return list(state), action, list(next_state), reward 建立优先队列类，用于prioritized sweeping方法123456789101112131415161718192021222324252627282930313233343536# 在8.4 Prioritized Sweeping的优先更新算法中使用class PriorityQueue: def __init__(self): self.pq = [] # entry_finder的values格式： # [priority,self.counter,item] self.entry_finder = &#123;&#125; self.REMOVED = '&lt;removed-task&gt;' self.counter = 0 def add_item(self, item, priority=0): # 这种方法判断的是item是否在self.entry_finder的keys里 if item in self.entry_finder: self.remove_item(item) entry = [priority, self.counter, item] self.counter += 1 # 堆的索引存储在self.entry_finder self.entry_finder[item] = entry heapq.heappush(self.pq, entry) def remove_item(self, item): # entry存储self.entry_finder退出的key=item的value entry = self.entry_finder.pop(item) entry[-1] = self.REMOVED # 如果self.pq中的元素的item值不等于self.REMOVED，那么返回该元素的item和priority值 def pop_item(self): while len(self.pq) &gt; 0: priority, count, item = heapq.heappop(self.pq) if item is not self.REMOVED: del self.entry_finder[item] return item, priority raise KeyError('pop from an empty priority queue') def empty(self): return not self.entry_finder 建立模型类，使用基于prioritized sweeping的模型建立方法123456789101112131415161718192021222324252627282930313233343536373839404142434445# Model containing a priority queue for Prioritized Sweepingclass PriorityModel(TrivialModel): def __init__(self, rand=np.random): TrivialModel.__init__(self, rand) # maintain a priority queue self.priority_queue = PriorityQueue() # track predecessors for every state self.predecessors = dict() # add a @state-@action pair into the priority queue with priority @priority def insert(self, priority, state, action): # note the priority queue is a minimum heap, so we use -priority self.priority_queue.add_item((tuple(state), action), -priority) # @return: whether the priority queue is empty def empty(self): return self.priority_queue.empty() # get the first item in the priority queue def sample(self): # 取出priority最大的那个元素 (state, action), priority = self.priority_queue.pop_item() next_state, reward = self.model[state][action] state = deepcopy(state) next_state = deepcopy(next_state) return -priority, list(state), action, list(next_state), reward # feed the model with previous experience def feed(self, state, action, next_state, reward): state = deepcopy(state) next_state = deepcopy(next_state) TrivialModel.feed(self, state, action, next_state, reward) if tuple(next_state) not in self.predecessors.keys(): self.predecessors[tuple(next_state)] = set() self.predecessors[tuple(next_state)].add((tuple(state), action)) # get all seen predecessors of a state @state # 返回一个state的所有前导state-action对 def predecessor(self, state): if tuple(state) not in self.predecessors.keys(): return [] predecessors = [] for state_pre, action_pre in list(self.predecessors[tuple(state)]): predecessors.append([list(state_pre), action_pre, self.model[state_pre][action_pre][1]]) return predecessors 使用Dyna-Q算法完成一次episode并更新value function12345678910111213141516171819202122232425262728293031323334353637383940414243# play for an episode for Dyna-Q algorithm# @q_value: state action pair values, will be updated# @model: model instance for planning# @maze: a maze instance containing all information about the environment# @dyna_params: several params for the algorithm# @return: the number of all steps in an episodedef dyna_q(q_value, model, maze, dyna_params): state = maze.START_STATE steps = 0 while state not in maze.GOAL_STATES: # track the steps steps += 1 # get action action = choose_action(state, q_value, maze, dyna_params) # take action next_state, reward = maze.step(state, action) # Q-Learning update q_value[state[0], state[1], action] += \ dyna_params.alpha * (reward + dyna_params.gamma * np.max(q_value[next_state[0], next_state[1], :]) - q_value[state[0], state[1], action]) # feed the model with experience model.feed(state, action, next_state, reward) # sample experience from the model # 在建立起来的model中进行dyna_params.planning_steps次采样，并对每次采样的得到的state-action的Q使用 # 其对应的next-state和reward更新 for t in range(0, dyna_params.planning_steps): state_, action_, next_state_, reward_ = model.sample() q_value[state_[0], state_[1], action_] += \ dyna_params.alpha * (reward_ + dyna_params.gamma * np.max(q_value[next_state_[0], next_state_[1], :]) - q_value[state_[0], state_[1], action_]) state = next_state # check whether it has exceeded the step limit if steps &gt; maze.max_steps: break return steps 使用prioritized sweeping方法进行一次episode更新123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# play for an episode for prioritized sweeping algorithm# @q_value: state action pair values, will be updated# @model: model instance for planning# @maze: a maze instance containing all information about the environment# @dyna_params: several params for the algorithm# @return: # of backups during this episodedef prioritized_sweeping(q_value, model, maze, dyna_params): state = maze.START_STATE # track the steps in this episode steps = 0 # track the backups in planning phase backups = 0 while state not in maze.GOAL_STATES: steps += 1 # get action action = choose_action(state, q_value, maze, dyna_params) # take action next_state, reward = maze.step(state, action) # feed the model with experience model.feed(state, action, next_state, reward) # get the priority for current state action pair # priority是当前state-action对应的更新规则中的target的绝对值 priority = np.abs(reward + dyna_params.gamma * np.max(q_value[next_state[0], next_state[1], :]) - q_value[state[0], state[1], action]) if priority &gt; dyna_params.theta: model.insert(priority, state, action) # 注意到这里并没有进行Q-Learning的Q更新 # start planning planning_step = 0 # 这个算法很真实，前面的部分只是通过sample的方式存储下来每个随机state的前导pre-state，并判断更新值(target)是否大于阈值 # 如果大于阈值，那么后半部分就要开始value的更新了 # 如果看了P138页的伪代码的(g)部分就会发现，代码的后半部分会直接把priority queue消耗空的，但是这里加了一个限制应该是想限制计算资源的 # 所以前半部分只是来找那些更新值高的state-action，后半部分则更新它的所有满足要求的前导state-action，也就是backward focusing的planning计算 # planning for several steps, # although keep planning until the priority queue becomes empty will converge much faster while planning_step &lt; dyna_params.planning_steps and not model.empty(): # get a sample with highest priority from the model priority, state_, action_, next_state_, reward_ = model.sample() # update the state action value for the sample # delta是sampling得到的target delta = reward_ + dyna_params.gamma * np.max(q_value[next_state_[0], next_state_[1], :]) - \ q_value[state_[0], state_[1], action_] q_value[state_[0], state_[1], action_] += dyna_params.alpha * delta # deal with all the predecessors of the sample state # 使用sample得到的state-next_state进行planning，使用predecessor得到的state_pre-state更新priority-queue # 可以看到在算法实现中，更新了一次Q，带阈值的更新了两次priority-queue for state_pre, action_pre, reward_pre in model.predecessor(state_): priority = np.abs(reward_pre + dyna_params.gamma * np.max(q_value[state_[0], state_[1], :]) - q_value[state_pre[0], state_pre[1], action_pre]) if priority &gt; dyna_params.theta: model.insert(priority, state_pre, action_pre) planning_step += 1 state = next_state # update the # of backups backups += planning_step + 1 return backups 改变planning-step，比较不同的Dyna-Q算法的性能(找到终点的平均step)1234567891011121314151617181920212223242526272829303132333435363738# Figure 8.2, DynaMaze, use 10 runs instead of 30 runsdef figure_8_2(): # set up an instance for DynaMaze dyna_maze = Maze() dyna_params = DynaParams() runs = 10 episodes = 50 planning_steps = [0, 5, 50] steps = np.zeros((len(planning_steps), episodes)) for run in tqdm(range(runs)): for index, planning_step in zip(range(len(planning_steps)), planning_steps): dyna_params.planning_steps = planning_step q_value = np.zeros(dyna_maze.q_size) # generate an instance of Dyna-Q model model = TrivialModel() for ep in range(episodes): # print('run:', run, 'planning step:', planning_step, 'episode:', ep) steps[index, ep] += dyna_q(q_value, model, dyna_maze, dyna_params) # averaging over runs steps /= runs # 输出最终收敛的平均step print(steps[:,-10:].mean(axis=1)) for i in range(len(planning_steps)): plt.plot(steps[i, :], label='%d planning steps' % (planning_steps[i])) plt.xlabel('episodes') plt.ylabel('steps per episode') plt.legend() plt.savefig('./figure_8_2.png') plt.show()# testfigure_8_2() 100%|██████████| 10/10 [00:55&lt;00:00, 5.00s/it] [18.06 17.26 15.9 ] 改变maze障碍的位置，并计算相应的累计reward1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# wrapper function for changing maze# @maze: a maze instance# @dynaParams: several parameters for dyna algorithmsdef changing_maze(maze, dyna_params): # set up max steps max_steps = maze.max_steps # track the cumulative rewards rewards = np.zeros((dyna_params.runs, 2, max_steps)) for run in tqdm(range(dyna_params.runs)): # set up models models = [TrivialModel(), TimeModel(maze, time_weight=dyna_params.time_weight)] # initialize state action values q_values = [np.zeros(maze.q_size), np.zeros(maze.q_size)] for i in range(len(dyna_params.methods)): # print('run:', run, dyna_params.methods[i]) # set old obstacles for the maze maze.obstacles = maze.old_obstacles steps = 0 last_steps = steps while steps &lt; max_steps: # play for an episode steps += dyna_q(q_values[i], models[i], maze, dyna_params) # update cumulative rewards # 使用last_steps的reward更新[last_steps,steps-1]的rewards rewards[run, i, last_steps: steps] = rewards[run, i, last_steps] # steps表示最新的episode的terminal state所对应的在总的steps中的位置，因为terminal-state的reward=1，所以+1 rewards[run, i, min(steps, max_steps - 1)] = rewards[run, i, last_steps] + 1 # 上面两句程序第一次读起来觉得很晦涩，但是这种计算reward的方法之前使用过 # 其实就是将这次的reward建立在上一次episode的reward的基础上，如果将x轴设置为step，则可以看到reward随step的变化趋势 last_steps = steps if steps &gt; maze.obstacle_switch_time: # change the obstacles maze.obstacles = maze.new_obstacles # averaging over runs rewards = rewards.mean(axis=0) return rewards 改变迷宫障碍的位置，比较Dyna-Q和Dyna-Q+方法的性能123456789101112131415161718192021222324252627282930313233343536373839404142# Figure 8.5, BlockingMazedef figure_8_5(): # set up a blocking maze instance blocking_maze = Maze() blocking_maze.START_STATE = [5, 3] blocking_maze.GOAL_STATES = [[0, 8]] blocking_maze.old_obstacles = [[3, i] for i in range(0, 8)] # new obstalces will block the optimal path blocking_maze.new_obstacles = [[3, i] for i in range(1, 9)] # step limit blocking_maze.max_steps = 3000 # obstacles will change after 1000 steps # the exact step for changing will be different # However given that 1000 steps is long enough for both algorithms to converge, # the difference is guaranteed to be very small blocking_maze.obstacle_switch_time = 1000 # set up parameters dyna_params = DynaParams() dyna_params.alpha = 1.0 dyna_params.planning_steps = 10 dyna_params.runs = 20 # kappa must be small, as the reward for getting the goal is only 1 dyna_params.time_weight = 1e-4 # play rewards = changing_maze(blocking_maze, dyna_params) for i in range(len(dyna_params.methods)): plt.plot(rewards[i, :], label=dyna_params.methods[i]) plt.xlabel('time steps') plt.ylabel('cumulative reward') plt.legend() plt.savefig('./figure_8_5.png') plt.show()figure_8_5() 100%|██████████| 20/20 [01:16&lt;00:00, 3.79s/it] 给迷宫添加一条更近的可用路径，比较两个算法的更新情况123456789101112131415161718192021222324252627282930313233343536373839404142# Figure 8.6, ShortcutMazedef figure_8_6(): # set up a shortcut maze instance shortcut_maze = Maze() shortcut_maze.START_STATE = [5, 3] shortcut_maze.GOAL_STATES = [[0, 8]] shortcut_maze.old_obstacles = [[3, i] for i in range(1, 9)] # new obstacles will have a shorter path shortcut_maze.new_obstacles = [[3, i] for i in range(1, 8)] # step limit shortcut_maze.max_steps = 6000 # obstacles will change after 3000 steps # the exact step for changing will be different # However given that 3000 steps is long enough for both algorithms to converge, # the difference is guaranteed to be very small shortcut_maze.obstacle_switch_time = 3000 # set up parameters dyna_params = DynaParams() # 50-step planning dyna_params.planning_steps = 50 dyna_params.runs = 5 dyna_params.time_weight = 1e-3 dyna_params.alpha = 1.0 # play rewards = changing_maze(shortcut_maze, dyna_params) for i in range(len(dyna_params.methods)): plt.plot( rewards[i, :], label=dyna_params.methods[i]) plt.xlabel('time steps') plt.ylabel('cumulative reward') plt.legend() plt.savefig('./figure_8_6.png') plt.show() figure_8_6() 100%|██████████| 5/5 [02:32&lt;00:00, 30.57s/it] 检查当前的Q是否已经是最优12345678910111213141516# Check whether state-action values are already optimaldef check_path(q_values, maze): # get the length of optimal path # 14 is the length of optimal path of the original maze # 1.2 means it's a relaxed optifmal path max_steps = 14 * maze.resolution * 1.2 state = maze.START_STATE steps = 0 while state not in maze.GOAL_STATES: # 使用完全的greedy policy，判断maze step完成的步数是否&lt;max_steps action = np.argmax(q_values[state[0], state[1], :]) state, _ = maze.step(state, action) steps += 1 if steps &gt; max_steps: return False return True 比较Dyna-Q方法和Priority Sweeping方法的性能12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# Example 8.4, mazes with different resolutiondef example_8_4(): # get the original 6 * 9 maze original_maze = Maze() # set up the parameters for each algorithm params_dyna = DynaParams() params_dyna.planning_steps = 5 params_dyna.alpha = 0.5 params_dyna.gamma = 0.95 params_prioritized = DynaParams() params_prioritized.theta = 0.0001 params_prioritized.planning_steps = 5 params_prioritized.alpha = 0.5 params_prioritized.gamma = 0.95 params = [params_prioritized, params_dyna] # set up models for planning models = [PriorityModel, TrivialModel] method_names = ['Prioritized Sweeping', 'Dyna-Q'] # due to limitation of my machine, I can only perform experiments for 5 mazes # assuming the 1st maze has w * h states, then k-th maze has w * h * k * k states num_of_mazes = 5 # build all the mazes mazes = [original_maze.extend_maze(i) for i in range(1, num_of_mazes + 1)] methods = [prioritized_sweeping, dyna_q] # My machine cannot afford too many runs... runs = 5 # track the # of backups backups = np.zeros((runs, 2, num_of_mazes)) for run in range(0, runs): for i in range(0, len(method_names)): for mazeIndex, maze in zip(range(0, len(mazes)), mazes): print('run %d, %s, maze size %d' % (run, method_names[i], maze.WORLD_HEIGHT * maze.WORLD_WIDTH)) # initialize the state action values q_value = np.zeros(maze.q_size) # track steps / backups for each episode steps = [] # generate the model model = models[i]() # 这里不止一个episode了，只有判断Q是最优之后才会跳出 while True: # play for an episode steps.append(methods[i](q_value, model, maze, params[i])) # print best actions w.r.t. current state-action values # printActions(currentStateActionValues, maze) # check whether the (relaxed) optimal path is found if check_path(q_value, maze): break # update the total steps / backups for this maze backups[run, i, mazeIndex] = np.sum(steps) backups = backups.mean(axis=0) # Dyna-Q performs several backups per step backups[1, :] *= params_dyna.planning_steps + 1 for i in range(0, len(method_names)): plt.plot(np.arange(1, num_of_mazes + 1), backups[i, :], label=method_names[i]) plt.xlabel('maze resolution factor') plt.ylabel('backups until optimal solution') plt.yscale('log') plt.legend() plt.savefig('./example_8_4.png') plt.show() example_8_4() run 0, Prioritized Sweeping, maze size 54 run 0, Prioritized Sweeping, maze size 216 run 0, Prioritized Sweeping, maze size 486 run 0, Prioritized Sweeping, maze size 864 run 0, Prioritized Sweeping, maze size 1350 run 0, Dyna-Q, maze size 54 run 0, Dyna-Q, maze size 216 run 0, Dyna-Q, maze size 486 run 0, Dyna-Q, maze size 864 run 0, Dyna-Q, maze size 1350 run 1, Prioritized Sweeping, maze size 54 run 1, Prioritized Sweeping, maze size 216 run 1, Prioritized Sweeping, maze size 486 run 1, Prioritized Sweeping, maze size 864 run 1, Prioritized Sweeping, maze size 1350 run 1, Dyna-Q, maze size 54 run 1, Dyna-Q, maze size 216 run 1, Dyna-Q, maze size 486 run 1, Dyna-Q, maze size 864 run 1, Dyna-Q, maze size 1350 run 2, Prioritized Sweeping, maze size 54 run 2, Prioritized Sweeping, maze size 216 run 2, Prioritized Sweeping, maze size 486 run 2, Prioritized Sweeping, maze size 864 run 2, Prioritized Sweeping, maze size 1350 run 2, Dyna-Q, maze size 54 run 2, Dyna-Q, maze size 216 run 2, Dyna-Q, maze size 486 run 2, Dyna-Q, maze size 864 run 2, Dyna-Q, maze size 1350 run 3, Prioritized Sweeping, maze size 54 run 3, Prioritized Sweeping, maze size 216 run 3, Prioritized Sweeping, maze size 486 run 3, Prioritized Sweeping, maze size 864 run 3, Prioritized Sweeping, maze size 1350 run 3, Dyna-Q, maze size 54 run 3, Dyna-Q, maze size 216 run 3, Dyna-Q, maze size 486 run 3, Dyna-Q, maze size 864 run 3, Dyna-Q, maze size 1350 run 4, Prioritized Sweeping, maze size 54 run 4, Prioritized Sweeping, maze size 216 run 4, Prioritized Sweeping, maze size 486 run 4, Prioritized Sweeping, maze size 864 run 4, Prioritized Sweeping, maze size 1350 run 4, Dyna-Q, maze size 54 run 4, Dyna-Q, maze size 216 run 4, Dyna-Q, maze size 486 run 4, Dyna-Q, maze size 864 run 4, Dyna-Q, maze size 1350]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter07 random walk 19-states]]></title>
    <url>%2F2018%2F12%2F03%2FChapter07-random-walk-19-states%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter07/random_walk.py n-step TD方法在random-walk问题上的应用 问题描述本例通过将不同step的TD方法应用在Chapter06的random-walk问题中，不过将原来的5-state问题修改为了19-state问题。以此来对比不同的n-step算法的性能。 引入模块并定义常量1234567891011121314151617181920212223242526272829import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm# 除了终止state以外的19个stateN_STATES = 19# discountGAMMA = 1# all states but terminal statesSTATES = np.arange(1, N_STATES + 1)# start from the middle state# 0和20是终止state，所以有效state是1-19START_STATE = 10# two terminal states# an action leading to the left terminal state has reward -1# an action leading to the right terminal state has reward 1END_STATES = [0, N_STATES + 1]# 通过bellman equation计算得到真实的V*，这个方法在下面两个地方都有用到：# https://xinge650.github.io/2018/11/22/Chapter03-gird-world/# https://xinge650.github.io/2018/12/01/Chapter06-TD-0-vs-constant-alpha-MC/TRUE_VALUE = np.arange(-20, 22, 2) / 20.0TRUE_VALUE[0] = TRUE_VALUE[-1] = 0 使用n-step TD方法来对policy π进行predict1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# n-steps TD method# @value: values for each state, will be updated# @n: # of steps# @alpha: # step sizedef temporal_difference(value, n, alpha): # initial starting state state = START_STATE # arrays to store states and rewards for an episode # space isn't a major consideration, so I didn't use the mod trick states = [state] rewards = [0] # track the time time = 0 # the length of this episode T = float('inf') while True: # go to next time step # time总是next state的索引 time += 1 if time &lt; T: # choose an action randomly if np.random.binomial(1, 0.5) == 1: next_state = state + 1 else: next_state = state - 1 if next_state == 0: reward = -1 elif next_state == 20: reward = 1 else: reward = 0 # store new state and new reward states.append(next_state) rewards.append(reward) if next_state in END_STATES: # T记录的是terminal state的time-index # 可以看到n-step TD方法也和MC方法一样需要等待整个episode结束，但是Q的更新是在结束前就开始的 # 所以这种方法仍然比MC方法要快 T = time # get the time of the state to update update_time = time - n if update_time &gt;= 0: returns = 0.0 # calculate corresponding rewards for t in range(update_time + 1, min(T, update_time + n) + 1): returns += pow(GAMMA, t - update_time - 1) * rewards[t] # add state value to the return # 如果update_time+n&gt;T，则n-step TD算法退化为constant-α算法，即使用estimate of return来作为target if update_time + n &lt;= T: returns += pow(GAMMA, n) * value[states[(update_time + n)]] state_to_update = states[update_time] # update the state value if not state_to_update in END_STATES: value[state_to_update] += alpha * (returns - value[state_to_update]) if update_time == T - 1: break state = next_state 绘制图表，通过图表观察不同n的算法的性能123456789101112131415161718192021222324252627282930313233343536373839404142# Figure 7.2, it will take quite a whiledef figure_7_2(): # all possible n steps steps = np.power(2, np.arange(0, 10)) # all possible alphas alphas = np.arange(0, 1.1, 0.1) # each run has 10 episodes episodes = 10 # perform 100 independent runs runs = 100 # track the errors for each (step, alpha) combination errors = np.zeros((len(steps), len(alphas))) for run in tqdm(range(0, runs)): for step_ind, step in zip(range(len(steps)), steps): for alpha_ind, alpha in zip(range(len(alphas)), alphas): # print('run:', run, 'step:', step, 'alpha:', alpha) value = np.zeros(N_STATES + 2) for ep in range(0, episodes): temporal_difference(value, step, alpha) # calculate the RMS error errors[step_ind, alpha_ind] += np.sqrt(np.sum(np.power(value - TRUE_VALUE, 2)) / N_STATES) # take average errors /= episodes * runs for i in range(0, len(steps)): plt.plot(alphas, errors[i, :], label='n = %d' % (steps[i])) plt.xlabel('alpha') plt.ylabel('RMS error') plt.ylim([0.25, 0.55]) plt.legend() plt.savefig('./figure_7_2.png') plt.show()figure_7_2() 100%|██████████| 100/100 [08:19&lt;00:00, 4.90s/it]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter07 n-step Bootstrapping]]></title>
    <url>%2F2018%2F12%2F03%2FChapter07-n-step-Bootstrapping%2F</url>
    <content type="text"><![CDATA[上一章提到了强化学习的一种重要方法:TD方法。TD方法结合了DP方法和MC方法的优点，不需要环境信息，并且通过DP的迭代思想大幅提高运算速度。这一章在上一章的基础上深入思考，如果增加更新时候使用的采样点数，即将TD方法推向MC方法，会有哪些新的算法，会获得更好的效果吗？ n-step TD Prediction在引入一种新的控制算法前首先考虑就是prediction方法的实现。首先思考一下MC方法和TD(0)方法的区别：MC方法使用了episode中产生的所有action对应的reward，而TD(0)方法只使用了episode中需要更新的Q(s,a)的后续一个step的Q(s’,a’)。如果我想在两者中间取一个折中？答案很明显吧，就是使用更多的后续state-action和对应的reward。 这里我们便引出了n-step TD的想法，为了统一期间，我们把上一章提到的TD(0)也叫作one-step TD方法，下面给出了不同n的n-step方法的backup diagrams(关于这个图形这本书里见太多了，我的理解就是为了update Q(s,a)需要备份的episode产生的数据，比如state、action、reward等，但是像step size、epsilon、gamma等就不包含其中了，它们应该归于updating algorithm中): 接着给出n-step TD方法的核心更新公式的target： 关于这个更新规则真的不能在说什么了，注意下V的下标，t+n表示是使用的time=t+n的时候的估计值更新的V(S_t)。给出使用n-step TD方法的预测伪代码： 关于这个伪代码其实一眼看上去挺复杂的，但是几个关键的点理解之后就容易很多了: 1、每个episode的循环内部维护一个递增变量time，如果初始的state-action对是(S0,A0)，那么time在第一个循环值=1。对，time表征的就是当前的next state的index。 2、在time &lt; n+1 之前，只进行state-action对的推进，并存储对应的reward和state。当time&gt;=n+1后，按照更新规则进行更新。 3、当time&gt;T后，因为剩下的后续state已经不足n个了，所以更新规则退化为MC方法的更新规则：target=G_{time-n+1}。(G is reutrn …) 例子可以参考 Example7.1 random walk 19-state n-step Sarsa如果弄明白了predict算法，那么控制方法自然是简单啦^_^。这里就讲了一个最简单的on-policy n-step Sarsa方法。 最简单的n-step Sarsa直接把上一部分需要评估的policy改为epsilon-greey就行了。当然这里没有用到的Q(s,a)都是不更新的，本章后面会讲到一种需要使用未出现的Q(s,a)的off-policy算法。 backup diagrams和n-step TD很相似了就不给出了，看一下n-step Sarsa的伪代码: 如果是Expected Sarsa那？给出核心更新规则的target： n-step Off-policy Learning by Importance Sampling引入重要采样因子解决off-policy，调和target policy使用behavior policy产生的数据更新Q(s,a)的问题，更新使用的target如下： 其中重要采样因子(importance sampling ratio)表达式为： 给出使用重要采样因子的off-policy控制算法伪代码: Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithmoff-policy控制算法可以不使用重要采样因子吗？答案是肯定的，不过我们需要借助于那些episode中没有被采用的state-action对的Q(s,a)来完成更新，下面给出backup diagrams： 可以看出tree backup algorithm保留了更多的state-action对。下面具体看一下这种算法是如何将未选用的state-action对引入更新规则的： 首先回顾一下Expected Sarsa的更新规则的target： 然后给出一个2-step的tree backup algorithm算法，是不是很容易理解： 顺水推舟，给出n-step更新规则target的递归定义： 注意虽然更新的时候使用了未被选中的state-action对，但是实际上它们用来更新的是选中的state-action的Q，所以自己的Q并没有被改变… 下面给出tree backup Algorithm的伪代码: woc这次post粘了好多图…最近太懒了。如果岛学家什么时候打折了我可能就会精神一点✧(≖ ◡ ≖✿)]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter06 maximization-bias and Double-Learning]]></title>
    <url>%2F2018%2F12%2F01%2FChapter06-maximization-bias-and-Double-Learning%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter06/maximization_bias.py 因为TD算法中的target policy建立中经常会用到maximization操作，在这些算法中，a maximum over estimated values is used implicitly as an estimate of the maximum value,这可能会导致显著的正向偏差，本例通过一个简单的MRP来讨论这个问题。 问题描述这个例子是为了说明并解决TD方法将估计的最大值作为实际Q的最大值造成的正向偏差(bias)，使用了一个简单的MRP: A是start state，左右的灰色小方格是terminal state。A状态下可以有两个action：left和right；如果向右直接会结束，reward=0；向左则进入B状态，B状态有多个达到左边terminal state的actions可选，并对应reward=N(-0.1,1)，所以(A,left)开始的expect return = -0.1。 引入模块并定义常量12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 6.7 Maximization Bias and Double Learninimport numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm# copy module可以提供浅拷贝和深拷贝方法import copy# state ASTATE_A = 0# state BSTATE_B = 1# use one terminal stateSTATE_TERMINAL = 2# starts from state ASTATE_START = STATE_A# possible actions in AACTION_A_RIGHT = 0ACTION_A_LEFT = 1# probability for explorationEPSILON = 0.1# step sizeALPHA = 0.1# discount for max valueGAMMA = 1.0# possible actions in B, maybe 10 actionsACTIONS_B = range(0, 10)# all possible actionsSTATE_ACTIONS = [[ACTION_A_RIGHT, ACTION_A_LEFT], ACTIONS_B]# state action pair values, if a state is a terminal state, then the value is always 0# 按照顺序分别是:(A,left) (A,right) (B,0) ... (B,9) (C)INITIAL_Q = [np.zeros(2), np.zeros(len(ACTIONS_B)), np.zeros(1)]# set up destination for each state and each action# 这个list通过索引给出next state，之前的都是在take_action函数中顺便给出下一个state的# TRANSITION[state][action] = next-stateTRANSITION = [[STATE_TERMINAL, STATE_B], [STATE_TERMINAL] * len(ACTIONS_B)] 提供choose action和take action函数12345678910111213# choose an action based on epsilon greedy algorithmdef choose_action(state, q_value): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(STATE_ACTIONS[state]) else: values_ = q_value[state] return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])# take @action in @state, return the rewarddef take_action(state, action): if state == STATE_A: return 0 return np.random.normal(-0.1, 1) 使用Q-Learning 和 Double Q-Learning方法来训练并计算policy选择(A|left)的概率123456789101112131415161718192021222324252627282930313233343536373839# if there are two state action pair value array, use double Q-Learning# otherwise use normal Q-Learningdef q_learning(q1, q2=None): state = STATE_START # track the # of action left in state A left_count = 0 while state != STATE_TERMINAL: if q2 is None: action = choose_action(state, q1) else: # derive(得到) a action form Q1 and Q2 # choose action的时候是结合两个Q综合选择的 action = choose_action(state, [item1 + item2 for item1, item2 in zip(q1, q2)]) if state == STATE_A and action == ACTION_A_LEFT: left_count += 1 reward = take_action(state, action) next_state = TRANSITION[state][action] if q2 is None: active_q = q1 target = np.max(active_q[next_state]) else: # 根据50%的概率来选择更新q1还是q2 if np.random.binomial(1, 0.5) == 1: active_q = q1 target_q = q2 else: active_q = q2 target_q = q1 # 从active_q中选择max value的action best_action = np.random.choice([action_ for action_, value_ in enumerate(active_q[next_state]) if value_ == np.max(active_q[next_state])]) # 从target_q中选择与active_q对应的target来更新Q target = target_q[next_state][best_action] # Q-Learning update active_q[state][action] += ALPHA * ( reward + GAMMA * target - active_q[state][action]) state = next_state return left_count 12345678910111213141516171819202122232425262728# Figure 6.7, 1,000 runs may be enough, the number of actions in state B will also affect the curvesdef figure_6_7(): # each independent run has 300 episodes episodes = 300 runs = 1000 left_counts_q = np.zeros((runs, episodes)) left_counts_double_q = np.zeros((runs, episodes)) for run in tqdm(range(runs)): q = copy.deepcopy(INITIAL_Q) q1 = copy.deepcopy(INITIAL_Q) q2 = copy.deepcopy(INITIAL_Q) for ep in range(0, episodes): left_counts_q[run, ep] = q_learning(q) left_counts_double_q[run, ep] = q_learning(q1, q2) left_counts_q = left_counts_q.mean(axis=0) left_counts_double_q = left_counts_double_q.mean(axis=0) plt.plot(left_counts_q, label='Q-Learning') plt.plot(left_counts_double_q, label='Double Q-Learning') plt.plot(np.ones(episodes) * 0.05, label='Optimal') plt.xlabel('episodes') plt.ylabel('% left actions from A') plt.legend() plt.savefig('./figure_6_7.png') plt.show()figure_6_7() 100%|██████████| 1000/1000 [00:44&lt;00:00, 23.10it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter06 Sarsa vs Q-Learning vs Expected Sarsa]]></title>
    <url>%2F2018%2F12%2F01%2FChapter06-Sarsa-vs-Q-Learning-vs-Expected-Sarsa%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter06/windy_grid_world.py、chapter06/cliff_walking.py 本篇包含了两份代码，第一个主要测试了on-policy Sarsa的性能，第二个对标题的三种算法性能进行了比较 一、问题描述和普通的grid-world不同之处是，在格子的中间区域，存在上升的wind，所以在该处的action会附加一个up的action。 引入模块并定义常量12345678910111213141516171819202122232425262728293031323334#Sarsa import numpy as npimport matplotlibimport matplotlib.pyplot as plt%matplotlib inline# world heightWORLD_HEIGHT = 7# world widthWORLD_WIDTH = 10# wind strength for each columnWIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]# possible actionsACTION_UP = 0ACTION_DOWN = 1ACTION_LEFT = 2ACTION_RIGHT = 3# probability for explorationEPSILON = 0.1# Sarsa step sizeALPHA = 0.5# reward for each stepREWARD = -1.0START = [3, 0]GOAL = [3, 7]ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT] 使用on-policy策略的Sarsa来更新state-action-vlaue12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 根据current-state和action对返回next statedef step(state, action): i, j = state if action == ACTION_UP: return [max(i - 1 - WIND[j], 0), j] elif action == ACTION_DOWN: return [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), j] elif action == ACTION_LEFT: return [max(i - WIND[j], 0), max(j - 1, 0)] elif action == ACTION_RIGHT: return [max(i - WIND[j], 0), min(j + 1, WORLD_WIDTH - 1)] else: assert False# play for an episodedef episode(q_value): # track the total time steps in this episode time = 0 # initialize state state = START # choose an action based on epsilon-greedy algorithm if np.random.binomial(1, EPSILON) == 1: action = np.random.choice(ACTIONS) else: values_ = q_value[state[0], state[1], :] action = np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) # keep going until get to the goal state while state != GOAL: next_state = step(state, action) if np.random.binomial(1, EPSILON) == 1: next_action = np.random.choice(ACTIONS) else: values_ = q_value[next_state[0], next_state[1], :] next_action = np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) # Sarsa update q_value[state[0], state[1], action] += \ ALPHA * (REWARD + q_value[next_state[0], next_state[1], next_action] - q_value[state[0], state[1], action]) state = next_state action = next_action time += 1 return time 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def figure_6_3(): q_value = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4)) episode_limit = 10000 q_values = [] steps = [] ep = 0 while ep &lt; episode_limit: steps.append(episode(q_value)) # time = episode(q_value) # episodes.extend([ep] * time) ep += 1 # q_values.append(q_value) # q_values = np.array(q_values) # q_values = np.add.accumulate(q_values) steps = np.add.accumulate(steps)# plt.plot(steps, np.arange(1, len(steps) + 1))# plt.xlabel('Time steps')# plt.ylabel('Episodes') plt.plot(np.arange(1,episode_limit+1),np.around(steps/np.arange(1,episode_limit+1))) plt.xlabel('episodes') plt.ylabel('time steps') plt.ylim(18,22) plt.savefig('./figure_6_3.png') plt.show() # display the optimal policy optimal_policy = [] for i in range(0, WORLD_HEIGHT): optimal_policy.append([]) for j in range(0, WORLD_WIDTH): if [i, j] == GOAL: optimal_policy[-1].append('G') continue #返回最大action的索引index bestAction = np.argmax(q_value[i, j, :]) if bestAction == ACTION_UP: optimal_policy[-1].append('U') elif bestAction == ACTION_DOWN: optimal_policy[-1].append('D') elif bestAction == ACTION_LEFT: optimal_policy[-1].append('L') elif bestAction == ACTION_RIGHT: optimal_policy[-1].append('R') print('Optimal policy is:') for row in optimal_policy: print(row) print('Wind strength for each column:\n&#123;&#125;'.format([str(w) for w in WIND])) figure_6_3() 可以看到随着多个episode的迭代，使用的step数目逐渐收敛。 Optimal policy is: [&#39;R&#39;, &#39;D&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;D&#39;] [&#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;L&#39;, &#39;R&#39;, &#39;D&#39;] [&#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;U&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;D&#39;] [&#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;G&#39;, &#39;R&#39;, &#39;D&#39;] [&#39;R&#39;, &#39;U&#39;, &#39;U&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;U&#39;, &#39;D&#39;, &#39;L&#39;, &#39;L&#39;] [&#39;R&#39;, &#39;D&#39;, &#39;R&#39;, &#39;U&#39;, &#39;R&#39;, &#39;U&#39;, &#39;U&#39;, &#39;D&#39;, &#39;R&#39;, &#39;L&#39;] [&#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;L&#39;] Wind strength for each column: [&#39;0&#39;, &#39;0&#39;, &#39;0&#39;, &#39;1&#39;, &#39;1&#39;, &#39;1&#39;, &#39;2&#39;, &#39;2&#39;, &#39;1&#39;, &#39;0&#39;] 二、问题描述这个问题在grid-world基础上做了一些改动，主要测试了3种算法的性能：Sarsa、Q-Learning、Expect Sarsa。 在一般区域(非阴影部分)action的reward=-1，如果action导致next state落到cliff区域，则reward=-100，而且agent会回到start state。到达goal state的return=0。 引入模块并定义常量12345678910111213141516171819202122232425262728293031import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm# world heightWORLD_HEIGHT = 4# world widthWORLD_WIDTH = 12# probability for explorationEPSILON = 0.1# step sizeALPHA = 0.5# gamma for Q-Learning and Expected SarsaGAMMA = 1# all possible actionsACTION_UP = 0ACTION_DOWN = 1ACTION_LEFT = 2ACTION_RIGHT = 3ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]# initial state action pair valuesSTART = [3, 0]GOAL = [3, 11] function for taking action and choosing action12345678910111213141516171819202122232425262728293031# 根据所给state-action对给出next-state和对应的rewarddef step(state, action): i, j = state if action == ACTION_UP: next_state = [max(i - 1, 0), j] elif action == ACTION_LEFT: next_state = [i, max(j - 1, 0)] elif action == ACTION_RIGHT: next_state = [i, min(j + 1, WORLD_WIDTH - 1)] elif action == ACTION_DOWN: next_state = [min(i + 1, WORLD_HEIGHT - 1), j] else: # 如果action不在上述范围，直接触发异常 assert False reward = -1 if (action == ACTION_DOWN and i == 2 and 1 &lt;= j &lt;= 10) or ( action == ACTION_RIGHT and state == START): reward = -100 next_state = START return next_state, reward# choose an action based on epsilon greedy algorithmdef choose_action(state, q_value): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(ACTIONS) else: # greedy choose values_ = q_value[state[0], state[1], :] return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) 使用on-policy Sarsa方法训练1234567891011121314151617181920212223242526272829303132333435363738# an episode with Sarsa# @q_value: values for state action pair, will be updated# @expected: if True, will use expected Sarsa algorithm# @step_size: step size for updating# @return: total rewards within this episode# sarsa的方法可以按照这个循环来理解：choose action-&gt;take action-&gt;choose next-action-&gt;update，# 所以要把第一次choose action放到循环外面去。# 因为是on-policy方法，所以choose target使用的是基于Q的epsilon-greedy方法，take action更新的是Qdef sarsa(q_value, expected=False, step_size=ALPHA): state = START action = choose_action(state, q_value) rewards = 0.0 while state != GOAL: next_state, reward = step(state, action) next_action = choose_action(next_state, q_value) rewards += reward if not expected: target = q_value[next_state[0], next_state[1], next_action] else: # calculate the expected value of new state target = 0.0 q_next = q_value[next_state[0], next_state[1], :] best_actions = np.argwhere(q_next == np.max(q_next)) for action_ in ACTIONS: if action_ in best_actions: # 通过epsilon-greedy policy的π(a|s)计算期望 target += ((1.0 - EPSILON) / len(best_actions) + EPSILON / len(ACTIONS)) * q_value[next_state[0], next_state[1], action_] else: target += EPSILON / len(ACTIONS) * q_value[next_state[0], next_state[1], action_] target *= GAMMA q_value[state[0], state[1], action] += step_size * ( reward + target - q_value[state[0], state[1], action]) state = next_state action = next_action # rewards是所有action的reward总和 return rewards 使用off-policy Q-Learning方法训练123456789101112131415161718192021# an episode with Q-Learning# @q_value: values for state action pair, will be updated# @step_size: step size for updating# @return: total rewards within this episode# Q-Learning采用循环：choose action-&gt;take action-&gt;update，因为是off-policy的所以结构挺简单的# 个人觉得Q-Learning不是严格的off-policy的，因为target policy通过改变Q，其实也是间接的影响了behavior policy# 所以train data is not strictly off target policydef q_learning(q_value, step_size=ALPHA): state = START rewards = 0.0 while state != GOAL: action = choose_action(state, q_value) next_state, reward = step(state, action) rewards += reward # Q-Learning update q_value[state[0], state[1], action] += step_size * ( reward + GAMMA * np.max(q_value[next_state[0], next_state[1], :]) - q_value[state[0], state[1], action]) state = next_state return rewards 输出优化的action1234567891011121314151617181920# print optimal policydef print_optimal_policy(q_value): optimal_policy = [] for i in range(0, WORLD_HEIGHT): optimal_policy.append([]) for j in range(0, WORLD_WIDTH): if [i, j] == GOAL: optimal_policy[-1].append('G') continue bestAction = np.argmax(q_value[i, j, :]) if bestAction == ACTION_UP: optimal_policy[-1].append('U') elif bestAction == ACTION_DOWN: optimal_policy[-1].append('D') elif bestAction == ACTION_LEFT: optimal_policy[-1].append('L') elif bestAction == ACTION_RIGHT: optimal_policy[-1].append('R') for row in optimal_policy: print(row) 比较Sarsa和Q-Learning的rewards并给出两者的优化action123456789101112131415161718192021222324252627282930313233343536373839404142434445# Use multiple runs instead of a single run and a sliding window# With a single run I failed to present a smooth curve# However the optimal policy converges well with a single run# Sarsa converges to the safe path, while Q-Learning converges to the optimal pathdef figure_6_4(): # episodes of each run episodes = 500 # perform 40 independent runs runs = 50 rewards_sarsa = np.zeros(episodes) rewards_q_learning = np.zeros(episodes) for r in tqdm(range(runs)): q_sarsa = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4)) q_q_learning = np.copy(q_sarsa) for i in range(0, episodes): # cut off the value by -100 to draw the figure more elegantly # rewards_sarsa[i] += max(sarsa(q_sarsa), -100) # rewards_q_learning[i] += max(q_learning(q_q_learning), -100) rewards_sarsa[i] += sarsa(q_sarsa) rewards_q_learning[i] += q_learning(q_q_learning) # averaging over independt runs rewards_sarsa /= runs rewards_q_learning /= runs # draw reward curves plt.plot(rewards_sarsa, label='Sarsa') plt.plot(rewards_q_learning, label='Q-Learning') plt.xlabel('Episodes') plt.ylabel('Sum of rewards during episode') plt.ylim([-100, 0]) plt.legend() plt.savefig('./figure_6_4.png') plt.show() # display optimal policy print('Sarsa Optimal Policy:') print_optimal_policy(q_sarsa) print('Q-Learning Optimal Policy:') print_optimal_policy(q_q_learning)figure_6_4() 100%|██████████| 50/50 [00:56&lt;00:00, 1.15s/it] 最终训练结果Sarsa收敛到较安全的位于上部区域的路径(问题描述图片中的 safe path)，Q-Learning收敛到靠近cliff的路径(optimal path)，但是因为epsilon-greedy的behavior policy，所以会导致Q-Learning偶尔会落入cliff，所以最终的总的reward低于Sarsa。 Sarsa Optimal Policy: [&#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;D&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;D&#39;] [&#39;R&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;R&#39;, &#39;R&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;R&#39;, &#39;D&#39;] [&#39;U&#39;, &#39;R&#39;, &#39;R&#39;, &#39;L&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;L&#39;, &#39;U&#39;, &#39;U&#39;, &#39;R&#39;, &#39;D&#39;] [&#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;G&#39;] Q-Learning Optimal Policy: [&#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;R&#39;, &#39;R&#39;, &#39;U&#39;, &#39;D&#39;, &#39;R&#39;, &#39;R&#39;, &#39;D&#39;, &#39;D&#39;, &#39;D&#39;] [&#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;D&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;D&#39;] [&#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;D&#39;] [&#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;U&#39;, &#39;G&#39;] Interim and asymptotic performance of TD control methods as a function of α12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# Due to limited capacity of calculation of my machine, I can't complete this experiment# with 100,000 episodes and 50,000 runs to get the fully averaged performance# However even I only play for 1,000 episodes and 10 runs, the curves looks still good.def figure_6_6(): # α step_sizes = np.arange(0.1, 1.1, 0.1) episodes = 1000 runs = 10 # Asymptotic: 渐进的 ASY_SARSA = 0 ASY_EXPECTED_SARSA = 1 ASY_QLEARNING = 2 # Interim: 暂时的 INT_SARSA = 3 INT_EXPECTED_SARSA = 4 INT_QLEARNING = 5 methods = range(0, 6) performace = np.zeros((6, len(step_sizes))) for run in range(runs): for ind, step_size in tqdm(list(zip(range(0, len(step_sizes)), step_sizes))): q_sarsa = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4)) q_expected_sarsa = np.copy(q_sarsa) q_q_learning = np.copy(q_sarsa) for ep in range(episodes): sarsa_reward = sarsa(q_sarsa, expected=False, step_size=step_size) expected_sarsa_reward = sarsa(q_expected_sarsa, expected=True, step_size=step_size) q_learning_reward = q_learning(q_q_learning, step_size=step_size) performace[ASY_SARSA, ind] += sarsa_reward performace[ASY_EXPECTED_SARSA, ind] += expected_sarsa_reward performace[ASY_QLEARNING, ind] += q_learning_reward if ep &lt; 100: performace[INT_SARSA, ind] += sarsa_reward performace[INT_EXPECTED_SARSA, ind] += expected_sarsa_reward performace[INT_QLEARNING, ind] += q_learning_reward performace[:3, :] /= episodes * runs performace[3:, :] /= 100 * runs labels = ['Asymptotic Sarsa', 'Asymptotic Expected Sarsa', 'Asymptotic Q-Learning', 'Interim Sarsa', 'Interim Expected Sarsa', 'Interim Q-Learning'] for method, label in zip(methods, labels): plt.plot(step_sizes, performace[method, :], label=label) plt.xlabel('alpha') plt.ylabel('reward per episode') plt.legend() plt.savefig('./figure_6_6.png') plt.show()figure_6_6() 100%|██████████| 10/10 [00:41&lt;00:00, 4.57s/it] 100%|██████████| 10/10 [00:41&lt;00:00, 4.53s/it] 100%|██████████| 10/10 [00:41&lt;00:00, 4.55s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.35s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.36s/it] 100%|██████████| 10/10 [00:41&lt;00:00, 4.40s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.33s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.27s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.37s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.28s/it]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter06 TD(0) vs constant-alpha-MC]]></title>
    <url>%2F2018%2F12%2F01%2FChapter06-TD-0-vs-constant-alpha-MC%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter06/random_walk.py 通过一个例子比较了TD(0)和constant-α MC方法的训练性能 问题描述通过一个MRP来比较constant-α MC Method和TD(0)方法之间的训练性能。 MRP的状态转移示意图： 其中state C 是起始状态，向右达到终点return=1，向左return=0，其余每一步reward=0 所以每个state的true value就是该state到达最右端端点的概率。A-E：1/6,2/6,..5/6 计算也很容易，通过迭代计算列出5个式子，P(s)代表从改点到达右端点的概率： P(E)=1/2 + 1/2 * P(D) P(D)=1/2 P(C) + 1/2 P(E) P(C)=1/2 P(B) + 1/2 P(D) P(B)=1/2 P(A) + 1/2 P(C) P(A)=1/2 * P(B) 计算即可得到上面提到的答案。当然这个方法就是我们在Chapter03讲到的Bellman equation: 引入模块并定义常量12345678910111213141516171819202122232425# 6.2 Advantages of TD Prediction Methodsimport numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm# 0 is the left terminal state# 6 is the right terminal state# 1 ... 5 represents A ... EVALUES = np.zeros(7)VALUES[1:6] = 0.5# For convenience, we assume all rewards are 0# and the left terminal state has value 0, the right terminal state has value 1# This trick has been used in Gambler's ProblemVALUES[6] = 1# set up true state valuesTRUE_VALUE = np.zeros(7)TRUE_VALUE[1:6] = np.arange(1, 6) / 6.0TRUE_VALUE[6] = 1ACTION_LEFT = 0ACTION_RIGHT = 1 使用TD(0)方法update state-value123456789101112131415161718192021222324# @values: current states value, will be updated if @batch is False# @alpha: step size# @batch: whether to update @valuesdef temporal_difference(values, alpha=0.1, batch=False): state = 3 trajectory = [state] # TD方法的rewards array是每一步action产生的reward rewards = [0] while True: old_state = state if np.random.binomial(1, 0.5) == ACTION_LEFT: state -= 1 else: state += 1 # Assume all rewards are 0 reward = 0 trajectory.append(state) # TD update if not batch: values[old_state] += alpha * (reward + values[state] - values[old_state]) if state == 6 or state == 0: break rewards.append(reward) return trajectory, rewards 使用Monte Carlo方法update state-value1234567891011121314151617181920212223242526272829# @values: current states value, will be updated if @batch is False# @alpha: step size# @batch: whether to update @valuesdef monte_carlo(values, alpha=0.1, batch=False): state = 3 trajectory = [3] # if end up with left terminal state, all returns are 0 # if end up with right terminal state, all returns are 1 while True: if np.random.binomial(1, 0.5) == ACTION_LEFT: state -= 1 else: state += 1 trajectory.append(state) if state == 6: # monte carlo方法的return是指从state s开始到结束产生的收益,即G_t returns = 1.0 break elif state == 0: returns = 0.0 break if not batch: for state_ in trajectory[:-1]: # MC update values[state_] += alpha * (returns - values[state_]) # 因为中间状态的reward=0，所以所以trajectory上的state的return都是一样的，并且取决于最终结束状态 return trajectory, [returns] * (len(trajectory) - 1) 通过使用TD(0)方法建立state-value，并绘制收敛图像1234567891011121314151617# Example 6.2 leftdef compute_state_value(): episodes = [0, 1, 10, 100] current_values = np.copy(VALUES) plt.figure(1) for i in tqdm(range(episodes[-1] + 1)): if i in episodes: plt.plot(current_values, label=str(i) + ' episodes') temporal_difference(current_values) plt.plot(TRUE_VALUE, label='true values') plt.xlabel('state') plt.ylabel('estimated value') plt.legend()# test# 可以看到结果逐渐收敛了# compute_state_value() 计算并比较TD(0)和constant α-MC method的rms(均方差误差)1234567891011121314151617181920212223242526272829303132333435# Example 6.2 rightdef rms_error(): # Same alpha value can appear in both arrays td_alphas = [0.15, 0.1, 0.05] mc_alphas = [0.01, 0.02, 0.03, 0.04] episodes = 100 + 1 runs = 100 # list 相加相当于将两个list首尾相接 for i, alpha in enumerate(td_alphas + mc_alphas): total_errors = np.zeros(episodes) if i &lt; len(td_alphas): method = 'TD' linestyle = 'solid' else: method = 'MC' linestyle = 'dashdot' for r in tqdm(range(runs)): errors = [] current_values = np.copy(VALUES) for i in range(0, episodes): errors.append(np.sqrt(np.sum(np.power(TRUE_VALUE - current_values, 2)) / 5.0)) if method == 'TD': temporal_difference(current_values, alpha=alpha) else: monte_carlo(current_values, alpha=alpha) total_errors += np.asarray(errors) total_errors /= runs plt.plot(total_errors, linestyle=linestyle, label=method + ', alpha = %.02f' % (alpha)) plt.xlabel('episodes') plt.ylabel('RMS') plt.legend()# test# 可以看到TD方法收敛的更快，rms更低# rms_error() 使用一般的方法进行state-value收敛并绘制图像12345678910111213def example_6_2(): plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) compute_state_value() plt.subplot(2, 1, 2) rms_error() plt.tight_layout() plt.savefig('./example_6_2.png') plt.show()example_6_2() 100%|██████████| 101/101 [00:00&lt;00:00, 7677.14it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 173.89it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 179.95it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 169.18it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 247.49it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 239.68it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 236.73it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 229.08it/s] 使用batch-update方法优化state-value的收敛过程并绘制图像123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# Figure 6.2(Example 6.3)# @method: 'TD' or 'MC'def batch_updating(method, episodes, alpha=0.001): # perform 100 independent runs runs = 100 total_errors = np.zeros(episodes) for r in tqdm(range(0, runs)): current_values = np.copy(VALUES) errors = [] # track shown trajectories and reward/return sequences trajectories = [] rewards = [] for ep in range(episodes): # batch=True将导致state-value不被更新 if method == 'TD': trajectory_, rewards_ = temporal_difference(current_values, batch=True) else: trajectory_, rewards_ = monte_carlo(current_values, batch=True) trajectories.append(trajectory_) rewards.append(rewards_) while True: # keep feeding our algorithm with trajectories seen so far until state value function converges updates = np.zeros(7) for trajectory_, rewards_ in zip(trajectories, rewards): # len(trajectory)-1表明不考虑终止状态 for i in range(0, len(trajectory_) - 1): if method == 'TD': updates[trajectory_[i]] += rewards_[i] + current_values[trajectory_[i + 1]] - current_values[trajectory_[i]] else: updates[trajectory_[i]] += rewards_[i] - current_values[trajectory_[i]] updates *= alpha if np.sum(np.abs(updates)) &lt; 1e-3: break # perform batch updating current_values += updates # calculate rms error errors.append(np.sqrt(np.sum(np.power(current_values - TRUE_VALUE, 2)) / 5.0)) total_errors += np.asarray(errors) total_errors /= runs return total_errorsdef figure_6_2(): episodes = 100 + 1 td_erros = batch_updating('TD', episodes) mc_erros = batch_updating('MC', episodes) plt.plot(td_erros, label='TD') plt.plot(mc_erros, label='MC') plt.xlabel('episodes') plt.ylabel('RMS error') plt.legend() plt.savefig('./figure_6_2.png') plt.show() figure_6_2() 100%|██████████| 100/100 [00:45&lt;00:00, 2.29it/s] 100%|██████████| 100/100 [00:37&lt;00:00, 2.49it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter06 Temporal-Difference Learning]]></title>
    <url>%2F2018%2F12%2F01%2FChapter06-Temporal-Difference-Learning%2F</url>
    <content type="text"><![CDATA[Temporal Difference method 可以说是强化学习算法的代表，它结合了Monte Carlo方法和DP方法的优点：TD方法不需要借助环境的信息(model-free)，可以直接通过模拟得到经验进行学习；同时它像DP算法一样，一个state的估计(estimate)可以通过其它部分的估计来进行更新，而不需要等待完整的episode结束。 TD PredictionTD方法和MC方法都是根据生成的经验来学习的，但是MC方法需要在episode结束后才能更新，比如对一个非平稳问题有如下更新规则的every-visit MC方法： TD methods need to wait only until the next time step. At time t+1 they immediately form a target and make a useful update using the observed rewardR_{t+1} and the estimate V(S_{t+1}). The simplest TD method makes the update, which we call it as TD(0): 接着给出TD(0)方法预测(prediction)的伪代码： 关于TD方法的理论基础，我们需要分析一下target的由来。target的概念在第二章Bandit的增量实现那一部分提到过，在强化学习中使用的更新通式： 从第三章我们推导出这样的公式： 可以看到Monte Carlo方法是使用(6.3)的近似来作为target，使用样本返回的reward来近似；DP方法使用(6.4)的近似作为target。TD方法将两者结合起来，首先它并没有使用R_{t+1}的期望，而是使用了一次抽样的reward来近似，其次没有使用基于policy π的expect value，而是使用了它当前的近似V来更新。所以可以认为TD方法结合了Monte Carlo的抽样原理，使它不必基于model，同时基于DP的原理使得TD可以快速迭代更新。 这一部分关于TD方法的预测基本完成了，但是书中还提到了一个量δ_t，这个值是update rule右边括号里的值，是V(s)更新到更好的值所使用的偏差。因为需要采样来进行更新，所以δ_t只有到t+1时刻才可以知道。 如果假设V array的值不变，或者因为step size的值α很小，认为V array的更新比较缓慢，则有下述等式近似成立： Generalizations of this identity play an important role in the theory and algorithms of temporal-difference learning. Advantages of TD Prediction Methods这部分回答了几个问题： 1、TD方法比DP好吗？比MC好吗？ TD vs DP: Obviously, TD methods have an advantage over DP methods in that they do not require a model ofthe environment, of its reward and next-state probability distributions. TD vs MC: (1)One of most obvious advantage of TD methods over Monte Carlo methods is that they are naturallyimplemented in an on-line, fully incremental fashion. (2)With Monte Carlo methods one must wait untilthe end of an episode, because only then is the return known, whereas with TD methods one need waitonly one time step. Surprisingly often this turns out to be a critical consideration. Some applicationshave very long episodes, so that delaying all learning until the end of the episode is too slow. Otherapplications are continuing tasks and have no episodes at all. (3)Finally, as we noted in the previouschapter, some Monte Carlo methods must ignore or discount episodes on which experimental actionsare taken, which can greatly slow learning. TD methods are much less susceptible to these problemsbecause they learn from each transition regardless of what subsequent actions are taken. 2、TD方法收敛吗？ 答案是肯定的。对于任意给定的policy π，TD(0)方法的结果被证明会收敛到v_π，如果使用一个小的固定step size那么收敛情况是平均的(我觉得应该是在附近很小的振荡吧)或者变化的，如果使用满足如下条件的递减step size则一定会收敛v_π。 3、TD方法和MC方法那个训练起来更快？ 这个目前还没有人能通过数学推导出两者的速度孰快孰慢，但是一般来说TD方法总是比constant-α MC方法快一些。 这部分的实验Example6.2 Random Walk的代码可以看这个。 Optimality of TD(0)这一部分主要讲了batch updating的优化方法，这个方法不同于之前的方法,V(S_t)不是在得到V(S_{t+1})后就直接更新了,而是先模拟了n个完整的episode,然后再用每个episode计算相应的target值,相当于原来增量以episode为单位的总和，再用这个总和进行更新，如果value收敛了，这个episode就算用完了，再换下一个episode继续操作。这种方法适用于训练样本比较少的情况，在相同样本数量下这种方法训练速度较慢，而且训练结果从和理论值计算得到的rms error来看并没有很大的优化。 使用batch updating方法更新的python代码其中的temporal_difference(value,batch)函数是使用TD方法更新的函数，如果batch=True则不执行数据更新，只返回trajectory和reward的两个array； monte_carlo(value,batch)函数是使用MC方法更新的函数，如果batch=True，只返回trajectory和return的两个array。 同时因为这里使用的是MRP(example6.3)，所以可以直接计算state-value(model-based)。 12345678910111213141516171819202122232425262728293031323334353637for r in tqdm(range(0, runs)): # current_value是当前对v_π的近似(estimate) current_values = np.copy(VALUES) trajectories = [] rewards = [] for ep in range(episodes): if method == 'TD': # trajectory_ 是单次episode的轨迹，包括了(1)中提到的S0,S1,S2,...,S_n # rewards在TD方法里指的是每一步action引入的reward。 trajectory_, rewards_ = temporal_difference(current_values, batch=True) else: # trajectory_ 是单次episode的轨迹，包括了(1)中提到的S0,S1,S2,...,S_n # rewards在MC方法里指的是每个state的return trajectory_, rewards_ = monte_carlo(current_values, batch=True) # 将每一个episode产生的trajectory还有rewards都推进宏观的trajectories和rewards # 这两个list随着实验进行长度迅速增长 trajectories.append(trajectory_) rewards.append(rewards_) while True: # update相当于累加版的target update = np.zeros(7) for trajectory_, rewards_ in zip(trajectories, rewards): # len(trajectory)-1表明不考虑终止状态 # 使用所有的trajectory和reward计算所以trajectory上的state的update for i in range(0, len(trajectory_) - 1): if method == 'TD': updates[trajectory_[i]] += rewards_[i] + current_values[trajectory_[i + 1]] - current_values[trajectory_[i]] else: updates[trajectory_[i]] += rewards_[i] - current_values[trajectory_[i]] updates *= alpha if np.sum(np.abs(updates)) &lt; 1e-3: break # 使用update进行一次batch update current_values += updates 现在来讨论一下Example6.4并对TD和MC性能差异给出解释首先给出8个episode： A, 0, B, 0 B, 1 B, 1 B, 1 B, 1 B, 1 B, 1 B, 0 这是一条参数未知的MRP，如何根据给出的episode来估计其中的参数？ 首先来讨论一下value(B)，因为TD(0)method 和MC method更新最后一个state的方法一致，所有最终会收敛到最大似然估计值：0.75。 最大似然估计值在《统计学习方法》里经常可以看到，抛开详细的证明，根据频率来逼近概率的思路就是最大似然估计，所以这里对v(B)的最大似然估计就是出现1的概率：0.75。 但是在v(A)的计算中，TD和MC方法出现了分歧： MC方法直接从train data中来，所有只有一次A出现，而且G=0，所以很自然v(A)=0; 但是如果用TD方法来估计，因为reward(action:A-&gt;B)=0，所以最终v(A)会和v(B)一起收敛到0.75。 比较这两个方法可以看到，MC方法在train data上计算出来的rms error无疑更小，但是我们仍然认为TD方法更好，因为TD方法在未来或者说未知数据上可能会有更好的近似性。PS:这个问题比较像欠拟合和过拟合的辨析。 这里有一个问题，关于last state value，在github issue上问了一下代码作者，他给出的解释是最后一个state的value始终是0。的确是这样的，因为最后一个state都不更新action了，所以不会有value更新的。思考之后，我觉得问题出在Example6.4上，因为它这里让我预测的B的value，正是最后一个state的value!所以如果需要代码解决这个问题，需要在B之后再加上两个分别对应reward=0和reward=1的terminal state才行。 从Example6.4总结得到结论batch MC方法总是找到使训练集上的均方误差(rms error)最小的estimate，而batch-TD(0)总是找到对马尔可夫过程的最大似然模型精确的估计。 通常，参数的最大似然估计是其生成数据的概率最大的参数值。在这种情况下，最大似然估计得到的是马尔可夫过程的模型，该模型以明显的方式由观测事件形成：从i到j的转移概率的估计是从i到j的观测转移的频率，并且关联的期望报酬是在这些转变中观察到的回报。给定这个模型，我们可以计算值函数的估计，如果模型是正确的，则该估计将完全正确。这被称为确定性等价性估计(certainty-equivalence estimate)，因为它等价于假定基础过程的估计是确定的，而不是近似的。总体上，批处理TD(0)收敛于确定性等价估计。 总体来说，TD方法得到的estimate是最大似然估计(maximum-likelihood)，得到的是确定性等价评估；而MC方法则太过于注重样本的结果，得到的是训练样本的无差估计。虽然这个结论是我们从batch updating得到的，但是nonbatch算法的收敛趋势和batch updating是一致的，即总的方向是粗略的朝着batch updating的方向的。从而我们证明了TD方法训练效果和收敛速度快于constant-α MC。 Sarsa: On-policy TD Control从预测算法得到控制算法或者提升算法(chapter04)，重要的是引入greedy-policy。在DP方法那一章引入的GPI方法几乎给出了控制算法的”通解”。所以可以仿照上一章给出的基于MC方法的on-policy控制算法，给出TD方法的on-policy控制算法——Sarsa: target-policy:ε-greedy or ε-soft policies. convergence properties of the Sarsa algorithm: depend on the nature of the policy’s dependence on Q, ε-greedy or ε-soft policies are OK. Sarsa converges with probability 1 to an optimal policy and action-value function as long as all state–action pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy (which can be arranged, for example, with ε-greedy policies by setting ε = 1/t). Q-learning: Off-policy TD ControlQ-learning 是off-policy的控制方法，因为在update Q的时候，使用的不是behavior policy(比如ε-greedy)，而是使用的next-state对应的maximization state-action-value。伪代码如下： Expected Sarsa如果把Sarsa的update规则更换为如下： 就是Expected sarsa算法。 Expected sarsa算法相比Sarsa方法表现更好，因为引入了期望来更新Q，一定程度上消除了因为随机选择Action带来的方差(variance)。从cliff-walk的例子中我们可以看到，state的转换都是确定的，随机性或者说不确定性都是policy引入的，因为Expected Sarsa引入了期望，所以可以设置α=1，并且不会降低渐近性能(asymptotic performance)，而Sarsa只能把α设置的低一些才能有效的运行，并且只有长期训练才有好的效果。关于渐近性能和临时性能的比较，可以参考post。 Maximization Bias and Double Learning所谓的最大正偏差，产生的原因就是在算法中使用的maximization操作。这种maximization操作无形中将估计Q值中的最大值用作了对最大q*的估计值： In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a significant positive bias. 比如，假设只有一个state，并且它有很多对q(s,a)=0。但是estimate of q(s,a)有大于0的也有小于0的。如果把估计值中的最大值用作了estimate of maximization q*(s,a)，那么就会产生一个正偏差。 那么如何解决这个问题那？ One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. 意思就是维护两个q(s,a)的估计Q1和Q2，当估计max q*的时候，可以先找到对应最大Q1的A*，然后使用Q2(A*)来估计max q*。原理就是E[Q2(A*)] = q(A*)这个期望公式是无偏的。同理也可以反过来更新Q1，二者谁更新的问题就好比抛硬币正反面一样，可以使用二项分布来决定。这种算法和Q-learning结合起来就可以得到避免最大正向偏差的控制算法：Double Q-learning. 更新规则下例，反过来同理: Example6.7代码可以参考这个post Games, Afterstates, and Other Special Cases算法根据不同的情况也需要进行一些调整，给出的方法只是一种参考格式，并不一定适应所有问题。 一个例子就是在导论中学习的tic-tac-toe问题，可以看到我们在这里学习的value function并不每一步的state-action-value，而是采取action后当前棋盘的所有棋子分布的value。因为只考虑action的话，就无法将对手的走子情况考虑进去，作为一个竞技游戏很明显是不合适的。比如下面的两种棋盘，虽然所处state和采取action都不同，但是结果是一样的，所以这种afterstate的更新方法更加有效可靠。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter05 infinite_variance]]></title>
    <url>%2F2018%2F11%2F28%2FChapter05-infinite-variance%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter05/infinite_variance.py 通过一个例子论证了ordinary importance sampling的不稳定性 问题描述这个程序通过一个简单的例子证明了ordinary importance sampling 的方差经常会发生不收敛的问题。 本例使用了一个只有一个状态s和两个状态left和right，以及一个terminate state的MDP问题，详细的Reward和转移概率如下所示： 选择的target_policy是：π(left|s)=1,π(right|s)=0； 选择生成episode的behavior policy是:b(left|s)=b(right|s)=0.5 满足π cover b的要求，并根据target_policy可以估计出v_π(s)=1，接下来看看代码运行结果，看看通过behavior policy预测出来的可以收敛到什么情况。 引入模块并定义常量，其中action_back=left，action_end=right12345678import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom tqdm import tqdm %matplotlib inlineACTION_BACK = 0ACTION_END = 1 定义behavior-policy和target-policy并开始训练1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# behavior policydef behavior_policy(): return np.random.binomial(1, 0.5)# target policydef target_policy(): return ACTION_BACK# one turn# 返回reward和action trajectory，因为state已知，所以不用指定def play(): # track the action for importance ratio trajectory = [] while True: action = behavior_policy() trajectory.append(action) if action == ACTION_END: return 0, trajectory if np.random.binomial(1, 0.9) == 0: return 1, trajectorydef figure_5_4(): runs = 10 episodes = 100000 for run in tqdm(range(runs)): # 每轮run之间都是独立的 rewards = [] for episode in range(0, episodes): reward, trajectory = play() if trajectory[-1] == ACTION_END: rho = 0 else: rho = 1.0 / pow(0.5, len(trajectory)) rewards.append(rho * reward) rewards = np.add.accumulate(rewards) estimations = np.asarray(rewards) / np.arange(1, episodes + 1) plt.plot(estimations) plt.plot(np.ones(episodes+1)) plt.xlabel('Episodes (log scale)') plt.ylabel('Ordinary Importance Sampling') plt.xscale('log') plt.ylim(0,2) plt.savefig('./figure_5_4.png') plt.show()figure_5_4() 100%|██████████| 10/10 [00:06&lt;00:00, 1.59it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter05 blackjack]]></title>
    <url>%2F2018%2F11%2F28%2FChapter05-blackjack%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter05/blackjack.py 实现了基于Monte Carlo方法的三种算法： 1、基于Monte Carlo方法的策略预测，根据给出的策略计算state-value 2、使用exploring starts的训练方法，得出state-action-value，以及对应的优化policy π 3、使用off-policy的重要取样方法预测state-value，并和期望值对比 问题描述blackjack是一个常见的赌场游戏，规则如下： 牌组和牌值：总的牌堆由7到8组牌去掉大小王组成，所以不用考虑牌数量问题；其中2-10是原值，J-K当做10使用，A分两种情况，可以等于1 or 11。 规则：游戏开始，发牌的兄弟给玩家和庄家各发2张牌，玩家的牌是亮出来的明牌，庄家的是一张明牌一张暗牌，如果玩家当场数值和到21了(natural)，即拿到了一张A和一张10(或者J-K)，如果庄家没有达到21，那么判庄家输(这里不考虑赌金什么的)，反正则平局；如果玩家没有到21，可以选择继续要牌(hit)或者放弃要牌(strick or stand)，如果超过21了就叫做爆牌(go bust)，玩家就被判负；如果玩家strick1了，就轮到庄家发牌了，庄家一般会按照这样的方法来决策(也可以不这样，这里是一种规定吧)：如果牌值&lt;17，就hit，在17-21之间stand，如果庄家goes bust，庄家就被判负；如果庄家stand了，就比较双方的总牌值，大的一方为胜方。 这里使用policy：玩家hit当牌值&lt;20，20-21就stand。 monte carlo算法本身不难理解，但是这个blackjack问题可以说是目前接触到的最复杂的强化学习问题了。首先是state的理解，state=[usable_ace_player, player_sum, dealer_card1]，其中usable_ace_player指的是player是否将A用作11，player_sum指的是Player牌组总值，dealer_card1指的是庄家亮出的明牌。 算法中使用了很多blackjack游戏的游戏技巧，比如上面提到的一些策略，还有state的选取，增加了理解难度。 所以这个问题主要理解monte carlo算法工作原理，具体的技巧可以选择性忽略。 引入模块，并定义action常量，hit=继续抽，stand=停止1234567891011import numpy as npimport matplotlibimport matplotlib.pyplot as pltimport seaborn as snsfrom tqdm import tqdm%matplotlib inline# actions: hit or standACTION_HIT = 0ACTION_STAND = 1 # "strike" in the bookACTIONS = [ACTION_HIT, ACTION_STAND] 为player定义policy，属于游戏技巧，也是本例的一个参考，通过最终学到的policy和这里的POLICY_PLAYER对比来比较算法的优劣123456# policy for playerPOLICY_PLAYER = np.zeros(22)for i in range(12, 20): POLICY_PLAYER[i] = ACTION_HITPOLICY_PLAYER[20] = ACTION_STANDPOLICY_PLAYER[21] = ACTION_STAND 两个待定函数，这里的target_policy_player和behavior_policy_player的工作方式是理解off-policy Monte Carlo算法的关键1234567891011# use for off-policy method# function form of target policy of playerdef target_policy_player(usable_ace_player, player_sum, dealer_card): return POLICY_PLAYER[player_sum]# function form of behavior policy of playerdef behavior_policy_player(usable_ace_player, player_sum, dealer_card): # probality = 0.5 according to binomial distruction if np.random.binomial(1, 0.5) == 1: return ACTION_STAND return ACTION_HIT 为庄家定义policy，属于游戏技巧123456# policy for dealerPOLICY_DEALER = np.zeros(22)for i in range(12, 17): POLICY_DEALER[i] = ACTION_HITfor i in range(17, 22): POLICY_DEALER[i] = ACTION_STAND Monte Carlo方法的关键一步，通过模拟游戏来获得sample123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157# get a new card# 根据游戏规则发牌并定义牌值def get_card(): card = np.random.randint(1, 14) card = min(card, 10) return card# play a game# @policy_player: 为Player制定的policy，也是训练的目标？# @initial_state: 初始状态[whether player has a usable Ace, sum of player's cards, one card of dealer]# @initial_action: 初始行为 the initial action# return:(state,reward,player_trajectory)# 返回变量解释：# state：就是得到的初始状态，如果initial_state=None，就是随机产生的，反正其实就是initial_state的deep copy# reward：是相应的初始状态的最终结果，Player win=1，Player lost=-1，平局=0# player_trajectory：以[state,action]为元素的list，记录整个实验过程中的state和对应的actiondef play(policy_player, initial_state=None, initial_action=None): # player status # sum of player player_sum = 0 # trajectory of player player_trajectory = [] # whether player uses Ace as 11 usable_ace_player = False # dealer status dealer_card1 = 0 dealer_card2 = 0 usable_ace_dealer = False if initial_state is None: # generate a random initial state num_of_ace = 0 # initialize cards of player # 这里是一个理解的难点，主要是因为对游戏不太理解 # 游戏规定是要一开始给玩家和庄家发2张牌，这里没给初始状态，所以随机抽呗 # 但是问题是，这个循环把Player的牌值一直抽到11才跳出，也就是说其中大概率不止抽了2张，怎么回事？ # 因为反正发完牌也是让Player先搞，而且前面按照给的经验玩法，在12到20之间都是无脑hit的，所以直接在这抽到12算了。 while player_sum &lt; 12: # for _ in range(2): # if sum of player is less than 12, always hit card = get_card() # if get an Ace, use it as 11 if card == 1: num_of_ace += 1 card = 11 usable_ace_player = True player_sum += card # if player's sum is larger than 21, he must hold at least one Ace, two Aces are possible # 这里的理解也很有意思，就是如果超过21了，那么考虑前一个while循环，数值肯定是&lt;=11，而最后一次抽最大是11(A)，所以超过21 # 只有一种情况，就是前一次11，这次又抽到了A，所以肯定至少有一个A，当然两个A也是有可能的。 if player_sum &gt; 21: # use the Ace as 1 rather than 11 player_sum -= 10 # if the player only has one Ace, then he doesn't have usable Ace any more if num_of_ace == 1: usable_ace_player = False # initialize cards of dealer, suppose dealer will show the first card he gets # 这里也验证了前面的推测，因为下一次不论到庄家抽卡，所以庄家这里就老老实实抽了2张 # 不过其中card1是可见的，card2是暗牌看不到，所以card1也是会影响决策的(怎么影响就是游戏经验了) dealer_card1 = get_card() dealer_card2 = get_card() else: # use specified initial state usable_ace_player, player_sum, dealer_card1 = initial_state dealer_card2 = get_card() # initial state of the game state = [usable_ace_player, player_sum, dealer_card1] # initialize dealer's sum # 计算得到庄家的牌值总值，当然这个对玩家是不可见的。 dealer_sum = 0 if dealer_card1 == 1 and dealer_card2 != 1: dealer_sum += 11 + dealer_card2 usable_ace_dealer = True elif dealer_card1 != 1 and dealer_card2 == 1: dealer_sum += dealer_card1 + 11 usable_ace_dealer = True elif dealer_card1 == 1 and dealer_card2 == 1: dealer_sum += 1 + 11 usable_ace_dealer = True else: dealer_sum += dealer_card1 + dealer_card2 # game starts! # player's turn while True: # 第一次会对玩家的行为套用初始action，以后不会用了 if initial_action is not None: action = initial_action initial_action = None else: # get action based on current sum # 写了这么变量，其实就是根据state和给定的policy给出action action = policy_player(usable_ace_player, player_sum, dealer_card1) # track player's trajectory for importance sampling # 将state-action对append进player_trajectory player_trajectory.append([(usable_ace_player, player_sum, dealer_card1), action]) if action == ACTION_STAND: break # if hit, get new card player_sum += get_card() # player busts if player_sum &gt; 21: # if player has a usable Ace, use it as 1 to avoid busting and continue if usable_ace_player == True: player_sum -= 10 usable_ace_player = False else: # otherwise player loses return state, -1, player_trajectory # dealer's turn while True: # get action based on current sum action = POLICY_DEALER[dealer_sum] if action == ACTION_STAND: break # if hit, get a new card new_card = get_card() if new_card == 1 and dealer_sum + 11 &lt; 21: dealer_sum += 11 usable_ace_dealer = True else: dealer_sum += new_card # dealer busts if dealer_sum &gt; 21: if usable_ace_dealer == True: # if dealer has a usable Ace, use it as 1 to avoid busting and continue dealer_sum -= 10 usable_ace_dealer = False else: # otherwise dealer loses return state, 1, player_trajectory # compare the sum between player and dealer if player_sum &gt; dealer_sum: return state, 1, player_trajectory elif player_sum == dealer_sum: return state, 0, player_trajectory else: return state, -1, player_trajectory 使用on-policy策略训(yu)练(ce)算法伪代码： 其实给的target_policy才是last boss，这里只是使用Monte Carlo的方法来预测了一下，看看结果是不是符合的。 这里有一个比较有意思的地方感觉可以讨论一下： 算法的最后一部分，也就是循环的最小部分，即对value的更新，因为blackjack问题本身就是state不重复的，所以first-visit和every-visit是一样的；其次就是G，这个G是被累加了，因为这里S_t是按照索引减小的方向移动的，而且注意符号，这里累加了R_{t+1}，这个形式和上一章的MDP问题中的表达式是一致的，关于这里R的索引是n还是n+1，我觉得也有必要提一下。因为在第二章Bandit问题里，R是用的n： 而且这里对value的更新也是从前往后的，state是有限的； 而第4章DP的时候，R使用的就变成n+1了： 而且也变成反向更新了（虽然形式上仍是正向写的code，但是事实上最先确定的是最终的state-value） 仔细看的话会发现，其实这两个问题还是差别挺大的，首先说下bandit问题把。Bandit问题是通过学习找到最优的平均收益，所以n控制的试验次数，所以其实本质上它只计算了一个value值，就是最终目标是为了收敛到最优的action，即找到reward期望最大的action。 MDP问题中的t代表的是state随step出现的时刻，即使一般问题可能不满足MDP的马尔科夫性，但是对于一个普遍的多状态序列决策问题，t时刻的state对应的value肯定是其后时刻states的value的一种和的形式(通常会考虑discount如果问题是continuing task)。所以两个问题本质就是不同的，不能混淆。 再谈谈另一个问题，就是伪算法最里层计算state-value的时候，计算顺序是沿着t减小的方向的，因为这样有利于迭代，其实也是一种DP的思路。但是这个blackjack问题有它的特殊性，就是每一步action的reward是设为0的，只有最后的结果才是有reward的。这样设计是有道理的，避免达到次优点嘛。 看到这里我不禁想到，Monte Carlo方法和MDP其实原理是相通的，都利用的DP的思路来解决问题，我记得在MDP那一章也讲过这个问题，在原书46页： 123456789101112131415161718192021222324252627282930# Monte Carlo Sample with On-Policydef monte_carlo_on_policy(episodes): states_usable_ace = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided states_usable_ace_count = np.ones((10, 10)) states_no_usable_ace = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided states_no_usable_ace_count = np.ones((10, 10)) for i in tqdm(range(0, episodes)): _, reward, player_trajectory = play(target_policy_player) for (usable_ace, player_sum, dealer_card), _ in player_trajectory: # 这里也是一个理解的难点，一点一点分析： # 1、“player_sum-=12”这个操作，通过play函数我发现，append进player_trajectory # 中的state的player_sum一定是&lt;=21的，即state中的action是在player_sum基础上的，player_sum # 的值是在action之前的值。那么可以知道player_sum-12&lt;9 # 接着分析，-12的操作结果是否会向下溢出，根据play函数中的处理，player_sum一定是从&gt;=12的值开始的， # 这一点可以从play函数一开始的初始状态的处理看出来，while循环只有当player_sum&gt;=12才会跳出 # 所以player_sum-12 ~ [0,9]，刚好符合states_usable_ace等4个np.ndarray的范围 # 同理，dealer_card-1也是一样的道理，将[1,10]的范围换算到[0,9] # 那dealer_card就不可能是11吗？根据play函数的分析，dealer_card在计算dealer_sum的时候 # 需要根据另一张暗牌的牌值来计算，所以对于player来说只能认为它是1（暗牌不可见） player_sum -= 12 dealer_card -= 1 if usable_ace: states_usable_ace_count[player_sum, dealer_card] += 1 states_usable_ace[player_sum, dealer_card] += reward else: states_no_usable_ace_count[player_sum, dealer_card] += 1 states_no_usable_ace[player_sum, dealer_card] += reward return states_usable_ace / states_usable_ace_count, states_no_usable_ace / states_no_usable_ace_count 绘制图表1234567891011121314151617181920212223242526272829def figure_5_1(): states_usable_ace_1, states_no_usable_ace_1 = monte_carlo_on_policy(10000) states_usable_ace_2, states_no_usable_ace_2 = monte_carlo_on_policy(500000) states = [states_usable_ace_1, states_usable_ace_2, states_no_usable_ace_1, states_no_usable_ace_2] titles = ['Usable Ace, 10000 Episodes', 'Usable Ace, 500000 Episodes', 'No Usable Ace, 10000 Episodes', 'No Usable Ace, 500000 Episodes'] _, axes = plt.subplots(2, 2, figsize=(40, 30)) plt.subplots_adjust(wspace=0.1, hspace=0.2) axes = axes.flatten() for state, title, axis in zip(states, titles, axes): fig = sns.heatmap(np.flipud(state), cmap="YlGnBu", ax=axis, xticklabels=range(1, 11), yticklabels=list(reversed(range(12, 22)))) fig.set_ylabel('player sum', fontsize=30) fig.set_xlabel('dealer showing', fontsize=30) fig.set_title(title, fontsize=30) plt.savefig('./figure_5_1.png') plt.show() figure_5_1() 100%|██████████| 10000/10000 [00:00&lt;00:00, 51795.15it/s] 100%|██████████| 500000/500000 [00:08&lt;00:00, 55705.87it/s] 使用exploring-start训练算法的伪代码如下： 注意其中几个要点： 在Monte Carlo模拟的时候，即play函数里，使用的player_policy是greey的； 初始状态是explore的，并保证每个初始state和action组合都有概率出现 1234567891011121314151617181920212223242526272829303132333435363738394041# Monte Carlo with Exploring Startsdef monte_carlo_es(episodes): # (playerSum, dealerCard, usableAce, action) state_action_values = np.zeros((10, 10, 2, 2)) # initialze counts to 1 to avoid division by 0 state_action_pair_count = np.ones((10, 10, 2, 2)) # behavior policy is greedy def behavior_policy(usable_ace, player_sum, dealer_card): usable_ace = int(usable_ace) player_sum -= 12 dealer_card -= 1 # get argmax of the average returns(s, a) values_ = state_action_values[player_sum, dealer_card, usable_ace, :] / \ state_action_pair_count[player_sum, dealer_card, usable_ace, :] # np.random.choice 函数通过给定一个list或者np.ndarray或者整数n，以及一个表示选择概率的list来从一系列数中 # 按照概率选出相应的值 # 如果第一个参数是n，其实是相当于np.arange(n)的，第二个概率如果不指定就默认uniform distribution(均匀分布) # 这里是从最大的value的action中选出任一个的 return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) # play for several episodes for episode in tqdm(range(episodes)): # for each episode, use a randomly initialized state and action # 使用exploring start-action对 initial_state = [bool(np.random.choice([0, 1])), np.random.choice(range(12, 22)), np.random.choice(range(1, 11))] initial_action = np.random.choice(ACTIONS) # 这个地方挺有意思的，第一次先使用target_policy，免得state_action_value一直是0，陷入死循环 current_policy = behavior_policy if episode else target_policy_player _, reward, trajectory = play(current_policy, initial_state, initial_action) for (usable_ace, player_sum, dealer_card), action in trajectory: usable_ace = int(usable_ace) player_sum -= 12 dealer_card -= 1 # update values of state-action pairs state_action_values[player_sum, dealer_card, usable_ace, action] += reward state_action_pair_count[player_sum, dealer_card, usable_ace, action] += 1 return state_action_values / state_action_pair_count 绘制图表1234567891011121314151617181920212223242526272829303132333435def figure_5_2(): state_action_values = monte_carlo_es(500000) state_value_no_usable_ace = np.max(state_action_values[:, :, 0, :], axis=-1) state_value_usable_ace = np.max(state_action_values[:, :, 1, :], axis=-1) # get the optimal policy action_no_usable_ace = np.argmax(state_action_values[:, :, 0, :], axis=-1) action_usable_ace = np.argmax(state_action_values[:, :, 1, :], axis=-1) images = [action_usable_ace, state_value_usable_ace, action_no_usable_ace, state_value_no_usable_ace] titles = ['Optimal policy with usable Ace', 'Optimal value with usable Ace', 'Optimal policy without usable Ace', 'Optimal value without usable Ace'] _, axes = plt.subplots(2, 2, figsize=(40, 30)) plt.subplots_adjust(wspace=0.1, hspace=0.2) axes = axes.flatten() for image, title, axis in zip(images, titles, axes): fig = sns.heatmap(np.flipud(image), cmap="YlGnBu", ax=axis, xticklabels=range(1, 11), yticklabels=list(reversed(range(12, 22)))) fig.set_ylabel('player sum', fontsize=30) fig.set_xlabel('dealer showing', fontsize=30) fig.set_title(title, fontsize=30) plt.savefig('./figure_5_2.png') plt.show() figure_5_2() 100%|██████████| 500000/500000 [00:31&lt;00:00, 15708.47it/s] 使用off-policy策略训练(预测)通过计算Ordinary Importance Sampling 和 Weighted Importance Sampling并和target_policy训练的结果比对，分别计算均方误差来测试算法的性能。 重点是两个重要取样的意义和计算： 重要取样(Importance Sampling)的推导是通过MDP得到的，它是一种off-policy方法中通过behavior policy来训练target policy的重要途径，其中一个重要的概念就是重要取样比率(importance-sampling ratio)，它的定义式是target policy下的状态轨迹的π(a|s)和behavior policy下的状态轨迹的b(a|s)的比值。具体推导如下： 首先定义通过behavior policy取样得到的states-chain的联合概率，这里的进一步计算使用了马尔科夫独立性假设： 然后给出重要取样比率的定义： 其中的状态转移概率p被消去了，可以证明这个结论虽然是通过马尔科夫独立性推出的，却具有一般性。 好了，现在我们基本理解了重要采样比率的概念，那么它到底有什么用那？下面的公式解释了这个问题（具体推导不清楚QAQ）： 这个值是联系episode的return和target policy π下的v_π(s)的关键！ 所以很直观的从这个期望公式引出ordinary importance sampling： 但是有一个问题，就是如果重要采样因子有可能是方差无限的，这时我们近似得到的state-value就会发散而无法收敛，这个问题很严重，随后也会通过代码来解释。 所以通过对采样因子做归一化，引出了Weighted importance sampling的概念： 和前者相比，虽然weighted importance sampling是有偏的(bias)，因为它不是直接从上面的期望公式来的，但是经过多次迭代bias会趋于0。但是如果采样因子的方差不是有限的，ordinary importance sampling就很有可能无法收敛，也就是说它的方差(variance)是高于weighted importance sampling的，所以总的来说后者更常用。 12345678910111213141516171819202122232425262728293031323334353637# Monte Carlo Sample with Off-Policydef monte_carlo_off_policy(episodes): # 不用exploring start了，所以可以直接指定起始状态 initial_state = [True, 13, 2] rhos = [] returns = [] for i in range(0, episodes): _, reward, player_trajectory = play(behavior_policy_player, initial_state=initial_state) # get the importance ratio numerator = 1.0 denominator = 1.0 for (usable_ace, player_sum, dealer_card), action in player_trajectory: if action == target_policy_player(usable_ace, player_sum, dealer_card): denominator *= 0.5 else: numerator = 0.0 break rho = numerator / denominator rhos.append(rho) returns.append(reward) rhos = np.asarray(rhos) returns = np.asarray(returns) weighted_returns = rhos * returns weighted_returns = np.add.accumulate(weighted_returns) rhos = np.add.accumulate(rhos) ordinary_sampling = weighted_returns / np.arange(1, episodes + 1) with np.errstate(divide='ignore',invalid='ignore'): weighted_sampling = np.where(rhos != 0, weighted_returns / rhos, 0) return ordinary_sampling, weighted_sampling 绘制图表12345678910111213141516171819202122232425def figure_5_3(): true_value = -0.27726 episodes = 10000 runs = 100 error_ordinary = np.zeros(episodes) error_weighted = np.zeros(episodes) for i in tqdm(range(0, runs)): ordinary_sampling_, weighted_sampling_ = monte_carlo_off_policy(episodes) # get the squared error error_ordinary += np.power(ordinary_sampling_ - true_value, 2) error_weighted += np.power(weighted_sampling_ - true_value, 2) error_ordinary /= runs error_weighted /= runs plt.plot(error_ordinary, label='Ordinary Importance Sampling') plt.plot(error_weighted, label='Weighted Importance Sampling') plt.xlabel('Episodes (log scale)') plt.ylabel('Mean square error') plt.xscale('log') plt.legend() plt.savefig('./figure_5_3.png') plt.show() figure_5_3() 100%|██████████| 100/100 [00:20&lt;00:00, 4.87it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter05 Monte Carlo Methods]]></title>
    <url>%2F2018%2F11%2F28%2FChapter05-Monte-Carlo-Methods%2F</url>
    <content type="text"><![CDATA[Unlike the previous chapter, here we do not assume complete knowledge of the environment. Monte Carlo methods require only experience—sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Learning from actual experience is striking because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior. Monte Carlo Prediction蒙特卡洛预测是利用蒙特卡洛方法的最简单的强化学习算法，通过给定的policy进行模拟，并根据模拟episodes来进行state-value期望计算。 这里要注意的是，预测分为first-visit和every-visit，前者是本章讲述的重点，后面其他的算法也都是first-visit的，它的意思是指模拟过程中只对一次状态s计算return，换句话说episode中只能存在一个s，如果存在多个s的episode这种方法就会直接舍弃； 后者会在后面章节讲述，every-visit会对episode中所有s的return进行计算。 first-visit的Monte Carlo预测伪代码： Monte Carlo Estimation of Action Values看了网上的一些解释，如果是model-free的强化学习问题，都是要学习Q(s,a)的，只有model-based的方法可以直接学习v(s)。 其实感觉model-free是一种更普适的方法，因为一个一般的决策问题如果知道行为带来的value的话决策起来就会很舒服，直接greedy action嘛。 但是如果我对环境有一些了解，比如像MDP中那样使用model-based的学习方法，知道状态间的转移概率p(s,a)，那么我在训练的过程中不同step对应的state可以通过动态规划联系在一起，即state和following states之间并没有通过action来连接。那么我在学习的时候就不用去考虑不同的action导致的Q(s,a)，具体的算法可以参考前面MDP的post。那我自然只通过建立v(s)就够了。 这里关于如何建立action-value有一点需要注意：在MDP问题中，每个state都会被更新，所以是不需要maintain exploring的；但是对于action来说，如果采用完全greedy的policy的话，所有的action不一定都会选中，这个问题和第2章的Bandit问题一样，所以需要在greedy policy之外保持一种exploring的策略。于是这里便引入了exploring start的方法，即每次episode的初始state-action会保证所有的state-action对都有概率被选中。 Monte Carlo Control蒙特卡洛控制通过上一章提到的GPI(generalized policy iteration)的思路来解决的。就是控制环保有一个近似的value function和一个近似的policy，the value function is repeatedly altered(改变) to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to thecurrent value function.GPI的流程可以简单理解为下图： 这个地方和上一章DP算法提到的policy Iteration和value Iteration原理是一致的就不多赘述了。下面看一下基于ES(exploring start)的Control方法： Monte Carlo Control without Exploring StartExploring start的假设在实际问题中不是很常见，所以这一部分提出了两个without exploring start的控制方法：on-policy和off-policy的。 首先需要了解on和off-policy之间的区别和联系： off-policy是通过behavior policy来模拟并产生数据(episode)，但是学习得到的是target policy π，即使用的episode is “off” the target policy π； on-policy则在模拟数据和学习时都使用同一个policy。一般来说on-policy更简单易用，off-policy则会有更多的参数和更复杂的学习过程，收敛起来也比较慢，但是off-policy可以适的范围更广，可以更好的解决问题。而且如果off-policy的behavior policy = target policy，二者就一致了。 这一部分主要讨论了on-policy的控制方法，先看一下算法的伪代码： 算法提升原理如下所示，这里的q(s,π’(s))指的是在π’策略下的s的state-value。π’是上面伪代码中的ε-greedy策略。 Off-policy Prediction via Importance Samplingbehavior policy vs target policy关于target policy π cover behavior policy b的概念，这里什么才能被称为cover(覆盖)那？ 文中解释是这样的：通过π选取的action一定有概率通过b选取，即通过π(a|s)&gt;0一定可以推出b(a|s)&gt;0，前者是后者的充分条件。 what is importance sampling重要取样(Importance Sampling)的推导是通过MDP得到的，它是一种off-policy方法中通过behavior policy来训练target policy的重要途径，其中一个重要的概念就是重要取样比率(importance-sampling ratio)，它的定义式是target policy下的状态轨迹的π(a|s)和behavior policy下的状态轨迹的b(a|s)的比值。具体推导如下： 首先定义通过behavior policy取样得到的states-chain的联合概率，这里的进一步计算使用了马尔科夫独立性假设： 然后给出重要取样比率的定义： 其中的状态转移概率p被消去了，可以证明这个结论虽然是通过马尔科夫独立性推出的，却具有一般性。 好了，现在我们基本理解了重要采样比率的概念，那么它到底有什么用那？下面的公式解释了这个问题（具体推导不清楚QAQ）： 这个值是联系episode的return和target policy π下的v_π(s)的关键！ 所以很直观的从这个期望公式引出ordinary importance sampling： 但是有一个问题，就是如果重要采样因子有可能是方差无限的，这时我们近似得到的state-value就会发散而无法收敛，这个问题很严重，随后也会通过代码来解释。 所以通过对采样因子做归一化，引出了Weighted importance sampling的概念： 和前者相比，虽然weighted importance sampling是有偏的(bias)，因为它不是直接从上面的期望公式来的，但是经过多次迭代bias会趋于0。但是如果采样因子的方差不是有限的，ordinary importance sampling就很有可能无法收敛，也就是说它的方差(variance)是高于weighted importance sampling的，所以总的来说后者更常用。 Incremental Implementation for Off-policy Prediction增量算法实现和第2章的增量实现原理类似，先给出简单的证明： 设V_n是模拟过程中第n对(s,a)，G1，G2,…Gn是behavior policy产生的对应第n对(s,a)的return，则有下述表达式成立: 额感觉翻译的不够好，粘一下原文吧： Suppose we have a sequence of returns G1 , G2 , . . . , Gn−1 , all starting in the same state and each with a corresponding random weight Wi (e.g., Wi = ρ_{t:T(t)−1} ). We wish to form the estimate: 增量形式表达式: 其中： 紧接着给出预测的增量形式伪代码： Off-policy Monte Carlo Control这一章脉络很清晰啊，基本是先阐述预测算法，再讨论控制算法。这种方法在本书中很常见，因为policy的improve，还是control算法都是建立在predict(or evalution)的基础上的，再加上greey的policy提升方法得到的。下面给出off-policy的增量控制算法： 针对这个算法有几点想讨论的： 1、b是soft-policy的，soft是指b在生成模拟过程的时候对所有的状态动作对的发生概率都不为0，即可以尽可能多的产生不同的过程，相当于一种exploring吧。原文解释是： policy π is soft, meaning that π(a|s)&gt;0 for all s and a. ε-soft policy is defined as policies for which π(a|s) ≥ ε/|A(s)| for all states and actions, for some ε &gt; 0 2、如果使用π(St)得到的action和episode中的At不同，那么重要采样因子的分子是为0的，此时不对W进行更新。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter04 gamblers_problem]]></title>
    <url>%2F2018%2F11%2F23%2FChapter04-gamblers-problem%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter04/gamblers_problem.py 赌徒问题 问题描述一个赌徒可以在每轮赌博中决定将自己手里的钱拿来赌硬币的正反，如果硬币向上，则可以获得押金一样的奖励，但是向下的话押金就没了。结束条件是赌徒手里的钱增长到100，或者把钱输光。 这个问题可以定义为state为赌徒手里的钱，action为每次拿去赌的钱，discount=1的MDP问题。 引入模块并定义全局变量12345678910111213import numpy as npimport matplotlibimport matplotlib.pyplot as plt%matplotlib inline# goalGOAL = 100# all states, including state 0 and state 100STATES = np.arange(GOAL + 1)# probability of head, which is the probability win moneyHEAD_PROB = 0.4 Value Iteration需要注意几点： 初始化value-state的时候，除了100的状态为1，其余都为0，可以理解为除了到达100可以获得reward=1，其余action对应reward=0，即利用value-state initialize来实现reward。 训练的时候把action=0去掉，是因为aciton=0会导致agent陷入局部最优，所以需要跳出这个点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def figure_4_3(): # state value initialize state_value = np.zeros(GOAL + 1) state_value[GOAL] = 1.0 # value iteration while True: delta = 0.0 for state in STATES[1:GOAL]: # get possilbe actions for current state actions = np.arange(1,min(state, GOAL - state) + 1) action_returns = [] for action in actions: action_returns.append( HEAD_PROB * state_value[state + action] + (1 - HEAD_PROB) * state_value[state - action]) new_value = np.max(action_returns) delta += np.abs(state_value[state] - new_value) # update state value state_value[state] = new_value if delta &lt; 1e-9: break # compute the optimal policy policy = np.zeros(GOAL + 1) for state in STATES[1:GOAL]: actions = np.arange(1,min(state, GOAL - state) + 1) action_returns = [] for action in actions: action_returns.append( HEAD_PROB * state_value[state + action] + (1 - HEAD_PROB) * state_value[state - action]) # round to resemble the figure in the book, see # https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/issues/83# policy[state] = actions[np.argmax(np.round(action_returns[1:], 5)) + 1] policy[state] = actions[np.argmax(np.round(action_returns,5))] plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) plt.plot(state_value) plt.xlabel('Capital') plt.ylabel('Value estimates') plt.subplot(2, 1, 2) plt.scatter(STATES, policy) plt.xlabel('Capital') plt.ylabel('Final policy (stake)') plt.savefig('./figure_4_3.png') plt.show()figure_4_3() 中奖率=0.4 中奖率=0.1 中奖率=0.8(所以说中奖率太高也不能浪吗。。。)]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter04 car_rental]]></title>
    <url>%2F2018%2F11%2F23%2FChapter04-car-rental%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter04/car_rental.py car rental 问题 问题描述这个问题就比较复杂了。。。说是Jack是两个汽车租赁公司的老板，他收入是靠租车出去，租出一辆赚10刀，每次有人还车，那么第二天这车就可以租出去了；每天夜里可以将一个地方的车运到另一个地方，不过每运一辆车要花2刀。关于业务，Jack发现了一些斯巴拉西的规律：每个地方每天汽车租借和归还的数量都遵循泊松分布： 我们就把两个位置称为first和second吧。 first的汽车每天借出去λ=3，归还λ=3； second每天借出去λ=4，归还λ=2； 并且每个地方汽车库存不能超过20辆，超过了总公司就会回收； 夜里从一个地方运到另一个地方的汽车数量不能超过5辆。 这个问题我们把它设计成discount因子=0.9的MDP问题，step是每一天，action是每晚运的车，并设从first运到second为正，从second运到first为负，state是first和second可以租赁车的总数量。同时做一个简化，就是如果泊松分布n&gt;10，就把概率人为截断为0。 引入模块并定义常量123456789101112131415161718192021222324252627282930313233343536373839import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom math import exp, factorialimport seaborn as sns%matplotlib inline# maximum # of cars in each locationMAX_CARS = 20# maximum # of cars to move during nightMAX_MOVE_OF_CARS = 5# expectation for rental requests in first locationRENTAL_REQUEST_FIRST_LOC = 3# expectation for rental requests in second locationRENTAL_REQUEST_SECOND_LOC = 4# expectation for # of cars returned in first locationRETURNS_FIRST_LOC = 3# expectation for # of cars returned in second locationRETURNS_SECOND_LOC = 2DISCOUNT = 0.9# credit earned by a carRENTAL_CREDIT = 10# cost of moving a carMOVE_CAR_COST = 2# all possible actionsactions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)# An up bound for poisson distribution# If n is greater than this value, then the probability of getting n is truncated to 0POISSON_UPPER_BOUND = 11 进行policy evaluation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Probability for poisson distribution# @lam: lambda should be less than 10 for this functionpoisson_cache = dict()def poisson(n, lam): global poisson_cache key = n * 10 + lam if key not in poisson_cache.keys(): poisson_cache[key] = exp(-lam) * pow(lam, n) / factorial(n) return poisson_cache[key]# @state: [# of cars in first location, # of cars in second location]# @action: positive if moving cars from first location to second location,# negative if moving cars from second location to first location# @stateValue: state value matrix# @constant_returned_cars: if set True, model is simplified such that# the # of cars returned in daytime becomes constant# rather than a random value from poisson distribution, which will reduce calculation time# and leave the optimal policy/value state matrix almost the samedef expected_return(state, action, state_value, constant_returned_cars): # initailize total return returns = 0.0 # cost for moving cars returns -= MOVE_CAR_COST * abs(action) # go through all possible rental requests for rental_request_first_loc in range(0, POISSON_UPPER_BOUND): for rental_request_second_loc in range(0, POISSON_UPPER_BOUND): # moving cars num_of_cars_first_loc = int(min(state[0] - action, MAX_CARS)) num_of_cars_second_loc = int(min(state[1] + action, MAX_CARS)) # valid rental requests should be less than actual # of cars real_rental_first_loc = min(num_of_cars_first_loc, rental_request_first_loc) real_rental_second_loc = min(num_of_cars_second_loc, rental_request_second_loc) # get credits for renting reward = (real_rental_first_loc + real_rental_second_loc) * RENTAL_CREDIT num_of_cars_first_loc -= real_rental_first_loc num_of_cars_second_loc -= real_rental_second_loc # probability for current combination of rental requests # possion(n,lam) # P(AB) = P(A)*P(B) prob = poisson(rental_request_first_loc, RENTAL_REQUEST_FIRST_LOC) * \ poisson(rental_request_second_loc, RENTAL_REQUEST_SECOND_LOC) if constant_returned_cars: # get returned cars, those cars can be used for renting tomorrow returned_cars_first_loc = RETURNS_FIRST_LOC returned_cars_second_loc = RETURNS_SECOND_LOC num_of_cars_first_loc = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS) num_of_cars_second_loc = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS) returns += prob * (reward + DISCOUNT * state_value[num_of_cars_first_loc, num_of_cars_second_loc]) else: for returned_cars_first_loc in range(0, POISSON_UPPER_BOUND): for returned_cars_second_loc in range(0, POISSON_UPPER_BOUND): num_of_cars_first_loc_ = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS) num_of_cars_second_loc_ = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS) prob_ = poisson(returned_cars_first_loc, RETURNS_FIRST_LOC) * \ poisson(returned_cars_second_loc, RETURNS_SECOND_LOC) * prob returns += prob_ * (reward + DISCOUNT * state_value[num_of_cars_first_loc_, num_of_cars_second_loc_]) return returns 进行policy iteration123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def figure_4_2(constant_returned_cars=True): value = np.zeros((MAX_CARS + 1, MAX_CARS + 1)) policy = np.zeros(value.shape, dtype=np.int) iterations = 0 _, axes = plt.subplots(2, 3, figsize=(40, 20)) plt.subplots_adjust(wspace=0.1, hspace=0.2) axes = axes.flatten() while True: fig = sns.heatmap(np.flipud(policy), cmap="YlGnBu", ax=axes[iterations]) fig.set_ylabel('# cars at first location', fontsize=30) fig.set_yticks(list(reversed(range(MAX_CARS + 1)))) fig.set_xlabel('# cars at second location', fontsize=30) fig.set_title('policy %d' % (iterations), fontsize=30) # policy evaluation (in-place) while True: new_value = np.copy(value) for i in range(MAX_CARS + 1): for j in range(MAX_CARS + 1): new_value[i, j] = expected_return([i, j], policy[i, j], new_value, constant_returned_cars) value_change = np.abs((new_value - value)).sum() print('value change %f' % (value_change)) value = new_value if value_change &lt; 1e-4: break # policy improvement new_policy = np.copy(policy) for i in range(MAX_CARS + 1): for j in range(MAX_CARS + 1): action_returns = [] for action in actions: if (action &gt;= 0 and i &gt;= action) or (action &lt; 0 and j &gt;= abs(action)): action_returns.append(expected_return([i, j], action, value, constant_returned_cars)) else: action_returns.append(-float('inf')) new_policy[i, j] = actions[np.argmax(action_returns)] policy_change = (new_policy != policy).sum() print('policy changed in %d states' % (policy_change)) policy = new_policy if policy_change == 0: fig = sns.heatmap(np.flipud(value), cmap="YlGnBu", ax=axes[-1]) fig.set_ylabel('# cars at first location', fontsize=30) fig.set_yticks(list(reversed(range(MAX_CARS + 1)))) fig.set_xlabel('# cars at second location', fontsize=30) fig.set_title('optimal value', fontsize=30) break iterations += 1 plt.savefig('./figure_4_2.png') plt.show() # figure_4_2() 注意图片，总共经过4次迭代最终收敛，前5个图片是policy的温度分布图，最后一个是value的分布图。还有一点需要注意，就是这个任务没有终止状态，所以是属于continuing task，discount因子&lt;1，而本章第一个和第三个都是有终止状态的，所以是undiscount的。]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter04 gird-world using DP]]></title>
    <url>%2F2018%2F11%2F23%2FChapter04-gird-world-using-DP%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter04/gird_world.py grid_world(policy evaluation) 问题描述4X4 网格： 左上角和右下角是终止状态(terminal state)，如果action使得state跳转到外面了，就返回上次位置，所有的action造成的reward都是-1。 引入模块并定义常量1234567891011121314import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom matplotlib.table import Table%matplotlib inlineWORLD_SIZE = 4# left, up, right, downACTIONS = [np.array([0, -1]), np.array([-1, 0]), np.array([0, 1]), np.array([1, 0])]ACTION_PROB = 0.25 action控制代码12345678910111213141516# judge whether comes to terminal statedef is_terminal(state): x, y = state return (x == 0 and y == 0) or (x == WORLD_SIZE - 1 and y == WORLD_SIZE - 1)# return the next_state s' and reward rdef step(state, action): state = np.array(state) next_state = (state + action).tolist() x, y = next_state if x &lt; 0 or x &gt;= WORLD_SIZE or y &lt; 0 or y &gt;= WORLD_SIZE: next_state = state.tolist() reward = -1 return next_state, reward 绘制方格图123456789101112131415161718192021222324252627def draw_image(image): fig, ax = plt.subplots() ax.set_axis_off() tb = Table(ax, bbox=[0, 0, 1, 1]) nrows, ncols = image.shape width, height = 1.0 / ncols, 1.0 / nrows # Add cells for (i,j), val in np.ndenumerate(image): # Index either the first or second item of bkg_colors based on # a checker board pattern idx = [j % 2, (j + 1) % 2][i % 2] color = 'white' tb.add_cell(i, j, width, height, text=val, loc='center', facecolor=color) # Row Labels... for i, label in enumerate(range(len(image))): tb.add_cell(i, -1, width, height, text=label+1, loc='right', edgecolor='none', facecolor='none') # Column Labels... for j, label in enumerate(range(len(image))): tb.add_cell(-1, j, width, height/2, text=label+1, loc='center', edgecolor='none', facecolor='none') ax.add_table(tb) 进行policy evaluation，即计算value-state123456789101112131415161718192021222324def compute_state_value(in_place=False): new_state_values = np.zeros((WORLD_SIZE, WORLD_SIZE)) state_values = new_state_values.copy() iteration = 1 while True: # in place algorithm is faster than 2-array edition src = new_state_values if in_place else state_values for i in range(WORLD_SIZE): for j in range(WORLD_SIZE): if is_terminal([i, j]): continue value = 0 for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) value += ACTION_PROB * (reward + src[next_i, next_j]) new_state_values[i, j] = value if np.sum(np.abs(new_state_values - state_values)) &lt; 1e-4: state_values = new_state_values.copy() break state_values = new_state_values.copy() iteration += 1 return state_values, iteration 运行并显示评估结果123456789101112def figure_4_1(): values, sync_iteration = compute_state_value(in_place=False) _, asycn_iteration = compute_state_value(in_place=True) # show the sync DP evaluation draw_image(np.round(values, decimals=2)) print('In-place: %d iterations' % (asycn_iteration)) print('Synchronous: %d iterations' % (sync_iteration)) plt.savefig('./figure_4_1.png') plt.show()figure_4_1() In-place: 142 iterations Synchronous: 218 iterations]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter04 Dynamic Programming]]></title>
    <url>%2F2018%2F11%2F23%2FChapter04-Dynamic-Programming%2F</url>
    <content type="text"><![CDATA[上一章讲了马尔科夫决策过程的概念，末尾提出了针对MDP的value-state、value-action-state建立方法，很明显可以看出是使用了动态规划的方法，这一章就在上一章基础上，进一步讲述如何使用动态规划来训练以及优化MDP问题的强化学习算法。 对了这里给一个概念辨析，关于MDP和MRP的，其实两者是不一样的，通过状态转移得到reward的是MRP，通过action得到reward的是MDP。不过话说，有区别吗？action的结果就是state的转换。哈哈，还有有区别的，因为在本书中例子都比较简单，一般action和state的转换基本是对应的，有一个例子可以帮助理解这个问题，就是trajectory sampling的例程，这里每个action对应一个随机的branch，而每个branch对应的next state也是随机的，所以用MDP来解释的话，reward是对应action的，不同的action对应reward；用MRP解释的话，action之后，还要考虑导致了的哪个branch最终转到哪个state了，然后reward根据(current state,next state)来给定，这就和MDP差别很大了。一般简单来说两者是可以混用的… 4.1 Policy Evaluation首先理解什么是策略的评估策略的评估也可以认为是一种预测行为，是解决MDP问题的必要环节。通过评估，我们使用已有的policy选择action（大部分问题policy是一个随机函数，即按照一定的概率分布产生action），使用动态规划的方法更新整个state-set的value并达到收敛。评估结果即value-table，是我们决策的重要依据，所以可以为未知的player如何行动提供预测的参考。 评估模型经常使用迭代运算的方式，迭代同时分为原地迭代和旧值迭代，区别仅是在动态规划使用未来状态来更新当前value的时候，是否使用更新后的值，运算通式是： 评估算法的伪代码： 4.2 Policy Improvement一开始看的时候，一直没明白上一步评估的作用何在，在策略提升这部分就讲了，策略评估是为了改进用的，因为如果在某个state s上，我改变了action a-&gt;a’，那么可以认为我采用了一种新的策略π’，即这一步我采取的行为是π’(s)，如果有： 则可以推出新的策略π’一定不差于原来的π，即： 推导过程： 顺势我们推出一种greedy的提升方法，就是直接获得最大reward的action： 这种方法也为下一部分的Policy迭代优化提供一种优化的思路。 4.3 Policy Iteration直接给出迭代优化的伪代码： 可以看出来算法其实就是评估和提升交替进行的，迭代终止条件就是没办法对当前的v_π(s)进一步优化了。 4.4 Value Iteration上面的策略迭代优化的方法很明显有个问题，就是每次提升的前提是需要对策略进行评估，value Iteration提出一种将提升和评估放在一起进行的方法，这个思路比较像4.1中评估的时候使用的in-place方法。 算法伪代码如下： 算法结束条件就是当value-table更新幅度小于阈值Θ时停止。 4.5 4.6 4.7因为后面三部分都很短，也没有给出具体的解释，所以我就放一块写。 4.5讲的是异步动态规划，其实前面讲的value iteration就是异步动态规划的一种，主要是希望可以改进DP算法会遍历整个state set的问题，有的value state没必要多次更新，而有的可以多次更新，具体的算法会在第8章提出。 4.6讲的是统一的策略迭代(GPI)，意指evaluation和improvement是相互竞争合作的，大部分强化学习都是这两者相互作用达到最优的policy和state-value。 4.7讲的是动态规划算法的效率，大致意思就是动规对于large-state的问题计算量仍旧很大，但是可以用异步动规来解决，policy iteration和value iteration现在仍很常用。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter03 gird-world]]></title>
    <url>%2F2018%2F11%2F22%2FChapter03-gird-world%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter03/gird_world.py 使用MDP的强化学习算法解决Grid-world问题 任务解释(example 3.5 in chapter 03)grid_world代表了一个网格，网格中每个小格子代表一种状态。其中每个状态可以有4种action:left、right、up、down。对应reward规则如下： 如果action导致agent跑到网格外面去，则reward=-1； 如果agent从A出发，则reward=10，下个状态是固定的A_PRIME_POS；从B出发，reward=5，下个状态时固定的B_PRIME_POS； 其他的state上的action均为0。 示意图如下： 引入模块，定义常量12345678910111213141516171819import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom matplotlib.table import Table%matplotlib inlineWORLD_SIZE = 5A_POS = [0, 1]A_PRIME_POS = [4, 1]B_POS = [0, 3]B_PRIME_POS = [2, 3]DISCOUNT = 0.9# left, up, right, downACTIONS = [np.array([0, -1]), np.array([-1, 0]), np.array([0, 1]), np.array([1, 0])]ACTION_PROB = 0.25 获取next_state（base on action），和对应的reward123456789101112131415def step(state, action): if state == A_POS: return A_PRIME_POS, 10 if state == B_POS: return B_PRIME_POS, 5 state = np.array(state) next_state = (state + action).tolist() x, y = next_state if x &lt; 0 or x &gt;= WORLD_SIZE or y &lt; 0 or y &gt;= WORLD_SIZE: reward = -1.0 next_state = state else: reward = 0 return next_state, reward 根据给定的np.array数据类型绘制方格图1234567891011121314151617181920212223242526272829303132def draw_image(image): fig, ax = plt.subplots() ax.set_axis_off() tb = Table(ax, bbox=[0, 0, 1, 1]) nrows, ncols = image.shape width, height = 1.0 / ncols, 1.0 / nrows # Add cells # np.ndenumerate() return an iterator yielding pairs of array coordinates and values. for (i,j), val in np.ndenumerate(image): # Index either the first or second item of bkg_colors based on # a checker board pattern idx = [j % 2, (j + 1) % 2][i % 2] color = 'white' tb.add_cell(i, j, width, height, text=val, loc='center', facecolor=color) # Row Labels... for i, label in enumerate(range(len(image))): tb.add_cell(i, -1, width, height, text=label+1, loc='right', edgecolor='none', facecolor='none') # Column Labels... for j, label in enumerate(range(len(image))): tb.add_cell(-1, j, width, height/2, text=label+1, loc='center', edgecolor='none', facecolor='none') ax.add_table(tb) # test draw_imagetest_image = np.array([[1.2,2.1],[3.5,4.7]])draw_image(test_image) Policies and Value Functions123456789101112131415161718192021def figure_3_2(): value = np.zeros((WORLD_SIZE, WORLD_SIZE)) while True: # keep iteration until convergence new_value = np.zeros(value.shape) for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) # bellman equation # each action is random chosen new_value[i, j] += ACTION_PROB * (reward + DISCOUNT * value[next_i, next_j]) if np.sum(np.abs(value - new_value)) &lt; 1e-4: draw_image(np.round(new_value, decimals=2)) plt.savefig('./figure_3_2.png') plt.show() plt.close() break value = new_valuefigure_3_2() Optimal Policies and Value Functions(Greedy choose action)123456789101112131415161718192021def figure_3_5(): value = np.zeros((WORLD_SIZE, WORLD_SIZE)) while True: # keep iteration until convergence new_value = np.zeros(value.shape) for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): values = [] for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) # value iteration values.append(reward + DISCOUNT * value[next_i, next_j]) new_value[i, j] = np.max(values) if np.sum(np.abs(new_value - value)) &lt; 1e-4: draw_image(np.round(new_value, decimals=2)) plt.savefig('./figure_3_5.png') plt.show() plt.close() break value = new_valuefigure_3_5() 后记这是关于马尔科夫决策过程一个相对简单的过程，可以看到这里一旦确定了previous state和action，current state和对应的reward就确定了。即Pr{s’,r|s,a}=1，事实上MDP的应用包括优化过程都是有相当的局限性的： （1）首先我们需要准确的知道环境的动态变化； （2）我们要有足够的算力来推出所有需要的state和value； （3）我们需要保证问题的马尔科夫性。 然而这里的（2）则很大程度上局限了MDP的推广，因为事实上如果需要在所有的state上做出action并update value，对于state数量庞大的任务几乎不可能完成的。]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter03 Finite Markov Decision Processes]]></title>
    <url>%2F2018%2F11%2F22%2FChapter03-Finite-Markov-Decision-Processes%2F</url>
    <content type="text"><![CDATA[马尔可夫决策过程(Markov Decision Process, MDP)，是指具有马尔可夫性的一类强化学习问题，即系统下个状态和当前的状态有关，以及当前采取的动作有关。 some notation in MDP1.state : S_t 2.action: A_t 3.reward: R_t 4.a probability of those values occurring at time t, given particular values of the preceding state and action: 上述概率对s’和r分别累加=1 5.state-transition probability 6.expect reward 7.expected reward for state-action-next-state return of episode tasks and continuing taskswhat is return of task?if we define each step reward as R_i(i=0,1,2,3…), then expect return is defined as some specific function of the reward sequence. what is episode task?Such as plays of a game, trips through a maze, or any sort of repeated interaction. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. And the next episode begins independently of how the previous one ended. what is continuing taks?agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. the return of above two taskepisode task continuing task Unified notationthrough add an absorbing state in episode task(solid square): unified notation: update value function in mdp methodstate-value function for policy π: state-action-value function for policy π: how to update the state in policy π?Bellman equation for v_π: Instead of getting average return in some random samples(Monte Carlo method), use some parameters to describle. π(a|s) is the policy π: decide action due to state s p(s’,r|s,a) is the transition probability of previous state s and action a, which follows the Markov property get the optimal policy π and optimal policy value-statechoose the max value in the actions of state s:]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter02 ten-armed-Testbed]]></title>
    <url>%2F2018%2F11%2F16%2FChapter02-ten-armed-Testbed%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter02/ten_armed_testbed.py 通过建立10-armed-Testbed来仿真第二章讲的几种Bandit算法 1、引入模块123456import matplotlibimport matplotlib.pyplot as plt%matplotlib inlineimport numpy as np# use for showing process barfrom tqdm import tqdm 2、创建Testbed类，实现基本的action和update value方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class Bandit: # @k_arm: # of arms # @epsilon: probability for exploration in epsilon-greedy algorithm # @initial: initial estimation for each action # @step_size: constant step size for updating estimations # @sample_averages: if True, use sample averages to update estimations instead of constant step size # @UCB_param: if not None, use UCB algorithm to select action # @gradient: if True, use gradient based bandit algorithm # @gradient_baseline: if True, use average reward as baseline for gradient based bandit algorithm def __init__(self, k_arm=10, epsilon=0., initial=0., step_size=0.1, sample_averages=False, UCB_param=None, gradient=False, gradient_baseline=False, true_reward=0.): self.k = k_arm self.step_size = step_size self.sample_averages = sample_averages self.indices = np.arange(self.k) self.time = 0 self.UCB_param = UCB_param self.gradient = gradient self.gradient_baseline = gradient_baseline self.average_reward = 0 self.true_reward = true_reward self.epsilon = epsilon self.initial = initial def reset(self): # real reward for each action # true_reward is baseline can be configed # np.random.randn(_size()) return standard normal array in the size of _size() self.q_true = np.random.randn(self.k) + self.true_reward # estimation for each action # here we can modify the serf.initial to export optimistic initial value self.q_estimation = np.zeros(self.k) + self.initial # of chosen times for each action self.action_count = np.zeros(self.k) # return the index of max-real-reward action self.best_action = np.argmax(self.q_true) # get an action for this bandit def act(self): # np.random.rand() return uniform distribution over [0,1) # explore if np.random.rand() &lt; self.epsilon: # return np.random.choice(self.indices) # I think UCB may choose actions in non-greedy actions if self.UCB_param is not None: # here is a little different from book, Why? # I think he may want to be avoid of zero problem in log and denominator position UCB_estimation = self.q_estimation + \ self.UCB_param * np.sqrt(np.log(self.time + 1) / (self.action_count + 1e-5)) q_best = np.max(UCB_estimation) return np.random.choice([action for action, q in enumerate(UCB_estimation) if q == q_best]) else: return np.random.choice(self.indices) if self.gradient: exp_est = np.exp(self.q_estimation) self.action_prob = exp_est / np.sum(exp_est) # According to the probability to choose action # here use the same method with wiki return np.random.choice(self.indices, p=self.action_prob) # greey-action return np.argmax(self.q_estimation) # take an action, update estimation for this action def step(self, action): # generate the reward under N(real reward, 1) reward = np.random.randn() + self.q_true[action] self.time += 1 # average_reward update uses self.time self.average_reward = (self.time - 1.0) / self.time * self.average_reward + reward / self.time self.action_count[action] += 1 if self.sample_averages: # update estimation using sample averages # q_estimation update uses action_count self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action]) # is different with book elif self.gradient: one_hot = np.zeros(self.k) one_hot[action] = 1 if self.gradient_baseline: baseline = self.average_reward else: baseline = 0 # update method is the same as book, here use q_estimation(a) replaces H_t(a) self.q_estimation = self.q_estimation + self.step_size * (reward - baseline) * (one_hot - self.action_prob) else: # update estimation with constant step size self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action]) return reward 3、训练Bandit123456789101112131415def simulate(runs, time, bandits): best_action_counts = np.zeros((len(bandits), runs, time)) rewards = np.zeros(best_action_counts.shape) for i, bandit in enumerate(bandits): for r in tqdm(range(runs)): bandit.reset() for t in range(time): action = bandit.act() reward = bandit.step(action) rewards[i, r, t] = reward if action == bandit.best_action: best_action_counts[i, r, t] = 1 best_action_counts = best_action_counts.mean(axis=1) rewards = rewards.mean(axis=1) return best_action_counts, rewards 4、结果显示（折线图格式）1、10-Bandit-Testbed value 和reward分布123456def figure_2_1(): plt.violinplot(dataset=np.random.randn(200,10) + np.random.randn(10)) plt.xlabel("Action") plt.ylabel("Reward distribution") plt.show()figure_2_1() 2、different epsilons12345678910111213141516171819202122def figure_2_2(runs=2000, time=1000): epsilons = [0, 0.1, 0.01] bandits = [Bandit(epsilon=eps, sample_averages=True) for eps in epsilons] best_action_counts, rewards = simulate(runs, time, bandits) plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) for eps, rewards in zip(epsilons, rewards): plt.plot(rewards, label='epsilon = %.02f' % (eps)) plt.xlabel('steps') plt.ylabel('average reward') plt.legend() plt.subplot(2, 1, 2) for eps, counts in zip(epsilons, best_action_counts): plt.plot(counts, label='epsilon = %.02f' % (eps)) plt.xlabel('steps') plt.ylabel('% optimal action') plt.legend() plt.show()# figure_2_2() 3、Initial value = 5 VS Initial value = 01234567891011121314def figure_2_3(runs=2000, time=1000): bandits = [] bandits.append(Bandit(epsilon=0, initial=5, step_size=0.1)) bandits.append(Bandit(epsilon=0.1, initial=0, step_size=0.1)) best_action_counts, _ = simulate(runs, time, bandits) plt.plot(best_action_counts[0], label='epsilon = 0, q = 5') plt.plot(best_action_counts[1], label='epsilon = 0.1, q = 0') plt.xlabel('Steps') plt.ylabel('% optimal action') plt.legend() plt.show()#figure_2_3() 4、UCB VS epsilon-greey12345678910111213def figure_2_4(runs=2000, time=1000): bandits = [] bandits.append(Bandit(epsilon=0, UCB_param=2, sample_averages=True)) bandits.append(Bandit(epsilon=0.1, sample_averages=True)) _, average_rewards = simulate(runs, time, bandits) plt.plot(average_rewards[0], label='UCB c = 2') plt.plot(average_rewards[1], label='epsilon greedy epsilon = 0.1') plt.xlabel('Steps') plt.ylabel('Average reward') plt.legend() plt.show()#figure_2_4() 5、softmax baseline VS non-baseline12345678910111213141516171819def figure_2_5(runs=2000, time=1000): bandits = [] bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=True, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=False, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=True, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=False, true_reward=4)) best_action_counts, _ = simulate(runs, time, bandits) labels = ['alpha = 0.1, with baseline', 'alpha = 0.1, without baseline', 'alpha = 0.4, with baseline', 'alpha = 0.4, without baseline'] for i in range(0, len(bandits)): plt.plot(best_action_counts[i], label=labels[i]) plt.xlabel('Steps') plt.ylabel('% Optimal action') plt.legend() plt.show()#figure_2_5() 6、epsilon-greey vs softmax vs UCB vs opt-initial123456789101112131415161718192021222324252627282930def figure_2_6(runs=2000, time=1000): labels = ['epsilon-greedy', 'gradient bandit', 'UCB', 'optimistic initialization'] generators = [lambda epsilon: Bandit(epsilon=epsilon, sample_averages=True), lambda alpha: Bandit(gradient=True, step_size=alpha, gradient_baseline=True), lambda coef: Bandit(epsilon=0, UCB_param=coef, sample_averages=True), lambda initial: Bandit(epsilon=0, initial=initial, step_size=0.1)] parameters = [np.arange(-7, -1, dtype=np.float), np.arange(-5, 2, dtype=np.float), np.arange(-4, 3, dtype=np.float), np.arange(-2, 3, dtype=np.float)] bandits = [] for generator, parameter in zip(generators, parameters): for param in parameter: bandits.append(generator(pow(2, param))) _, average_rewards = simulate(runs, time, bandits) rewards = np.mean(average_rewards, axis=1) i = 0 for label, parameter in zip(labels, parameters): l = len(parameter) plt.plot(parameter, rewards[i:i+l], label=label) i += l plt.xlabel('Parameter(2^x)') plt.ylabel('Average reward') plt.legend() plt.show()figure_2_6()]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter02 softmax-theory]]></title>
    <url>%2F2018%2F11%2F15%2FChapter02-softmax-theory%2F</url>
    <content type="text"><![CDATA[This post is my understanding about P29 deeper insight part:《The Bandit Gradient Algorithm as Stochastic Gradient Ascent》 1. Understand Why it comes from Stochastic Gradient Ascent在我们讨论这个问题之前先看看wiki上关于softmax在reinforcementLearning上的应用吧。 相信你和我一样惊奇，这里本来的H_t(a)变成了q_t(a)/τ。其中τ为温度系数(temperature parameter)，τ-&gt;无穷，所以action具有相同的选择概率，或者说是explore的；如果τ-&gt;0,则具有最大q_t(a)的行为会被选择，且τ越小，选择q_t(a)最大的概率越大并趋于1。 这太有意思了，wiki上直接解释了softmax function选择action的合理性，但是这就是这篇post想要讨论的问题啊@_@。 好了回到标题，书上这里这样写的： In exact gradient ascent, each preference H_t(a) would be incremented proportional to the increment’s effect on performance: 并给出了公式： E[R_t]的定义为： 从梯度上升的角度来解释：如果平均的value:E[R_t]对H_t(a)导数为正，则提高H_t(a)可以提高E[R_t]；如果相反，则减小H_t(a)可以提高E[R_t]。总结起来就是H_t(a)的增量和E[R_t]对H_t(a)的偏导成正比。 计算偏导计算公式如下： 公式的难点主要在第三步B_t的引入。B_t is called the baseline, can be any scalar that does not depend on x. We can include a baseline here without changing the equality because the gradient sums to zero over all the actions: change the equation to the form of expectation 这里的难点是使用R_t替换q_(A_t)，因为E[R_t|A_t] = q_(A_t)，所以可以替换。接着我们处理一下后半部分： 其中1_(a=x) means that if a=x, output 1; else output 0 因为我们通过无数次训练来更新H_t(a)，所以可以去掉期望符号，并在前面加上step-size α进行近似。如果α=1/n，那么就符合大数定律近似了，如果采用常数可能对解决非平稳问题更好一些。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter02 Multi-armed Bandits]]></title>
    <url>%2F2018%2F11%2F14%2FChapter02-Multi-armed-Bandits%2F</url>
    <content type="text"><![CDATA[k-arm-Bandits问题可说是强化学习最简单的任务了，因为他只涉及了1个state下的action选取。通过本章可以对强化学习的目标，评估方法和训练方法有一个初步的认识。 2.1.1 什么是k-armed Bandits问题You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps. 大致意思就是，每步有k种action选择，每种选择对应不同的reward，完成的目标就是需要在n次执行后，最大化总共的reward。 In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action: 2.1.2 k-armed Bandits问题的难点这里主要是explore和exploit的权衡。下面的解释摘自周老师的西瓜书强化学习部分： 若仅为知道每个摇臂的期望奖赏，则可采用“仅探索”(exploration-only)法：将所有的尝试机会平均分配给每个摇臂，最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。若仅为执行奖赏最大的动作，则可以用“仅利用”(exploitation-only)法，按下目前最优的摇臂。前者可以很好的估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会，没办法得到最好的收益；后者刚好相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优的摇臂，因此这两种方法都难以使最终的积累奖励最大化。 根据实验情况，exploring-only收敛结果明显劣于exploiting-only，而exploiting-only效果同样很差。 2.2 Action-value Method这是原文标题我直接抄过来了。因为上个标题提到了行为的价值(value)，所以需要给一种量化这种value的算法。 We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: 然后如何选择action？greedy action：选择Qt最大的a作为t时刻的action ε-greedy action: behave greedily most of the time, but every once in a while, say with small probability ε, instead select randomly from among all the actions with equal probability,independently of the action-value estimates. 2.3 The 10-armed Testbed10-armed Testbed是测试k-armed Bandits算法的一个基础模型，后面的模型评估都是基于此的。 What is it?简而言之，Testbed中包含了2000个10-armed Bandits问题，每个问题的10个行为action对应的value q(a)是通过正态分布得到的(mean=0,variance=1)，而实际对每个问题进行训练的时候，得到的实际回报(actual reward)是在action value上附加一个正态分布(mean=q(a),variance=1)。然后用greedy算法和ε-greedy算法对每个问题进行1000step的训练，对2000个问题求平均数得到 target(Average reward or Optimal action) - Steps的折线图。并以此来评价算法性能。 算法对比参照Python代码的博客 2.4 Incremental Implementation这部分讲了一个增量优化算法，这让我联想到了增量式PID和位置式PID，QAQ。其实这个思想和那个也挺相似的。 previous edition: PS:这个公式和第一章的tic-tac-toe的V(s)更新公式太像了。 but，还是有一些区别。第一章的V(s)更新的时候是仅针对greedy-action的，但是这里因为各个action是独立的，如果不对exploring更新的话，就没办法有效的找到最优的action了，即这次action是无意义的。 incremental edition: incremental bandit algorithm: an intresting update ruleThe update rule below is of a form that occurs frequently throughout this book. The general form is: about this rule:1、The expression [Target−OldEstimate] is an error in the estimate. It is reduced by taking a step toward the “Target.” The target is presumed to indicate a desirable direction in which to move, though it may be noisy. In the case above, for example, the target is the nth reward. 2、Note that the step-size parameter (StepSize) used in the incremental method described above changes from time step to time step. In processing the nth reward for action a, the method uses the step-size parameter 1/n . In this book we denote the step-size parameter by α or, more generally, by α_t(a). 2.5 Tracking a Nonstationary(非平稳) Problem上面讨论的Bandits-methods，都是建立在rewards的概率不变的前提下的，如Testbed中规定，reward(a)遵循(mean = q*(a),variance = 1)的概率分布。而在实际的强化学习任务中，经常是与之相对的概率不平稳的(nonstationary)。 考虑上一个标题讨论的update rule的一般形式，处理非平稳问题一个有效方法就是令α为定值(use a constant step-size parameter)，因为这样可以提高最近的reward的比重，降低过去久远的reward的比重，可以参考下面的公式： 因为α&lt;1，所以很明显如果i越小，R_i的weight就会越小。 但是有一个问题，就是如果希望最终的value是收敛的，则Stepsize需要满足如下条件： 第一条保证weight足够大，可以覆盖掉初值的误差以及一些波动，第二条保证最后能够收敛。 当α=1/n条件很明显是满足的，但是如果α=constant第二条就不满足了。但是对于非稳定的情况，这正是我们希望看到的，具体证明也没给出，下次看到补上？ 2.6 Optimistic Initial Values(encouraging exploration)我们在上面看到α=1/n时，Q1被消去了，但是事实上大部分情况下Q1会对学习情况产生影响，比如当α=constant时，Q1就被保留了，不过*了一个相当小的系数。。。 事实上，this kind of bias is usually not a problem and can sometimes be very helpful.举个栗子，在Testbed里，我把训练初值设为5，对于在1附近正态分布的reward，不管选择哪个，在初始阶段都会&lt;5，学习器对他们都很失望：Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being “disappointed” with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time. 但是对于非平稳问题(nonstationary)，因为分布会随时间改变，而这种只在训练开始引入exploring的方法相当于只是encourage exploration for once，故效果不会很好，但是对应sample-average(平稳)的问题就会有一定的效果。 当然，我们不会只采用一种方法，将不同的方法进行组合利用，将会是贯穿全书的思想。 2.7 Upper-Confidence-Bound Action Selection在ε-greedy方法中，通过一个小概率ε来选定non-greedy action进行exploring，但是这种方法是无差别的，如果对那些non-greedy action进行一个事前的评估，根据它们潜能(不确定度)来进行explore的话，那么可以提高explore的广度和效率，效果可能会更好。 One effective way of doing this is to select actions according to(UCB): N_t(a) denotes the number of times that action a has been selected prior to time t.number c &gt; 0 controls the degree of exploration.If N_t(a) = 0, then a is considered to be a maximizing action.公式里的平方根部分是对action a的不确定度的估计，可以从两方面来解释：如果action a经常被选中，则分母N_t就会变大，不确定度就会减小；如果action a不经常被选中，那么随着试验次数t增大，而N_t不变，则不确定度上升，我们应该多考虑一下a啦。 但是也存在一些问题UCB比起ε-greedy方法，相对来说更复杂，而且它在其他强化学习问题中的可扩展性(extend)远不如后者，而且这种方法在非平稳问题中表现的并不好，Another difficulty is dealing with large state spaces, particularly when using function approximation as developed in Part II of this book.In these more advanced settings the idea of UCB action selection is usually not practical. 2.8 Gradient Bandit Algorithms看了这么久，休息一下如何^_^ A new method to choose actionconsider learning a numerical preference for each action a, which we denote H_t(a). The larger the preference, the more often that action is taken: π_t(a) is the probability of taking action a at time t. Initially all preferences are the same(zero)so that all actions have an equal probability of being selected. How to update the H_t(a) in the rule: α&gt;0 is a step-size paramter; (R_t)_average is the baseline with which the reward is compared.It which can be computed incrementally as described in Incremental Implementation about the baselineIf the reward is higher than the baseline, then the probability of taking A_t in the future is increased, and if the reward is below baseline, then probability is decreased. The non-selected actions move in the opposite direction. the theory of softmax methodhere is a seperate blog to explain it. 2.9 Associative Search (Contextual Bandits)上面的算法只处理了一台k-arms-Bandit，接下来考虑这个问题：如果你面对的是10台k-arms-Bandit，每次你随机从这几台机器(k-arms-Bandit comes from slot machine)里抽一个来操作，那么可以认为这仍是在处理同一台slot machine，但是true value是随着一步一步操作剧烈变化的。 说到这里应该就明白了，我的每一步action都是会对接下来的situation和reward引起影响的，而普通的k-arms-Bandit每步之间是独立的。 associative search可以认为是k-arms-Bandit和full reinforcement learning之间的桥梁，现在它一般被称为contextual bandits问题。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter01 Tic-Tac-Toe]]></title>
    <url>%2F2018%2F11%2F14%2FChapter01-Tic-Tac-Toe%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter01/tic_tac_toe.py Tic-Tac-Toe的python代码实现 1、引入模块并定义井字棋的常量 12345678import numpy as npimport pickleimport matplotlib.pyplot as plt%matplotlib inline# 将rows和cols调至4，这种算法计算量就会大很多BOARD_ROWS = 3BOARD_COLS = 3BOARD_SIZE = BOARD_ROWS * BOARD_COLS 2、创建环境，State类，表征棋盘上的X和O子的情况。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class State: def __init__(self): # the board is represented by an n * n array, # 1 represents a chessman of the player who moves first, # -1 represents a chessman of another player # 0 represents an empty position self.data = np.zeros((BOARD_ROWS, BOARD_COLS)) self.winner = None self.hash_val = None self.end = None # compute the hash value for one state, it only work at first time def hash(self): if self.hash_val is None: self.hash_val = 0 for i in self.data.reshape(BOARD_ROWS * BOARD_COLS): if i == -1: # shallow copy i = 2 self.hash_val = self.hash_val * 3 + i return int(self.hash_val) # check whether a player has won the game, or it's a tie def is_end(self): if self.end is not None: return self.end results = [] # check row for i in range(0, BOARD_ROWS): results.append(np.sum(self.data[i, :])) # check columns for i in range(0, BOARD_COLS): results.append(np.sum(self.data[:, i])) # check diagonals results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, i] results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, BOARD_ROWS - 1 - i] for result in results: if result == 3: self.winner = 1 self.end = True return self.end if result == -3: self.winner = -1 self.end = True return self.end # whether it's a tie: no one wins, all places are filled. sum = np.sum(np.abs(self.data)) if sum == BOARD_ROWS * BOARD_COLS: self.winner = 0 self.end = True return self.end # game is still going on self.end = False return self.end # @symbol: 1 or -1 # put chessman symbol in position (i, j) def next_state(self, i, j, symbol): new_state = State() # deep copy new_state.data new_state.data = np.copy(self.data) new_state.data[i, j] = symbol return new_state # print the board def print(self): for i in range(0, BOARD_ROWS): print('-------------') out = '| ' for j in range(0, BOARD_COLS): if self.data[i, j] == 1: token = '*' if self.data[i, j] == 0: token = '0' if self.data[i, j] == -1: token = 'x' out += token + ' | ' print(out) print('-------------') 关于这里的hash函数，是为了得到不重复的所有棋盘情况。但是为什么可以通过这种算法那？假设两个不同的棋盘情况A和B，他们的hash值是一致的，我们不妨设他们的最高位不相同，那么我们看到，该位最少相差1，通过等比数列求和可以得到，3^(n+1)&gt;2*(1+3^1+3^2+…+3^n)，即最高位不同是无法通过其余位来弥补的，那么可见他们的最高位不同是不可能hash值相同的，以此递推，则可得A和B值是一样，即一样的棋盘，假设不成立。3、公共函数，用来获取当前state。12345678910111213141516171819202122232425def get_all_states_impl(current_state, current_symbol, all_states): for i in range(0, BOARD_ROWS): for j in range(0, BOARD_COLS): if current_state.data[i][j] == 0: newState = current_state.next_state(i, j, current_symbol) newHash = newState.hash() if newHash not in all_states.keys(): isEnd = newState.is_end() all_states[newHash] = (newState, isEnd) if not isEnd: get_all_states_impl(newState, -current_symbol, all_states)def get_all_states(): current_symbol = 1 current_state = State() all_states = dict() all_states[current_state.hash()] = (current_state, current_state.is_end()) get_all_states_impl(current_state, current_symbol, all_states) return all_states# all possible board configurationsall_states = get_all_states()# for i in all_states.values():# i[0].print()# print(i[1]) 4、Player类，用来模拟AI选手下棋的行为，得到value function1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# AI playerclass Player: # @step_size: the step size to update estimations # @epsilon: the probability to explore def __init__(self, step_size=0.1, epsilon=0.1): self.estimations = dict() self.step_size = step_size self.epsilon = epsilon self.states = [] # greedy means that state is caused by exploiting, otherwise is caused by exploring self.greedy = [] def reset(self): self.states = [] self.greedy = [] def set_state(self, state): self.states.append(state) self.greedy.append(True) # give the player symbol,and update the value table def set_symbol(self, symbol): self.symbol = symbol for hash_val in all_states.keys(): (state, is_end) = all_states[hash_val] if is_end: if state.winner == self.symbol: self.estimations[hash_val] = 1.0 elif state.winner == 0: # we need to distinguish between a tie and a lose self.estimations[hash_val] = 0.5 else: self.estimations[hash_val] = 0 else: self.estimations[hash_val] = 0.5 # update value estimation def backup(self): # for debug # print('player trajectory') # for state in self.states: # state.print() self.states = [state.hash() for state in self.states] for i in reversed(range(len(self.states) - 1)): state = self.states[i] # only udpate the state caused by exploiting td_error = self.greedy[i] * (self.estimations[self.states[i + 1]] - self.estimations[state]) self.estimations[state] += self.step_size * td_error # choose an action based on the state def act(self): state = self.states[-1] next_states = [] next_positions = [] for i in range(BOARD_ROWS): for j in range(BOARD_COLS): if state.data[i, j] == 0: next_positions.append([i, j]) next_states.append(state.next_state(i, j, self.symbol).hash()) # explore if np.random.rand() &lt; self.epsilon: action = next_positions[np.random.randint(len(next_positions))] action.append(self.symbol) self.greedy[-1] = False return action # exploit values = [] for hash, pos in zip(next_states, next_positions): values.append((self.estimations[hash], pos)) np.random.shuffle(values) values.sort(key=lambda x: x[0], reverse=True) # choose the biggest value action action = values[0][1] action.append(self.symbol) return action # policy -&gt; self.estimations def save_policy(self): with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f: pickle.dump(self.estimations, f) def load_policy(self): with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f: self.estimations = pickle.load(f) 这里需要注意一点就是，在backup方法里，只有greedy-action才会更新value，exploring虽然不会更新value，但是其随后的greedy-action会进行更新，个人的理解是：exploring并不能保证最优，它只是提供一种“探索”行为，帮助Learning method更好的学习value，所以只对greedy-action进行update。这地方原文有解释：Exploratory moves do not result in any learning, but each of our other moves does, causing updates as suggested by the curved arrow in which estimated values are moved up the tree from later nodes to earlier as detailed in the text. 5、创建对决类Judger，通过一次对局完成一次value table的update1234567891011121314151617181920212223242526272829303132333435363738394041424344class Judger: # @player1: the player who will move first, its chessman will be 1 # @player2: another player with a chessman -1 # @feedback: if True, both players will receive rewards when game is end def __init__(self, player1, player2): self.p1 = player1 self.p2 = player2 self.current_player = None self.p1_symbol = 1 self.p2_symbol = -1 self.p1.set_symbol(self.p1_symbol) self.p2.set_symbol(self.p2_symbol) self.current_state = State() def reset(self): self.p1.reset() self.p2.reset() # return p1 and p2 in turn(alternately) def alternate(self): while True: yield self.p1 yield self.p2 # @print: if True, print each board during the game def play(self, print=False): alternator = self.alternate() self.reset() current_state = State() self.p1.set_state(current_state) self.p2.set_state(current_state) while True: player = next(alternator) if print: current_state.print() [i, j, symbol] = player.act() next_state_hash = current_state.next_state(i, j, symbol).hash() current_state, is_end = all_states[next_state_hash] self.p1.set_state(current_state) self.p2.set_state(current_state) if is_end: if print: current_state.print() return current_state.winner 6、训练并让两名AI Player进行对局12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def train(epochs): player1 = Player(epsilon=0.01) player2 = Player(epsilon=0.01) judger = Judger(player1, player2) player1_win = 0.0 player2_win = 0.0 player1_a = [] player2_a = [] for i in range(1, epochs + 1): winner = judger.play(print=False) if winner == 1: player1_win += 1 if winner == -1: player2_win += 1 player1_a.append(player1_win/i) player2_a.append(player2_win/i) #print('Epoch %d, player 1 win %.02f, player 2 win %.02f' % (i, player1_win / i, player2_win / i)) player1.backup() player2.backup() judger.reset() player1.save_policy() player2.save_policy() x = range(1,1001,1) x = list(x) plt.plot(x,player1_a) plt.plot(x,player2_a) plt.axis([0,200,0,1]) plt.title('training chart') plt.show() def compete(turns): player1 = Player(epsilon=0) player2 = Player(epsilon=0) judger = Judger(player1, player2) player1.load_policy() player2.load_policy() player1_win = 0.0 player2_win = 0.0 for i in range(0, turns): winner = judger.play() if winner == 1: player1_win += 1 if winner == -1: player2_win += 1 judger.reset() print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))train(int(1e3))]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter01 Introduction]]></title>
    <url>%2F2018%2F11%2F12%2FChapter01-Introduction%2F</url>
    <content type="text"><![CDATA[希望能以更新博客的方式激励一下自己，目前是准备读一下强化学习的入门书《Reinforcement Learning Introduction》，然后做一下读书笔记。下面是绪论(Introduction)的内容。 What is ReinforcementLearningFirst Impression of RL如果说之前我接触到的机器学习方法，如逻辑回归、svm、决策树、人工神经网络等是数据驱动的，那么强化学习就是“情境”驱动的：行为由当前环境决定，行为导致数值化的reward更新，而reward又影响了环境。这是我对强化学习的初印象。 强化学习有两个特点：1、trial-and-error search：学习算法并未指导行为，但是却必须得出当行为做出相应的reward(近期) 2、delayed reward：每个行为之间不是独立的，当前行为可能会有比当前收益更丰盛的远期收益(value)。 强化学习有三个重要要素，就是学习器(agent，我对学习器的理解就是训练出模型时执行的程序）必须能够感知环境(state)并做出影响它的行为(action)，同时agent must have a goal or goals relating to the state of the environment。 RL vs Supervised learning监督学习就不赘述了，大致来说，监督学习算法通过带标签的训练集学习得到可以对未知样例的判断“能力”。但是如果让监督学习从交互中学习，按照监督学习的思路，获取正确的而且有意义的行为例子是不可取的，所以它没办法从经验中成长，不适合需要和环境交互的算法。 举一个不太恰当的例子，监督学习就像考前刷题的你，而强化学习就像每天认真学习的学霸，如果考题和你刷的吻合度比较高，那你就稳了，但是学霸每天学习，数据已经内化为他的能力了，所以他一点都不慌。 RL vs Unsupervised learning有不少人认为监督学习和非监督学习已经把机器学习进行详尽的划分了，他们将强化学习归入后者，因为它们都没有带标签的数据，但是从目的上来看，强化学习是为了最大化reward的，而非监督学习是为了学习到数据之中的隐藏结构，非监督学习的方法并没有从实质解决强化学习的问题。 exploration and exploitationThe agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future. 这个问题有点像梯度下降中使用的随机梯度法(SGD)或者模拟退火法，但是SGD是一种加速算法，或者说如果我头铁不用，我也是有可能得到还可以的结果，但是强化学习如果不学习未知的部分，结果肯定是有问题的，但是如果一直在试错 explore，不对“经验”进行总结 exploit，也是没办法使算法收敛的。 Tic-Tac-Toe我觉得先分析一个例子可以更好的帮助理解RL的各个属性之间的关系。Tic-Tac-Toe比较像我们小时候玩的井字棋，图我就不画了。 强化学习和传统的classical minimax solution from game theory以及Classical optimization methods for sequential decision problems, such as dynamic programming有区别，前者是按照有强约束的方式运动的（这里是不会走使自己失败的位置），而后者则是需要有每一步的先验概率才能进行优化。 强化学习是基于value functions的学习方法，而前面提到的minimax可以认为是基于evolutionary的学习方法，前者是基于value of state来学习的，后者则是通过结果来更新policy，前者对状态的利用优于后者，而后者则是仅利用了结果来修改policy。 tic-tac-toe的ython代码实现戳这里 Elements of RLBeyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal , a value function, and, optionally, a model of the environment. policy感觉是从state到action的一个映射关系吧，这里就对应了上面提到的explore和exploit的关系。In general, policies may be stochastic. reward每个action导致的短期收益。reward会决定policy，使得行为向高reward的方向发展，所以reward一般是state和action的随机函数：In general, reward signals may be stochastic functions of the state of the environment and the actions taken. value function由reward引起的长期收益。value由reward综合而来，而value同时会使reward朝着高的方向发展。value控制action的发展：Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run. model of the environmentThis is something that mimics(模拟) the behavior of the environment, or more generally, that allows inferences(推断) to be made about how the environment will behave. Summary总体来说强化学习和之前学过的其他机器学习方法还是区别挺大的，但是绪论有几点疑问： 1、action、state、reward、value之间的更新方式还是没搞懂？ 2、如何建立value table？ —————————-(answers in 11.14)————————————- 1、value建立在state之上，每次state更新伴随着value的更新；action分为两种，explore和exploit，前者随机，后者基于reward最大；reward认为是从current state -&gt; next state update时，对应的next value or (next value - current value) 2、value table建立基于state，原则视问题而定。比如tic-tac-toe问题，X赢对应的value = 1，O赢对应的value = 0，平局对应value = 0.5，当然我是站在X的角度。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[some config on next theme]]></title>
    <url>%2F2018%2F11%2F10%2Fsome-config-on-next-theme%2F</url>
    <content type="text"><![CDATA[上次配置了hexo+github的个人博客，这次我做了一些偏好的配置。 更换主题为next因为网上很多关于主题配置的博客都是基于next主题的，所以我先将主题换为了next主题： 1、先下载主题到本地 123$ cd ~$ cd GitBlog$ git clone https://github.com/iissnan/hexo-theme-next themes/next 2、更改站点的config文件来使用主题 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 3、更改theme/next的config文件(/theme/next/_config.yml): 123456789101112# 修改主题模式为Gemini# Schemes#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini#修改侧边栏头像# Sidebar Avatar# in theme directory(source/images): /images/avatar.gif# in site directory(source/uploads): /uploads/avatar.gifavatar: https://wx2.sinaimg.cn/large/0070VybLly1fx5b09mly4j30kx0i14kq.jpg 添加搜索功能下载hexo搜索插件hexo-generator-search然后修改站点的config文件启动搜索功能： 12345678910# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next#在插件位置插入搜索模块使能插件search: path: search.xml field: post format: html limit: 10000 添加渐变更换的壁纸效果比较像忧郁的弟弟(额马上就是loli了)背景效果。 1、在/theme/next/source/css/_custom/中修改_custom.styl，添加css3动画特效： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137body &#123;background: #000;background-attachment: fixed;word-wrap: break-word;-webkit-background-size: cover;-moz-background-size: cover;background-size: cover;background-repeat: no-repeat&#125; ul &#123;list-style: none&#125; .cb-slideshow li:nth-child(1) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b098is8j31hc0u0jzz.jpg)&#125; .cb-slideshow li:nth-child(2) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b095t28j31hc11hdph.jpg)&#125; .cb-slideshow li:nth-child(3) span &#123;background-image: url(https://wx2.sinaimg.cn/large/0070VybLly1fx5b09b17xj31hc0z2tim.jpg)&#125; .cb-slideshow li:nth-child(4) span &#123;background-image: url(https://wx4.sinaimg.cn/large/0070VybLly1fx5b09474xj31hc0x6jy5.jpg)&#125; .cb-slideshow li:nth-child(5) span &#123;background-image: url(https://wx2.sinaimg.cn/large/0070VybLly1fx5b08weitj31hc11otga.jpg)&#125; .cb-slideshow li:nth-child(6) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b1eraokj31hc0u0gp9.jpg)&#125; .cb-slideshow,.cb-slideshow:after &#123;position: fixed;width: 100%;height: 100%;top: 0;left: 0;z-index: -2&#125; .cb-slideshow:after &#123;content: ''&#125; .cb-slideshow li span &#123;width: 100%;height: 100%;position: absolute;top: 0;left: 0;color: transparent;background-size: cover;background-position: 50% 50%;background-repeat: none;opacity: 0;z-index: -2;-webkit-backface-visibility: hidden;-webkit-animation: imageAnimation 36s linear infinite 0s;-moz-animation: imageAnimation 36s linear infinite 0s;-o-animation: imageAnimation 36s linear infinite 0s;-ms-animation: imageAnimation 36s linear infinite 0s;animation: imageAnimation 36s linear infinite 0s&#125; .cb-slideshow li:nth-child(2) span &#123;-webkit-animation-delay: 6s;-moz-animation-delay: 6s;-o-animation-delay: 6s;-ms-animation-delay: 6s;animation-delay: 6s&#125; .cb-slideshow li:nth-child(3) span &#123;-webkit-animation-delay: 12s;-moz-animation-delay: 12s;-o-animation-delay: 12s;-ms-animation-delay: 12s;animation-delay: 12s&#125; .cb-slideshow li:nth-child(4) span &#123;-webkit-animation-delay: 18s;-moz-animation-delay: 18s;-o-animation-delay: 18s;-ms-animation-delay: 18s;animation-delay: 18s&#125; .cb-slideshow li:nth-child(5) span &#123;-webkit-animation-delay: 24s;-moz-animation-delay: 24s;-o-animation-delay: 24s;-ms-animation-delay: 24s;animation-delay: 24s&#125; .cb-slideshow li:nth-child(6) span &#123;-webkit-animation-delay: 30s;-moz-animation-delay: 30s;-o-animation-delay: 30s;-ms-animation-delay: 30s;animation-delay: 30s&#125; @-webkit-keyframes imageAnimation &#123;0% &#123;opacity: 0;-webkit-animation-timing-function: ease-in&#125; 8% &#123;opacity: 1;-webkit-transform: scale(1.05);-webkit-animation-timing-function: ease-out&#125; 17% &#123;opacity: 1;-webkit-transform: scale(1.1) rotate(0)&#125; 25% &#123;opacity: 0;-webkit-transform: scale(1.1) rotate(0)&#125; 100% &#123;opacity: 0&#125;&#125; 2、在/theme/next/layout/中修改_layout.swig，在标签中加入下述代码： 1234567891011121314&lt;ul class="cb-slideshow"&gt; &lt;li&gt; &lt;span&gt;1&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;2&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;3&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;4&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;5&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;6&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt; 在Markdown内添加html锚点来实现页面内跳转在跳转的目的地放置如下锚点： 1&lt;span id="jump"&gt;跳转到的地方&lt;/span&gt; 在跳转的出发点，就是希望点一下跳转的地方放置锚点的链接： 1[点击跳转](#jump) 在Markdown内使用Latex格式公式1、更换渲染引擎，从marked更换为kramed 12$ npm uninstall hexo-renderer-marked --save$ npm install hexo-renderer-kramed --save 2、修改主题配置文件/theme/next/_config.yml，开启mathjax功能，并修改cdn值 12345mathjax: enable: true per_page: true# cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML 3、解决语义冲突，在hexo目录下找到文件node_modules/kramed/lib/rules/inline.js，找到如下位置进行修改： 1234567//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,...//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 4、在写blog时候，在tags:下一行添加： 12mathjax: true]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first try of hexo+github]]></title>
    <url>%2F2018%2F11%2F09%2Fhow-to-config-the-hexo%2F</url>
    <content type="text"><![CDATA[今天我学习了如何配置Hexo来写博客，并放到github上，但是我还不会markdown，自己还是好菜啊QAQ。 卸载之前安装不成功的hexo，npm和nodejs卸载hexo12$ cd /usr/bin$ npm uninstall hexo -g 卸载nodejs和npm123$ sudo apt-get remove nodejs$ sudo apt-get remove node$ sudo apt-get remove npm 可能会没用，我是运行了第一个就把都卸载了，但是不影响哈。 安装nvmnvm是node的包版本控制工具，使用这个我装上了稳定版的hexo，之前我直接apt的hexo在运行 hexo s时会报错： ERROR Local hexo not found。 下载nvm12345$ export NVM_DIR="$HOME/.nvm" &amp;&amp; ( git clone https://github.com/creationix/nvm.git "$NVM_DIR" cd "$NVM_DIR" git checkout `git describe --abbrev=0 --tags --match "v[0-9]*" origin`) &amp;&amp; . "$NVM_DIR/nvm.sh" 配置为nvm自启动在 bash配置文件.bashrc中加入（maybe .zshrc）： 12export NVM_DIR="$HOME/.nvm"[ -s "$NVM_DIR/nvm.sh" ] &amp;&amp; . "$NVM_DIR/nvm.sh" 使用nvm安装nodejs和npm检测远程仓库1$ nvm ls-remote 安装稳定版本nodejs1$ nvm install stable 启动安装好的nodejs1$ nvm use node 设置为nodejs默认版本1$ nvm alias default node 安装hexo使用npm安装hexo1$ npm install -g hexo 初始化hexo本地博客空间1234$ mkdir GitBlog$ cd GitBlog &amp;&amp; hexo init$ npm install$ npm update -g 运行本地服务器查看博客12$ hexo g # 生成静态界面$ hexo s # 开启本地服务 将博客在github上托管配置本机全局git环境假设github使用的邮箱是hehe@qq.com，github用户名是hehe： 12$ git config -global user.email "hehe@qq.com"$ git config -global user.name "hehe" 生成ssh秘钥先检查一下之前有没有生成过ssh的key: 1$ less ~/.ssh/id_rsa.pub 如果有的话会有输出，则不需要生成秘钥，如果没有执行指令： 1$ ssh-keygen -t rsa -C hehe@qq.com 接着会提示输入文件夹位置来放置ssh秘钥，并会让你确认一个验证的密码，如果不需要可以回车跳过。成功的话会在~/.ssh下生成ssh秘钥，即所谓的公钥id_rsa.pub和私钥id_rsa(RSA加密)。 在github创建博客工程在github用户下新建一个仓库，按照上面的假设，那么我生成的这个仓库名应该是hehe@github.io。然后将之前生成的ssh秘钥添加到github上。 在hexo中配置使用git部署博客在站点的配置文件(博客根目录下的_config.yml)中配置你的git: 1234deploy:type: gitrepo: git@github.com:hehe/hehe.github.io.gitbrach: master 将本地文件上传到github上本地修改好了，可以先在本地运行一下看一下效果： 1$ hexo s 如果没什么问题，接下来将本地内容部署到github服务器上： 123$ hexo clean$ hexo g$ hexo d 有的时候可能要清除一下浏览器的缓存才能显示更新后的内容，具体原因我也不知道。部署完成后，可以直接在浏览器输入hehe@github.io就能看到部署完成的hexo blog了。]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F08%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
