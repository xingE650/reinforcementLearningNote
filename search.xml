<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[10-armed-Testbed]]></title>
    <url>%2F2018%2F11%2F16%2F10-armed-Testbed%2F</url>
    <content type="text"><![CDATA[通过建立10-armed-Testbed来仿真第二章讲的几种Bandit算法1、引入模块123456import matplotlibimport matplotlib.pyplot as plt%matplotlib inlineimport numpy as np# use for showing process barfrom tqdm import tqdm 2、创建Testbed类，实现基本的action和update value方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class Bandit: # @k_arm: # of arms # @epsilon: probability for exploration in epsilon-greedy algorithm # @initial: initial estimation for each action # @step_size: constant step size for updating estimations # @sample_averages: if True, use sample averages to update estimations instead of constant step size # @UCB_param: if not None, use UCB algorithm to select action # @gradient: if True, use gradient based bandit algorithm # @gradient_baseline: if True, use average reward as baseline for gradient based bandit algorithm def __init__(self, k_arm=10, epsilon=0., initial=0., step_size=0.1, sample_averages=False, UCB_param=None, gradient=False, gradient_baseline=False, true_reward=0.): self.k = k_arm self.step_size = step_size self.sample_averages = sample_averages self.indices = np.arange(self.k) self.time = 0 self.UCB_param = UCB_param self.gradient = gradient self.gradient_baseline = gradient_baseline self.average_reward = 0 self.true_reward = true_reward self.epsilon = epsilon self.initial = initial def reset(self): # real reward for each action # true_reward is baseline can be configed # np.random.randn(_size()) return standard normal array in the size of _size() self.q_true = np.random.randn(self.k) + self.true_reward # estimation for each action # here we can modify the serf.initial to export optimistic initial value self.q_estimation = np.zeros(self.k) + self.initial # of chosen times for each action self.action_count = np.zeros(self.k) # return the index of max-real-reward action self.best_action = np.argmax(self.q_true) # get an action for this bandit def act(self): # np.random.rand() return uniform distribution over [0,1) # explore if np.random.rand() &lt; self.epsilon: # return np.random.choice(self.indices) # I think UCB may choose actions in non-greedy actions if self.UCB_param is not None: # here is a little different from book, Why? # I think he may want to be avoid of zero problem in log and denominator position UCB_estimation = self.q_estimation + \ self.UCB_param * np.sqrt(np.log(self.time + 1) / (self.action_count + 1e-5)) q_best = np.max(UCB_estimation) return np.random.choice([action for action, q in enumerate(UCB_estimation) if q == q_best]) else: return np.random.choice(self.indices) if self.gradient: exp_est = np.exp(self.q_estimation) self.action_prob = exp_est / np.sum(exp_est) # According to the probability to choose action # here use the same method with wiki return np.random.choice(self.indices, p=self.action_prob) # greey-action return np.argmax(self.q_estimation) # take an action, update estimation for this action def step(self, action): # generate the reward under N(real reward, 1) reward = np.random.randn() + self.q_true[action] self.time += 1 # average_reward update uses self.time self.average_reward = (self.time - 1.0) / self.time * self.average_reward + reward / self.time self.action_count[action] += 1 if self.sample_averages: # update estimation using sample averages # q_estimation update uses action_count self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action]) # is different with book elif self.gradient: one_hot = np.zeros(self.k) one_hot[action] = 1 if self.gradient_baseline: baseline = self.average_reward else: baseline = 0 # update method is the same as book, here use q_estimation(a) replaces H_t(a) self.q_estimation = self.q_estimation + self.step_size * (reward - baseline) * (one_hot - self.action_prob) else: # update estimation with constant step size self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action]) return reward 3、训练Bandit123456789101112131415def simulate(runs, time, bandits): best_action_counts = np.zeros((len(bandits), runs, time)) rewards = np.zeros(best_action_counts.shape) for i, bandit in enumerate(bandits): for r in tqdm(range(runs)): bandit.reset() for t in range(time): action = bandit.act() reward = bandit.step(action) rewards[i, r, t] = reward if action == bandit.best_action: best_action_counts[i, r, t] = 1 best_action_counts = best_action_counts.mean(axis=1) rewards = rewards.mean(axis=1) return best_action_counts, rewards 4、结果显示（折线图格式）1、10-Bandit-Testbed value 和reward分布123456def figure_2_1(): plt.violinplot(dataset=np.random.randn(200,10) + np.random.randn(10)) plt.xlabel("Action") plt.ylabel("Reward distribution") plt.show()figure_2_1() 2、different epsilons12345678910111213141516171819202122def figure_2_2(runs=2000, time=1000): epsilons = [0, 0.1, 0.01] bandits = [Bandit(epsilon=eps, sample_averages=True) for eps in epsilons] best_action_counts, rewards = simulate(runs, time, bandits) plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) for eps, rewards in zip(epsilons, rewards): plt.plot(rewards, label='epsilon = %.02f' % (eps)) plt.xlabel('steps') plt.ylabel('average reward') plt.legend() plt.subplot(2, 1, 2) for eps, counts in zip(epsilons, best_action_counts): plt.plot(counts, label='epsilon = %.02f' % (eps)) plt.xlabel('steps') plt.ylabel('% optimal action') plt.legend() plt.show()# figure_2_2() 3、Initial value = 5 VS Initial value = 01234567891011121314def figure_2_3(runs=2000, time=1000): bandits = [] bandits.append(Bandit(epsilon=0, initial=5, step_size=0.1)) bandits.append(Bandit(epsilon=0.1, initial=0, step_size=0.1)) best_action_counts, _ = simulate(runs, time, bandits) plt.plot(best_action_counts[0], label='epsilon = 0, q = 5') plt.plot(best_action_counts[1], label='epsilon = 0.1, q = 0') plt.xlabel('Steps') plt.ylabel('% optimal action') plt.legend() plt.show()#figure_2_3() 4、UCB VS epsilon-greey12345678910111213def figure_2_4(runs=2000, time=1000): bandits = [] bandits.append(Bandit(epsilon=0, UCB_param=2, sample_averages=True)) bandits.append(Bandit(epsilon=0.1, sample_averages=True)) _, average_rewards = simulate(runs, time, bandits) plt.plot(average_rewards[0], label='UCB c = 2') plt.plot(average_rewards[1], label='epsilon greedy epsilon = 0.1') plt.xlabel('Steps') plt.ylabel('Average reward') plt.legend() plt.show()#figure_2_4() 5、softmax baseline VS non-baseline12345678910111213141516171819def figure_2_5(runs=2000, time=1000): bandits = [] bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=True, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=False, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=True, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=False, true_reward=4)) best_action_counts, _ = simulate(runs, time, bandits) labels = ['alpha = 0.1, with baseline', 'alpha = 0.1, without baseline', 'alpha = 0.4, with baseline', 'alpha = 0.4, without baseline'] for i in range(0, len(bandits)): plt.plot(best_action_counts[i], label=labels[i]) plt.xlabel('Steps') plt.ylabel('% Optimal action') plt.legend() plt.show()#figure_2_5() 6、epsilon-greey vs softmax vs UCB vs opt-initial123456789101112131415161718192021222324252627282930def figure_2_6(runs=2000, time=1000): labels = ['epsilon-greedy', 'gradient bandit', 'UCB', 'optimistic initialization'] generators = [lambda epsilon: Bandit(epsilon=epsilon, sample_averages=True), lambda alpha: Bandit(gradient=True, step_size=alpha, gradient_baseline=True), lambda coef: Bandit(epsilon=0, UCB_param=coef, sample_averages=True), lambda initial: Bandit(epsilon=0, initial=initial, step_size=0.1)] parameters = [np.arange(-7, -1, dtype=np.float), np.arange(-5, 2, dtype=np.float), np.arange(-4, 3, dtype=np.float), np.arange(-2, 3, dtype=np.float)] bandits = [] for generator, parameter in zip(generators, parameters): for param in parameter: bandits.append(generator(pow(2, param))) _, average_rewards = simulate(runs, time, bandits) rewards = np.mean(average_rewards, axis=1) i = 0 for label, parameter in zip(labels, parameters): l = len(parameter) plt.plot(parameter, rewards[i:i+l], label=label) i += l plt.xlabel('Parameter(2^x)') plt.ylabel('Average reward') plt.legend() plt.show()figure_2_6()]]></content>
  </entry>
  <entry>
    <title><![CDATA[softmax-theory]]></title>
    <url>%2F2018%2F11%2F15%2Fsoftmax-theory%2F</url>
    <content type="text"><![CDATA[This post is my understanding about P29 deeper insight part:《The Bandit Gradient Algorithm as Stochastic Gradient Ascent》 1. Understand Why it comes from Stochastic Gradient Ascent在我们讨论这个问题之前先看看wiki上关于softmax在reinforcementLearning上的应用吧。 相信你和我一样惊奇，这里本来的H_t(a)变成了q_t(a)/τ。其中τ为温度系数(temperature parameter)，τ-&gt;无穷，所以action具有相同的选择概率，或者说是explore的；如果τ-&gt;0,则具有最大q_t(a)的行为会被选择，且τ越小，选择q_t(a)最大的概率越大并趋于1。 这太有意思了，wiki上直接解释了softmax function选择action的合理性，但是这就是这篇post想要讨论的问题啊@_@。 好了回到标题，书上这里这样写的： In exact gradient ascent, each preference H_t(a) would be incremented proportional to the increment’s effect on performance: 并给出了公式： E[R_t]的定义为： 从梯度上升的角度来解释：如果平均的value:E[R_t]对H_t(a)导数为正，则提高H_t(a)可以提高E[R_t]；如果相反，则减小H_t(a)可以提高E[R_t]。总结起来就是H_t(a)的增量和E[R_t]对H_t(a)的偏导成正比。 计算偏导计算公式如下： 公式的难点主要在第三步B_t的引入。B_t is called the baseline, can be any scalar that does not depend on x. We can include a baseline here without changing the equality because the gradient sums to zero over all the actions: change the equation to the form of expectation 这里的难点是使用R_t替换q_(A_t)，因为E[R_t|A_t] = q_(A_t)，所以可以替换。接着我们处理一下后半部分： 其中1_(a=x) means that if a=x, output 1; else output 0 因为我们通过无数次训练来更新H_t(a)，所以可以去掉期望符号，并在前面加上step-size α进行近似。如果α=1/n，那么就符合大数定律近似了，如果采用常数可能对解决非平稳问题更好一些。]]></content>
  </entry>
  <entry>
    <title><![CDATA[k-armed Bandits]]></title>
    <url>%2F2018%2F11%2F14%2Fk-armed-Bandits%2F</url>
    <content type="text"><![CDATA[2.1.1 什么是k-armed Bandits问题You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.大致意思就是，每步有k种action选择，每种选择对应不同的reward，完成的目标就是需要在n次执行后，最大化总共的reward。 In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action: 2.1.2 k-armed Bandits问题的难点这里主要是explore和exploit的权衡。下面的解释摘自周老师的西瓜书强化学习部分： 若仅为知道每个摇臂的期望奖赏，则可采用“仅探索”(exploration-only)法：将所有的尝试机会平均分配给每个摇臂，最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。若仅为执行奖赏最大的动作，则可以用“仅利用”(exploitation-only)法，按下目前最优的摇臂。前者可以很好的估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会，没办法得到最好的收益；后者刚好相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优的摇臂，因此这两种方法都难以使最终的积累奖励最大化。 根据实验情况，exploring-only收敛结果明显劣于exploiting-only，而exploiting-only效果同样很差。 2.2 Action-value Method这是原文标题我直接抄过来了。因为上个标题提到了行为的价值(value)，所以需要给一种量化这种value的算法。 We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: 然后如何选择action？greedy action：选择Qt最大的a作为t时刻的action ε-greedy action: behave greedily most of the time, but every once in a while, say with small probability ε, instead select randomly from among all the actions with equal probability,independently of the action-value estimates. 2.3 The 10-armed Testbed10-armed Testbed是测试k-armed Bandits算法的一个基础模型，后面的模型评估都是基于此的。 What is it?简而言之，Testbed中包含了2000个10-armed Bandits问题，每个问题的10个行为action对应的value q(a)是通过正态分布得到的(mean=0,variance=1)，而实际对每个问题进行训练的时候，得到的实际回报(actual reward)是在action value上附加一个正态分布(mean=q(a),variance=1)。然后用greedy算法和ε-greedy算法对每个问题进行1000step的训练，对2000个问题求平均数得到 target(Average reward or Optimal action) - Steps的折线图。并以此来评价算法性能。 算法对比参照Python代码的博客 2.4 Incremental Implementation这部分讲了一个增量优化算法，这让我联想到了增量式PID和位置式PID，QAQ。其实这个思想和那个也挺相似的。 previous edition: PS:这个公式和第一章的tic-tac-toe的V(s)更新公式太像了。 but，还是有一些区别。第一章的V(s)更新的时候是仅针对greedy-action的，但是这里因为各个action是独立的，如果不对exploring更新的话，就没办法有效的找到最优的action了，即这次action是无意义的。 incremental edition: incremental bandit algorithm: an intresting update ruleThe update rule below is of a form that occurs frequently throughout this book. The general form is: about this rule:1、The expression [Target−OldEstimate] is an error in the estimate. It is reduced by taking a step toward the “Target.” The target is presumed to indicate a desirable direction in which to move, though it may be noisy. In the case above, for example, the target is the nth reward. 2、Note that the step-size parameter (StepSize) used in the incremental method described above changes from time step to time step. In processing the nth reward for action a, the method uses the step-size parameter 1/n . In this book we denote the step-size parameter by α or, more generally, by α_t(a). 2.5 Tracking a Nonstationary(非平稳) Problem上面讨论的Bandits-methods，都是建立在rewards的概率不变的前提下的，如Testbed中规定，reward(a)遵循(mean = q*(a),variance = 1)的概率分布。而在实际的强化学习任务中，经常是与之相对的概率不平稳的(nonstationary)。 考虑上一个标题讨论的update rule的一般形式，处理非平稳问题一个有效方法就是令α为定值(use a constant step-size parameter)，因为这样可以提高最近的reward的比重，降低过去久远的reward的比重，可以参考下面的公式： 因为α&lt;1，所以很明显如果i越小，R_i的weight就会越小。 但是有一个问题，就是如果希望最终的value是收敛的，则Stepsize需要满足如下条件： 第一条保证weight足够大，可以覆盖掉初值的误差以及一些波动，第二条保证最后能够收敛。 当α=1/n条件很明显是满足的，但是如果α=constant第二条就不满足了。但是对于非稳定的情况，这正是我们希望看到的，具体证明也没给出，下次看到补上？ 2.6 Optimistic Initial Values(encouraging exploration)我们在上面看到α=1/n时，Q1被消去了，但是事实上大部分情况下Q1会对学习情况产生影响，比如当α=constant时，Q1就被保留了，不过*了一个相当小的系数。。。 事实上，this kind of bias is usually not a problem and can sometimes be very helpful.举个栗子，在Testbed里，我把训练初值设为5，对于在1附近正态分布的reward，不管选择哪个，在初始阶段都会&lt;5，学习器对他们都很失望：Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being “disappointed” with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time. 但是对于非平稳问题(nonstationary)，因为分布会随时间改变，而这种只在训练开始引入exploring的方法相当于只是encourage exploration for once，故效果不会很好，但是对应sample-average(平稳)的问题就会有一定的效果。 当然，我们不会只采用一种方法，将不同的方法进行组合利用，将会是贯穿全书的思想。 2.7 Upper-Confidence-Bound Action Selection在ε-greedy方法中，通过一个小概率ε来选定non-greedy action进行exploring，但是这种方法是无差别的，如果对那些non-greedy action进行一个事前的评估，根据它们潜能(不确定度)来进行explore的话，那么可以提高explore的广度和效率，效果可能会更好。 One effective way of doing this is to select actions according to(UCB): N_t(a) denotes the number of times that action a has been selected prior to time t.number c &gt; 0 controls the degree of exploration.If N_t(a) = 0, then a is considered to be a maximizing action.公式里的平方根部分是对action a的不确定度的估计，可以从两方面来解释：如果action a经常被选中，则分母N_t就会变大，不确定度就会减小；如果action a不经常被选中，那么随着试验次数t增大，而N_t不变，则不确定度上升，我们应该多考虑一下a啦。 但是也存在一些问题UCB比起ε-greedy方法，相对来说更复杂，而且它在其他强化学习问题中的可扩展性(extend)远不如后者，而且这种方法在非平稳问题中表现的并不好，Another difficulty is dealing with large state spaces, particularly when using function approximation as developed in Part II of this book.In these more advanced settings the idea of UCB action selection is usually not practical. 2.8 Gradient Bandit Algorithms看了这么久，休息一下如何^_^ A new method to choose actionconsider learning a numerical preference for each action a, which we denote H_t(a). The larger the preference, the more often that action is taken: π_t(a) is the probability of taking action a at time t. Initially all preferences are the same(zero)so that all actions have an equal probability of being selected. How to update the H_t(a) in the rule: α&gt;0 is a step-size paramter; (R_t)_average is the baseline with which the reward is compared.It which can be computed incrementally as described in Incremental Implementation about the baselineIf the reward is higher than the baseline, then the probability of taking A_t in the future is increased, and if the reward is below baseline, then probability is decreased. The non-selected actions move in the opposite direction. the theory of softmax methodhere is a seperate blog to explain it. 2.9 Associative Search (Contextual Bandits)上面的算法只处理了一台k-arms-Bandit，接下来考虑这个问题：如果你面对的是10台k-arms-Bandit，每次你随机从这几台机器(k-arms-Bandit comes from slot machine)里抽一个来操作，那么可以认为这仍是在处理同一台slot machine，但是true value是随着一步一步操作剧烈变化的。 说到这里应该就明白了，我的每一步action都是会对接下来的situation和reward引起影响的，而普通的k-arms-Bandit每步之间是独立的。 associative search可以认为是k-arms-Bandit和full reinforcement learning之间的桥梁，现在它一般被称为contextual bandits问题。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Tic-Tac-Toe]]></title>
    <url>%2F2018%2F11%2F14%2FTic-Tac-Toe%2F</url>
    <content type="text"><![CDATA[Tic-Tac-Toe的python代码实现这份代码来自Oxford大佬的github,很受教。1、引入模块并定义井字棋的常量 12345678import numpy as npimport pickleimport matplotlib.pyplot as plt%matplotlib inline# 将rows和cols调至4，这种算法计算量就会大很多BOARD_ROWS = 3BOARD_COLS = 3BOARD_SIZE = BOARD_ROWS * BOARD_COLS 2、创建环境，State类，表征棋盘上的X和O子的情况。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class State: def __init__(self): # the board is represented by an n * n array, # 1 represents a chessman of the player who moves first, # -1 represents a chessman of another player # 0 represents an empty position self.data = np.zeros((BOARD_ROWS, BOARD_COLS)) self.winner = None self.hash_val = None self.end = None # compute the hash value for one state, it only work at first time def hash(self): if self.hash_val is None: self.hash_val = 0 for i in self.data.reshape(BOARD_ROWS * BOARD_COLS): if i == -1: # shallow copy i = 2 self.hash_val = self.hash_val * 3 + i return int(self.hash_val) # check whether a player has won the game, or it's a tie def is_end(self): if self.end is not None: return self.end results = [] # check row for i in range(0, BOARD_ROWS): results.append(np.sum(self.data[i, :])) # check columns for i in range(0, BOARD_COLS): results.append(np.sum(self.data[:, i])) # check diagonals results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, i] results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, BOARD_ROWS - 1 - i] for result in results: if result == 3: self.winner = 1 self.end = True return self.end if result == -3: self.winner = -1 self.end = True return self.end # whether it's a tie: no one wins, all places are filled. sum = np.sum(np.abs(self.data)) if sum == BOARD_ROWS * BOARD_COLS: self.winner = 0 self.end = True return self.end # game is still going on self.end = False return self.end # @symbol: 1 or -1 # put chessman symbol in position (i, j) def next_state(self, i, j, symbol): new_state = State() # deep copy new_state.data new_state.data = np.copy(self.data) new_state.data[i, j] = symbol return new_state # print the board def print(self): for i in range(0, BOARD_ROWS): print('-------------') out = '| ' for j in range(0, BOARD_COLS): if self.data[i, j] == 1: token = '*' if self.data[i, j] == 0: token = '0' if self.data[i, j] == -1: token = 'x' out += token + ' | ' print(out) print('-------------') 关于这里的hash函数，是为了得到不重复的所有棋盘情况。但是为什么可以通过这种算法那？假设两个不同的棋盘情况A和B，他们的hash值是一致的，我们不妨设他们的最高位不相同，那么我们看到，该位最少相差1，通过等比数列求和可以得到，3^(n+1)&gt;2*(1+3^1+3^2+…+3^n)，即最高位不同是无法通过其余位来弥补的，那么可见他们的最高位不同是不可能hash值相同的，以此递推，则可得A和B值是一样，即一样的棋盘，假设不成立。3、公共函数，用来获取当前state。12345678910111213141516171819202122232425def get_all_states_impl(current_state, current_symbol, all_states): for i in range(0, BOARD_ROWS): for j in range(0, BOARD_COLS): if current_state.data[i][j] == 0: newState = current_state.next_state(i, j, current_symbol) newHash = newState.hash() if newHash not in all_states.keys(): isEnd = newState.is_end() all_states[newHash] = (newState, isEnd) if not isEnd: get_all_states_impl(newState, -current_symbol, all_states)def get_all_states(): current_symbol = 1 current_state = State() all_states = dict() all_states[current_state.hash()] = (current_state, current_state.is_end()) get_all_states_impl(current_state, current_symbol, all_states) return all_states# all possible board configurationsall_states = get_all_states()# for i in all_states.values():# i[0].print()# print(i[1]) 4、Player类，用来模拟AI选手下棋的行为，得到value function1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# AI playerclass Player: # @step_size: the step size to update estimations # @epsilon: the probability to explore def __init__(self, step_size=0.1, epsilon=0.1): self.estimations = dict() self.step_size = step_size self.epsilon = epsilon self.states = [] # greedy means that state is caused by exploiting, otherwise is caused by exploring self.greedy = [] def reset(self): self.states = [] self.greedy = [] def set_state(self, state): self.states.append(state) self.greedy.append(True) # give the player symbol,and update the value table def set_symbol(self, symbol): self.symbol = symbol for hash_val in all_states.keys(): (state, is_end) = all_states[hash_val] if is_end: if state.winner == self.symbol: self.estimations[hash_val] = 1.0 elif state.winner == 0: # we need to distinguish between a tie and a lose self.estimations[hash_val] = 0.5 else: self.estimations[hash_val] = 0 else: self.estimations[hash_val] = 0.5 # update value estimation def backup(self): # for debug # print('player trajectory') # for state in self.states: # state.print() self.states = [state.hash() for state in self.states] for i in reversed(range(len(self.states) - 1)): state = self.states[i] # only udpate the state caused by exploiting td_error = self.greedy[i] * (self.estimations[self.states[i + 1]] - self.estimations[state]) self.estimations[state] += self.step_size * td_error # choose an action based on the state def act(self): state = self.states[-1] next_states = [] next_positions = [] for i in range(BOARD_ROWS): for j in range(BOARD_COLS): if state.data[i, j] == 0: next_positions.append([i, j]) next_states.append(state.next_state(i, j, self.symbol).hash()) # explore if np.random.rand() &lt; self.epsilon: action = next_positions[np.random.randint(len(next_positions))] action.append(self.symbol) self.greedy[-1] = False return action # exploit values = [] for hash, pos in zip(next_states, next_positions): values.append((self.estimations[hash], pos)) np.random.shuffle(values) values.sort(key=lambda x: x[0], reverse=True) # choose the biggest value action action = values[0][1] action.append(self.symbol) return action # policy -&gt; self.estimations def save_policy(self): with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f: pickle.dump(self.estimations, f) def load_policy(self): with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f: self.estimations = pickle.load(f) 这里需要注意一点就是，在backup方法里，只有greedy-action才会更新value，exploring虽然不会更新value，但是其随后的greedy-action会进行更新，个人的理解是：exploring并不能保证最优，它只是提供一种“探索”行为，帮助Learning method更好的学习value，所以只对greedy-action进行update。5、创建对决类Judger，通过一次对局完成一次value table的update1234567891011121314151617181920212223242526272829303132333435363738394041424344class Judger: # @player1: the player who will move first, its chessman will be 1 # @player2: another player with a chessman -1 # @feedback: if True, both players will receive rewards when game is end def __init__(self, player1, player2): self.p1 = player1 self.p2 = player2 self.current_player = None self.p1_symbol = 1 self.p2_symbol = -1 self.p1.set_symbol(self.p1_symbol) self.p2.set_symbol(self.p2_symbol) self.current_state = State() def reset(self): self.p1.reset() self.p2.reset() # return p1 and p2 in turn(alternately) def alternate(self): while True: yield self.p1 yield self.p2 # @print: if True, print each board during the game def play(self, print=False): alternator = self.alternate() self.reset() current_state = State() self.p1.set_state(current_state) self.p2.set_state(current_state) while True: player = next(alternator) if print: current_state.print() [i, j, symbol] = player.act() next_state_hash = current_state.next_state(i, j, symbol).hash() current_state, is_end = all_states[next_state_hash] self.p1.set_state(current_state) self.p2.set_state(current_state) if is_end: if print: current_state.print() return current_state.winner 6、训练并让两名AI Player进行对局12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def train(epochs): player1 = Player(epsilon=0.01) player2 = Player(epsilon=0.01) judger = Judger(player1, player2) player1_win = 0.0 player2_win = 0.0 player1_a = [] player2_a = [] for i in range(1, epochs + 1): winner = judger.play(print=False) if winner == 1: player1_win += 1 if winner == -1: player2_win += 1 player1_a.append(player1_win/i) player2_a.append(player2_win/i) #print('Epoch %d, player 1 win %.02f, player 2 win %.02f' % (i, player1_win / i, player2_win / i)) player1.backup() player2.backup() judger.reset() player1.save_policy() player2.save_policy() x = range(1,1001,1) x = list(x) plt.plot(x,player1_a) plt.plot(x,player2_a) plt.axis([0,200,0,1]) plt.title('training chart') plt.show() def compete(turns): player1 = Player(epsilon=0) player2 = Player(epsilon=0) judger = Judger(player1, player2) player1.load_policy() player2.load_policy() player1_win = 0.0 player2_win = 0.0 for i in range(0, turns): winner = judger.play() if winner == 1: player1_win += 1 if winner == -1: player2_win += 1 judger.reset() print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))train(int(1e3))]]></content>
  </entry>
  <entry>
    <title><![CDATA[ReinforcementLearningChapter1]]></title>
    <url>%2F2018%2F11%2F12%2FReinforcementLearningChapter1%2F</url>
    <content type="text"><![CDATA[希望能以更新博客的方式激励一下自己，目前是准备读一下强化学习的入门书《Reinforcement Learning Introduction》，然后做一下读书笔记。下面是绪论(Introduction)的内容。 What is ReinforcementLearningFirst Impression of RL如果说之前我接触到的机器学习方法，如逻辑回归、svm、决策树、人工神经网络等是数据驱动的，那么强化学习就是“情境”驱动的：行为由当前环境决定，行为导致数值化的reward更新，而reward又影响了环境。这是我对强化学习的初印象。 强化学习有两个特点：1、trial-and-error search：学习算法并未指导行为，但是却必须得出当行为做出相应的reward(近期) 2、delayed reward：每个行为之间不是独立的，当前行为可能会有比当前收益更丰盛的远期收益(value)。 强化学习有三个重要要素，就是学习器(agent，我对学习器的理解就是训练出模型时执行的程序）必须能够感知环境(state)并做出影响它的行为(action)，同时agent must have a goal or goals relating to the state of the environment。 RL vs Supervised learning监督学习就不赘述了，大致来说，监督学习算法通过带标签的训练集学习得到可以对未知样例的判断“能力”。但是如果让监督学习从交互中学习，按照监督学习的思路，获取正确的而且有意义的行为例子是不可取的，所以它没办法从经验中成长，不适合需要和环境交互的算法。 举一个不太恰当的例子，监督学习就像考前刷题的你，而强化学习就像每天认真学习的学霸，如果考题和你刷的吻合度比较高，那你就稳了，但是学霸每天学习，数据已经内化为他的能力了，所以他一点都不慌。 RL vs Unsupervised learning有不少人认为监督学习和非监督学习已经把机器学习进行详尽的划分了，他们将强化学习归入后者，因为它们都没有带标签的数据，但是从目的上来看，强化学习是为了最大化reward的，而非监督学习是为了学习到数据之中的隐藏结构，非监督学习的方法并没有从实质解决强化学习的问题。 exploration and exploitationThe agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future. 这个问题有点像梯度下降中使用的随机梯度法(SGD)或者模拟退火法，但是SGD是一种加速算法，或者说如果我头铁不用，我也是有可能得到还可以的结果，但是强化学习如果不学习未知的部分，结果肯定是有问题的，但是如果一直在试错 explore，不对“经验”进行总结 exploit，也是没办法使算法收敛的。 Tic-Tac-Toe我觉得先分析一个例子可以更好的帮助理解RL的各个属性之间的关系。Tic-Tac-Toe比较像我们小时候玩的井字棋，图我就不画了。 强化学习和传统的classical minimax solution from game theory以及Classical optimization methods for sequential decision problems, such as dynamic programming有区别，前者是按照有强约束的方式运动的（这里是不会走使自己失败的位置），而后者则是需要有每一步的先验概率才能进行优化。 强化学习是基于value functions的学习方法，而前面提到的minimax可以认为是基于evolutionary的学习方法，前者是基于value of state来学习的，后者则是通过结果来更新policy，前者对状态的利用优于后者，而后者则是仅利用了结果来修改policy。 tic-tac-toe的ython代码实现戳这里 Elements of RLBeyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal , a value function, and, optionally, a model of the environment. policy感觉是从state到action的一个映射关系吧，这里就对应了上面提到的explore和exploit的关系。In general, policies may be stochastic. reward每个action导致的短期收益。reward会决定policy，使得行为向高reward的方向发展，所以reward一般是state和action的随机函数：In general, reward signals may be stochastic functions of the state of the environment and the actions taken. value function由reward引起的长期收益。value由reward综合而来，而value同时会使reward朝着高的方向发展。value控制action的发展：Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run. model of the environmentThis is something that mimics(模拟) the behavior of the environment, or more generally, that allows inferences(推断) to be made about how the environment will behave. Summary总体来说强化学习和之前学过的其他机器学习方法还是区别挺大的，但是绪论有几点疑问： 1、action、state、reward、value之间的更新方式还是没搞懂？ 2、如何建立value table？ ——————-(answers in 11.14)————————- 1、value建立在state之上，每次state更新伴随着value的更新；action分为两种，explore和exploit，前者随机，后者基于reward最大；reward认为是从current state -&gt; next state update时，对应的next value or (next value - current value) 2、value table建立基于state，原则视问题而定。比如tic-tac-toe问题，X赢对应的value = 1，O赢对应的value = 0，平局对应value = 0.5，当然我是站在X的角度。]]></content>
  </entry>
  <entry>
    <title><![CDATA[some config on next theme]]></title>
    <url>%2F2018%2F11%2F12%2Fsome-config-on-next-theme%2F</url>
    <content type="text"><![CDATA[上周配置了hexo+github的个人博客，这次我做了一些偏好的配置。 更换主题为next因为网上很多关于主题配置的博客都是基于next主题的，所以我先将主题换为了next主题： 1、先下载主题到本地 123$ cd ~$ cd GitBlog$ git clone https://github.com/iissnan/hexo-theme-next themes/next 2、更改站点的config文件来使用主题 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 3、更改theme/next的config文件(/theme/next/_config.yml): 123456789101112# 修改主题模式为Gemini# Schemes#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini#修改侧边栏头像# Sidebar Avatar# in theme directory(source/images): /images/avatar.gif# in site directory(source/uploads): /uploads/avatar.gifavatar: https://wx2.sinaimg.cn/large/0070VybLly1fx5b09mly4j30kx0i14kq.jpg 添加搜索功能下载hexo搜索插件hexo-generator-search然后修改站点的config文件启动搜索功能： 12345678910# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next#在插件位置插入搜索模块使能插件search: path: search.xml field: post format: html limit: 10000 添加渐变更换的壁纸效果比较像忧郁的弟弟(额马上就是loli了)背景效果。 1、在/theme/next/source/css/_custom/中修改_custom.styl，添加css3动画特效： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137body &#123;background: #000;background-attachment: fixed;word-wrap: break-word;-webkit-background-size: cover;-moz-background-size: cover;background-size: cover;background-repeat: no-repeat&#125; ul &#123;list-style: none&#125; .cb-slideshow li:nth-child(1) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b098is8j31hc0u0jzz.jpg)&#125; .cb-slideshow li:nth-child(2) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b095t28j31hc11hdph.jpg)&#125; .cb-slideshow li:nth-child(3) span &#123;background-image: url(https://wx2.sinaimg.cn/large/0070VybLly1fx5b09b17xj31hc0z2tim.jpg)&#125; .cb-slideshow li:nth-child(4) span &#123;background-image: url(https://wx4.sinaimg.cn/large/0070VybLly1fx5b09474xj31hc0x6jy5.jpg)&#125; .cb-slideshow li:nth-child(5) span &#123;background-image: url(https://wx2.sinaimg.cn/large/0070VybLly1fx5b08weitj31hc11otga.jpg)&#125; .cb-slideshow li:nth-child(6) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b1eraokj31hc0u0gp9.jpg)&#125; .cb-slideshow,.cb-slideshow:after &#123;position: fixed;width: 100%;height: 100%;top: 0;left: 0;z-index: -2&#125; .cb-slideshow:after &#123;content: ''&#125; .cb-slideshow li span &#123;width: 100%;height: 100%;position: absolute;top: 0;left: 0;color: transparent;background-size: cover;background-position: 50% 50%;background-repeat: none;opacity: 0;z-index: -2;-webkit-backface-visibility: hidden;-webkit-animation: imageAnimation 36s linear infinite 0s;-moz-animation: imageAnimation 36s linear infinite 0s;-o-animation: imageAnimation 36s linear infinite 0s;-ms-animation: imageAnimation 36s linear infinite 0s;animation: imageAnimation 36s linear infinite 0s&#125; .cb-slideshow li:nth-child(2) span &#123;-webkit-animation-delay: 6s;-moz-animation-delay: 6s;-o-animation-delay: 6s;-ms-animation-delay: 6s;animation-delay: 6s&#125; .cb-slideshow li:nth-child(3) span &#123;-webkit-animation-delay: 12s;-moz-animation-delay: 12s;-o-animation-delay: 12s;-ms-animation-delay: 12s;animation-delay: 12s&#125; .cb-slideshow li:nth-child(4) span &#123;-webkit-animation-delay: 18s;-moz-animation-delay: 18s;-o-animation-delay: 18s;-ms-animation-delay: 18s;animation-delay: 18s&#125; .cb-slideshow li:nth-child(5) span &#123;-webkit-animation-delay: 24s;-moz-animation-delay: 24s;-o-animation-delay: 24s;-ms-animation-delay: 24s;animation-delay: 24s&#125; .cb-slideshow li:nth-child(6) span &#123;-webkit-animation-delay: 30s;-moz-animation-delay: 30s;-o-animation-delay: 30s;-ms-animation-delay: 30s;animation-delay: 30s&#125; @-webkit-keyframes imageAnimation &#123;0% &#123;opacity: 0;-webkit-animation-timing-function: ease-in&#125; 8% &#123;opacity: 1;-webkit-transform: scale(1.05);-webkit-animation-timing-function: ease-out&#125; 17% &#123;opacity: 1;-webkit-transform: scale(1.1) rotate(0)&#125; 25% &#123;opacity: 0;-webkit-transform: scale(1.1) rotate(0)&#125; 100% &#123;opacity: 0&#125;&#125; 2、在/theme/next/layout/中修改_layout.swig，在标签中加入下述代码： 1234567891011121314&lt;ul class="cb-slideshow"&gt; &lt;li&gt; &lt;span&gt;1&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;2&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;3&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;4&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;5&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;6&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[my first try of hexo+github]]></title>
    <url>%2F2018%2F11%2F12%2Fhow-to-config-the-hexo%2F</url>
    <content type="text"><![CDATA[今天我学习了如何配置Hexo来写博客，并放到github上，但是我还不会markdown，自己还是好菜啊QAQ。 卸载之前安装不成功的hexo，npm和nodejs卸载hexo12$ cd /usr/bin$ npm uninstall hexo -g ###卸载nodejs和npm 123$ sudo apt-get remove nodejs$ sudo apt-get remove node$ sudo apt-get remove npm 可能会没用，我是运行了第一个就把都卸载了，但是不影响哈。 安装nvmnvm是node的包版本控制工具，使用这个我装上了稳定版的hexo，之前我直接apt的hexo在运行 hexo s时会报错： ERROR Local hexo not found。 下载nvm12345$ export NVM_DIR="$HOME/.nvm" &amp;&amp; ( git clone https://github.com/creationix/nvm.git "$NVM_DIR" cd "$NVM_DIR" git checkout `git describe --abbrev=0 --tags --match "v[0-9]*" origin`) &amp;&amp; . "$NVM_DIR/nvm.sh" 配置为nvm自启动在 bash配置文件.bashrc中加入（maybe .zshrc）： 12export NVM_DIR="$HOME/.nvm"[ -s "$NVM_DIR/nvm.sh" ] &amp;&amp; . "$NVM_DIR/nvm.sh" 使用nvm安装nodejs和npm检测远程仓库1$ nvm ls-remote 安装稳定版本nodejs1$ nvm install stable 启动安装好的nodejs1$ nvm use node 设置为nodejs默认版本1$ nvm alias default node 安装hexo使用npm安装hexo1$ npm install -g hexo 初始化hexo本地博客空间1234$ mkdir GitBlog$ cd GitBlog &amp;&amp; hexo init$ npm install$ npm update -g 运行本地服务器查看博客12$ hexo g # 生成静态界面$ hexo s # 开启本地服务 将博客在github上托管配置本机全局git环境假设github使用的邮箱是hehe@qq.com，github用户名是hehe： 12$ git config -global user.email "hehe@qq.com"$ git config -global user.name "hehe" 生成ssh秘钥先检查一下之前有没有生成过ssh的key: 1$ less ~/.ssh/id_rsa.pub 如果有的话会有输出，则不需要生成秘钥，如果没有执行指令： 1$ ssh-keygen -t rsa -C hehe@qq.com 接着会提示输入文件夹位置来放置ssh秘钥，并会让你确认一个验证的密码，如果不需要可以回车跳过。成功的话会在~/.ssh下生成ssh秘钥，即所谓的公钥id_rsa.pub和私钥id_rsa(RSA加密)。 在github创建博客工程在github用户下新建一个仓库，按照上面的假设，那么我生成的这个仓库名应该是hehe@github.io。然后将之前生成的ssh秘钥添加到github上。 在hexo中配置使用git部署博客在站点的配置文件(博客根目录下的_config.yml)中配置你的git: 1234deploy:type: gitrepo: git@github.com:hehe/hehe.github.io.gitbrach: master 将本地文件上传到github上本地修改好了，可以先在本地运行一下看一下效果： 1$ hexo s 如果没什么问题，接下来将本地内容部署到github服务器上： 123$ hexo clean$ hexo g$ hexo d 有的时候可能要清除一下浏览器的缓存才能显示更新后的内容，具体原因我也不知道。部署完成后，可以直接在浏览器输入hehe@github.io就能看到部署完成的hexo blog了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F11%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
