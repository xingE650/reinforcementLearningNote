<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Tic-Tac-Toe]]></title>
    <url>%2F2018%2F11%2F14%2FTic-Tac-Toe%2F</url>
    <content type="text"><![CDATA[Tic-Tac-Toe的python代码实现1、引入模块并定义井字棋的常量 12345678import numpy as npimport pickleimport matplotlib.pyplot as plt%matplotlib inline# 将rows和cols调至4，这种算法计算量就会大很多BOARD_ROWS = 3BOARD_COLS = 3BOARD_SIZE = BOARD_ROWS * BOARD_COLS 2、创建环境，State类，表征棋盘上的X和O子的情况。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class State: def __init__(self): # the board is represented by an n * n array, # 1 represents a chessman of the player who moves first, # -1 represents a chessman of another player # 0 represents an empty position self.data = np.zeros((BOARD_ROWS, BOARD_COLS)) self.winner = None self.hash_val = None self.end = None # compute the hash value for one state, it only work at first time def hash(self): if self.hash_val is None: self.hash_val = 0 for i in self.data.reshape(BOARD_ROWS * BOARD_COLS): if i == -1: # shallow copy i = 2 self.hash_val = self.hash_val * 3 + i return int(self.hash_val) # check whether a player has won the game, or it's a tie def is_end(self): if self.end is not None: return self.end results = [] # check row for i in range(0, BOARD_ROWS): results.append(np.sum(self.data[i, :])) # check columns for i in range(0, BOARD_COLS): results.append(np.sum(self.data[:, i])) # check diagonals results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, i] results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, BOARD_ROWS - 1 - i] for result in results: if result == 3: self.winner = 1 self.end = True return self.end if result == -3: self.winner = -1 self.end = True return self.end # whether it's a tie: no one wins, all places are filled. sum = np.sum(np.abs(self.data)) if sum == BOARD_ROWS * BOARD_COLS: self.winner = 0 self.end = True return self.end # game is still going on self.end = False return self.end # @symbol: 1 or -1 # put chessman symbol in position (i, j) def next_state(self, i, j, symbol): new_state = State() # deep copy new_state.data new_state.data = np.copy(self.data) new_state.data[i, j] = symbol return new_state # print the board def print(self): for i in range(0, BOARD_ROWS): print('-------------') out = '| ' for j in range(0, BOARD_COLS): if self.data[i, j] == 1: token = '*' if self.data[i, j] == 0: token = '0' if self.data[i, j] == -1: token = 'x' out += token + ' | ' print(out) print('-------------') 关于这里的hash函数，是为了得到不重复的所有棋盘情况。但是为什么可以通过这种算法那？假设两个不同的棋盘情况A和B，他们的hash值是一致的，我们不妨设他们的最高位不相同，那么我们看到，该位最少相差1，通过等比数列求和可以得到，3^(n+1)&gt;2*(1+3^1+3^2+…+3^n)，即最高位不同是无法通过其余位来弥补的，那么可见他们的最高位不同是不可能hash值相同的，以此递推，则可得A和B值是一样，即一样的棋盘，假设不成立。3、公共函数，用来获取当前state。12345678910111213141516171819202122232425def get_all_states_impl(current_state, current_symbol, all_states): for i in range(0, BOARD_ROWS): for j in range(0, BOARD_COLS): if current_state.data[i][j] == 0: newState = current_state.next_state(i, j, current_symbol) newHash = newState.hash() if newHash not in all_states.keys(): isEnd = newState.is_end() all_states[newHash] = (newState, isEnd) if not isEnd: get_all_states_impl(newState, -current_symbol, all_states)def get_all_states(): current_symbol = 1 current_state = State() all_states = dict() all_states[current_state.hash()] = (current_state, current_state.is_end()) get_all_states_impl(current_state, current_symbol, all_states) return all_states# all possible board configurationsall_states = get_all_states()# for i in all_states.values():# i[0].print()# print(i[1]) 4、Player类，用来模拟AI选手下棋的行为，得到value function1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# AI playerclass Player: # @step_size: the step size to update estimations # @epsilon: the probability to explore def __init__(self, step_size=0.1, epsilon=0.1): self.estimations = dict() self.step_size = step_size self.epsilon = epsilon self.states = [] # greedy means that state is caused by exploiting, otherwise is caused by exploring self.greedy = [] def reset(self): self.states = [] self.greedy = [] def set_state(self, state): self.states.append(state) self.greedy.append(True) # give the player symbol,and update the value table def set_symbol(self, symbol): self.symbol = symbol for hash_val in all_states.keys(): (state, is_end) = all_states[hash_val] if is_end: if state.winner == self.symbol: self.estimations[hash_val] = 1.0 elif state.winner == 0: # we need to distinguish between a tie and a lose self.estimations[hash_val] = 0.5 else: self.estimations[hash_val] = 0 else: self.estimations[hash_val] = 0.5 # update value estimation def backup(self): # for debug # print('player trajectory') # for state in self.states: # state.print() self.states = [state.hash() for state in self.states] for i in reversed(range(len(self.states) - 1)): state = self.states[i] # only udpate the state caused by exploiting td_error = self.greedy[i] * (self.estimations[self.states[i + 1]] - self.estimations[state]) self.estimations[state] += self.step_size * td_error # choose an action based on the state def act(self): state = self.states[-1] next_states = [] next_positions = [] for i in range(BOARD_ROWS): for j in range(BOARD_COLS): if state.data[i, j] == 0: next_positions.append([i, j]) next_states.append(state.next_state(i, j, self.symbol).hash()) # explore if np.random.rand() &lt; self.epsilon: action = next_positions[np.random.randint(len(next_positions))] action.append(self.symbol) self.greedy[-1] = False return action # exploit values = [] for hash, pos in zip(next_states, next_positions): values.append((self.estimations[hash], pos)) np.random.shuffle(values) values.sort(key=lambda x: x[0], reverse=True) # choose the biggest value action action = values[0][1] action.append(self.symbol) return action # policy -&gt; self.estimations def save_policy(self): with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f: pickle.dump(self.estimations, f) def load_policy(self): with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f: self.estimations = pickle.load(f) 5、创建对决类Judger，通过一次对局完成一次value table的update1234567891011121314151617181920212223242526272829303132333435363738394041424344class Judger: # @player1: the player who will move first, its chessman will be 1 # @player2: another player with a chessman -1 # @feedback: if True, both players will receive rewards when game is end def __init__(self, player1, player2): self.p1 = player1 self.p2 = player2 self.current_player = None self.p1_symbol = 1 self.p2_symbol = -1 self.p1.set_symbol(self.p1_symbol) self.p2.set_symbol(self.p2_symbol) self.current_state = State() def reset(self): self.p1.reset() self.p2.reset() # return p1 and p2 in turn(alternately) def alternate(self): while True: yield self.p1 yield self.p2 # @print: if True, print each board during the game def play(self, print=False): alternator = self.alternate() self.reset() current_state = State() self.p1.set_state(current_state) self.p2.set_state(current_state) while True: player = next(alternator) if print: current_state.print() [i, j, symbol] = player.act() next_state_hash = current_state.next_state(i, j, symbol).hash() current_state, is_end = all_states[next_state_hash] self.p1.set_state(current_state) self.p2.set_state(current_state) if is_end: if print: current_state.print() return current_state.winner 6、训练并让两名AI Player进行对局12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def train(epochs): player1 = Player(epsilon=0.01) player2 = Player(epsilon=0.01) judger = Judger(player1, player2) player1_win = 0.0 player2_win = 0.0 player1_a = [] player2_a = [] for i in range(1, epochs + 1): winner = judger.play(print=False) if winner == 1: player1_win += 1 if winner == -1: player2_win += 1 player1_a.append(player1_win/i) player2_a.append(player2_win/i) #print('Epoch %d, player 1 win %.02f, player 2 win %.02f' % (i, player1_win / i, player2_win / i)) player1.backup() player2.backup() judger.reset() player1.save_policy() player2.save_policy() x = range(1,1001,1) x = list(x) plt.plot(x,player1_a) plt.plot(x,player2_a) plt.axis([0,200,0,1]) plt.title('training chart') plt.show() def compete(turns): player1 = Player(epsilon=0) player2 = Player(epsilon=0) judger = Judger(player1, player2) player1.load_policy() player2.load_policy() player1_win = 0.0 player2_win = 0.0 for i in range(0, turns): winner = judger.play() if winner == 1: player1_win += 1 if winner == -1: player2_win += 1 judger.reset() print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))train(int(1e3))]]></content>
  </entry>
  <entry>
    <title><![CDATA[ReinforcementLearningChapter1]]></title>
    <url>%2F2018%2F11%2F12%2FReinforcementLearningChapter1%2F</url>
    <content type="text"><![CDATA[希望能以更新博客的方式激励一下自己，目前是准备读一下强化学习的入门书《Reinforcement Learning Introduction》，然后做一下读书笔记。下面是绪论(Introduction)的内容。 What is ReinforcementLearningFirst Impression of RL如果说之前我接触到的机器学习方法，如逻辑回归、svm、决策树、人工神经网络等是数据驱动的，那么强化学习就是“情境”驱动的：行为由当前环境决定，行为导致数值化的reward更新，而reward又影响了环境。这是我对强化学习的初印象。 强化学习有两个特点：1、trial-and-error search：学习算法并未指导行为，但是却必须得出当行为做出相应的reward(近期) 2、delayed reward：每个行为之间不是独立的，当前行为可能会有比当前收益更丰盛的远期收益(value)。 强化学习有三个重要要素，就是学习器(agent，我对学习器的理解就是训练出模型时执行的程序）必须能够感知环境(state)并做出影响它的行为(action)，同时agent must have a goal or goals relating to the state of the environment。 RL vs Supervised learning监督学习就不赘述了，大致来说，监督学习算法通过带标签的训练集学习得到可以对未知样例的判断“能力”。但是如果让监督学习从交互中学习，按照监督学习的思路，获取正确的而且有意义的行为例子是不可取的，所以它没办法从经验中成长，不适合需要和环境交互的算法。 举一个不太恰当的例子，监督学习就像考前刷题的你，而强化学习就像每天认真学习的学霸，如果考题和你刷的吻合度比较高，那你就稳了，但是学霸每天学习，数据已经内化为他的能力了，所以他一点都不慌。 RL vs Unsupervised learning有不少人认为监督学习和非监督学习已经把机器学习进行详尽的划分了，他们将强化学习归入后者，因为它们都没有带标签的数据，但是从目的上来看，强化学习是为了最大化reward的，而非监督学习是为了学习到数据之中的隐藏结构，非监督学习的方法并没有从实质解决强化学习的问题。 exploration and exploitationThe agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future. 这个问题有点像梯度下降中使用的随机梯度法(SGD)或者模拟退火法，但是SGD是一种加速算法，或者说如果我头铁不用，我也是有可能得到还可以的结果，但是强化学习如果不学习未知的部分，结果肯定是有问题的，但是如果一直在试错explore，不对“经验”进行总结exploit，也是没办法使算法收敛的。 Tic-Tac-Toe我觉得先分析一个例子可以更好的帮助理解RL的各个属性之间的关系。Tic-Tac-Toe比较像我们小时候玩的井字棋，图我就不画了。 强化学习和传统的classical minimax solution from game theory以及Classical optimization methods for sequential decision problems, such as dynamic programming有区别，前者是按照有强约束的方式运动的（这里是不会走使自己失败的位置），而后者则是需要有每一步的先验概率才能进行优化。 强化学习是基于value functions的学习方法，而前面提到的minimax可以认为是基于evolutionary的学习方法，前者是基于value of state来学习的，后者则是通过结果来更新policy，前者对状态的利用优于后者，而后者则是仅利用了结果来修改policy。 所以我认为强化学习理解的难点应该是在于如果建立value function以及如何收敛value tables，包括收敛的证明？ Elements of RLBeyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal , a value function, and, optionally, a model of the environment. policy感觉是从state到action的一个映射关系吧，这里就对应了上面提到的explore和exploit的关系。In general, policies may be stochastic. reward每个action导致的短期收益。reward会决定policy，使得行为向高reward的方向发展，所以reward一般是state和action的随机函数：In general, reward signals may be stochastic functions of the state of the environment and the actions taken. value function由reward引起的长期收益。value由reward综合而来，而value同时会使reward朝着高的方向发展。value控制action的发展：Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run. model of the environmentThis is something that mimics(模拟) the behavior of the environment, or more generally, that allows inferences(推断) to be made about how the environment will behave. Summary总体来说强化学习和之前学过的其他机器学习方法还是区别挺大的，但是绪论有几点疑问： 1、action、state、reward、value之间的更新方式还是没搞懂？ 2、如何建立value table？]]></content>
  </entry>
  <entry>
    <title><![CDATA[some config on next theme]]></title>
    <url>%2F2018%2F11%2F12%2Fsome-config-on-next-theme%2F</url>
    <content type="text"><![CDATA[上周配置了hexo+github的个人博客，这次我做了一些偏好的配置。 更换主题为next因为网上很多关于主题配置的博客都是基于next主题的，所以我先将主题换为了next主题： 1、先下载主题到本地 123$ cd ~$ cd GitBlog$ git clone https://github.com/iissnan/hexo-theme-next themes/next 2、更改站点的config文件来使用主题 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 3、更改theme/next的config文件(/theme/next/_config.yml): 123456789101112# 修改主题模式为Gemini# Schemes#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini#修改侧边栏头像# Sidebar Avatar# in theme directory(source/images): /images/avatar.gif# in site directory(source/uploads): /uploads/avatar.gifavatar: https://wx2.sinaimg.cn/large/0070VybLly1fx5b09mly4j30kx0i14kq.jpg 添加搜索功能下载hexo搜索插件hexo-generator-search然后修改站点的config文件启动搜索功能： 12345678910# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next#在插件位置插入搜索模块使能插件search: path: search.xml field: post format: html limit: 10000 添加渐变更换的壁纸效果比较像忧郁的弟弟(额马上就是loli了)背景效果。 1、在/theme/next/source/css/_custom/中修改_custom.styl，添加css3动画特效： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137body &#123;background: #000;background-attachment: fixed;word-wrap: break-word;-webkit-background-size: cover;-moz-background-size: cover;background-size: cover;background-repeat: no-repeat&#125; ul &#123;list-style: none&#125; .cb-slideshow li:nth-child(1) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b098is8j31hc0u0jzz.jpg)&#125; .cb-slideshow li:nth-child(2) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b095t28j31hc11hdph.jpg)&#125; .cb-slideshow li:nth-child(3) span &#123;background-image: url(https://wx2.sinaimg.cn/large/0070VybLly1fx5b09b17xj31hc0z2tim.jpg)&#125; .cb-slideshow li:nth-child(4) span &#123;background-image: url(https://wx4.sinaimg.cn/large/0070VybLly1fx5b09474xj31hc0x6jy5.jpg)&#125; .cb-slideshow li:nth-child(5) span &#123;background-image: url(https://wx2.sinaimg.cn/large/0070VybLly1fx5b08weitj31hc11otga.jpg)&#125; .cb-slideshow li:nth-child(6) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b1eraokj31hc0u0gp9.jpg)&#125; .cb-slideshow,.cb-slideshow:after &#123;position: fixed;width: 100%;height: 100%;top: 0;left: 0;z-index: -2&#125; .cb-slideshow:after &#123;content: ''&#125; .cb-slideshow li span &#123;width: 100%;height: 100%;position: absolute;top: 0;left: 0;color: transparent;background-size: cover;background-position: 50% 50%;background-repeat: none;opacity: 0;z-index: -2;-webkit-backface-visibility: hidden;-webkit-animation: imageAnimation 36s linear infinite 0s;-moz-animation: imageAnimation 36s linear infinite 0s;-o-animation: imageAnimation 36s linear infinite 0s;-ms-animation: imageAnimation 36s linear infinite 0s;animation: imageAnimation 36s linear infinite 0s&#125; .cb-slideshow li:nth-child(2) span &#123;-webkit-animation-delay: 6s;-moz-animation-delay: 6s;-o-animation-delay: 6s;-ms-animation-delay: 6s;animation-delay: 6s&#125; .cb-slideshow li:nth-child(3) span &#123;-webkit-animation-delay: 12s;-moz-animation-delay: 12s;-o-animation-delay: 12s;-ms-animation-delay: 12s;animation-delay: 12s&#125; .cb-slideshow li:nth-child(4) span &#123;-webkit-animation-delay: 18s;-moz-animation-delay: 18s;-o-animation-delay: 18s;-ms-animation-delay: 18s;animation-delay: 18s&#125; .cb-slideshow li:nth-child(5) span &#123;-webkit-animation-delay: 24s;-moz-animation-delay: 24s;-o-animation-delay: 24s;-ms-animation-delay: 24s;animation-delay: 24s&#125; .cb-slideshow li:nth-child(6) span &#123;-webkit-animation-delay: 30s;-moz-animation-delay: 30s;-o-animation-delay: 30s;-ms-animation-delay: 30s;animation-delay: 30s&#125; @-webkit-keyframes imageAnimation &#123;0% &#123;opacity: 0;-webkit-animation-timing-function: ease-in&#125; 8% &#123;opacity: 1;-webkit-transform: scale(1.05);-webkit-animation-timing-function: ease-out&#125; 17% &#123;opacity: 1;-webkit-transform: scale(1.1) rotate(0)&#125; 25% &#123;opacity: 0;-webkit-transform: scale(1.1) rotate(0)&#125; 100% &#123;opacity: 0&#125;&#125; 2、在/theme/next/layout/中修改_layout.swig，在标签中加入下述代码： 1234567891011121314&lt;ul class="cb-slideshow"&gt; &lt;li&gt; &lt;span&gt;1&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;2&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;3&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;4&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;5&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;6&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[my first try of hexo+github]]></title>
    <url>%2F2018%2F11%2F12%2Fhow-to-config-the-hexo%2F</url>
    <content type="text"><![CDATA[今天我学习了如何配置Hexo来写博客，并放到github上，但是我还不会markdown，自己还是好菜啊QAQ。 卸载之前安装不成功的hexo，npm和nodejs卸载hexo12$ cd /usr/bin$ npm uninstall hexo -g ###卸载nodejs和npm 123$ sudo apt-get remove nodejs$ sudo apt-get remove node$ sudo apt-get remove npm 可能会没用，我是运行了第一个就把都卸载了，但是不影响哈。 安装nvmnvm是node的包版本控制工具，使用这个我装上了稳定版的hexo，之前我直接apt的hexo在运行 hexo s时会报错： ERROR Local hexo not found。 下载nvm12345$ export NVM_DIR="$HOME/.nvm" &amp;&amp; ( git clone https://github.com/creationix/nvm.git "$NVM_DIR" cd "$NVM_DIR" git checkout `git describe --abbrev=0 --tags --match "v[0-9]*" origin`) &amp;&amp; . "$NVM_DIR/nvm.sh" 配置为nvm自启动在 bash配置文件.bashrc中加入（maybe .zshrc）： 12export NVM_DIR="$HOME/.nvm"[ -s "$NVM_DIR/nvm.sh" ] &amp;&amp; . "$NVM_DIR/nvm.sh" 使用nvm安装nodejs和npm检测远程仓库1$ nvm ls-remote 安装稳定版本nodejs1$ nvm install stable 启动安装好的nodejs1$ nvm use node 设置为nodejs默认版本1$ nvm alias default node 安装hexo使用npm安装hexo1$ npm install -g hexo 初始化hexo本地博客空间1234$ mkdir GitBlog$ cd GitBlog &amp;&amp; hexo init$ npm install$ npm update -g 运行本地服务器查看博客12$ hexo g # 生成静态界面$ hexo s # 开启本地服务 将博客在github上托管配置本机全局git环境假设github使用的邮箱是hehe@qq.com，github用户名是hehe： 12$ git config -global user.email "hehe@qq.com"$ git config -global user.name "hehe" 生成ssh秘钥先检查一下之前有没有生成过ssh的key: 1$ less ~/.ssh/id_rsa.pub 如果有的话会有输出，则不需要生成秘钥，如果没有执行指令： 1$ ssh-keygen -t rsa -C hehe@qq.com 接着会提示输入文件夹位置来放置ssh秘钥，并会让你确认一个验证的密码，如果不需要可以回车跳过。成功的话会在~/.ssh下生成ssh秘钥，即所谓的公钥id_rsa.pub和私钥id_rsa(RSA加密)。 在github创建博客工程在github用户下新建一个仓库，按照上面的假设，那么我生成的这个仓库名应该是hehe@github.io。然后将之前生成的ssh秘钥添加到github上。 在hexo中配置使用git部署博客在站点的配置文件(博客根目录下的_config.yml)中配置你的git: 1234deploy:type: gitrepo: git@github.com:hehe/hehe.github.io.gitbrach: master 将本地文件上传到github上本地修改好了，可以先在本地运行一下看一下效果： 1$ hexo s 如果没什么问题，接下来将本地内容部署到github服务器上： 123$ hexo clean$ hexo g$ hexo d 有的时候可能要清除一下浏览器的缓存才能显示更新后的内容，具体原因我也不知道。部署完成后，可以直接在浏览器输入hehe@github.io就能看到部署完成的hexo blog了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F11%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
