<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Chapter06 maximization-bias and Double-Learning]]></title>
    <url>%2F2018%2F12%2F01%2FChapter06-maximization-bias-and-Double-Learning%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter06/maximization_bias.py 因为TD算法中的target policy建立中经常会用到maximization操作，在这些算法中，a maximum over estimated values is used implicitly as an estimate of the maximum value,这可能会导致显著的正向偏差，本例通过一个简单的MRP来讨论这个问题。 问题描述这个例子是为了说明并解决TD方法将估计的最大值作为实际Q的最大值造成的正向偏差(bias)，使用了一个简单的MRP: A是start state，左右的灰色小方格是terminal state。A状态下可以有两个action：left和right；如果向右直接会结束，reward=0；向左则进入B状态，B状态有多个达到左边terminal state的actions可选，并对应reward=N(-0.1,1)，所以(A,left)开始的expect return = -0.1。 引入模块并定义常量12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 6.7 Maximization Bias and Double Learninimport numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm# copy module可以提供浅拷贝和深拷贝方法import copy# state ASTATE_A = 0# state BSTATE_B = 1# use one terminal stateSTATE_TERMINAL = 2# starts from state ASTATE_START = STATE_A# possible actions in AACTION_A_RIGHT = 0ACTION_A_LEFT = 1# probability for explorationEPSILON = 0.1# step sizeALPHA = 0.1# discount for max valueGAMMA = 1.0# possible actions in B, maybe 10 actionsACTIONS_B = range(0, 10)# all possible actionsSTATE_ACTIONS = [[ACTION_A_RIGHT, ACTION_A_LEFT], ACTIONS_B]# state action pair values, if a state is a terminal state, then the value is always 0# 按照顺序分别是:(A,left) (A,right) (B,0) ... (B,9) (C)INITIAL_Q = [np.zeros(2), np.zeros(len(ACTIONS_B)), np.zeros(1)]# set up destination for each state and each action# 这个list通过索引给出next state，之前的都是在take_action函数中顺便给出下一个state的# TRANSITION[state][action] = next-stateTRANSITION = [[STATE_TERMINAL, STATE_B], [STATE_TERMINAL] * len(ACTIONS_B)] 提供choose action和take action函数12345678910111213# choose an action based on epsilon greedy algorithmdef choose_action(state, q_value): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(STATE_ACTIONS[state]) else: values_ = q_value[state] return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])# take @action in @state, return the rewarddef take_action(state, action): if state == STATE_A: return 0 return np.random.normal(-0.1, 1) 使用Q-Learning 和 Double Q-Learning方法来训练并计算policy选择(A|left)的概率123456789101112131415161718192021222324252627282930313233343536373839# if there are two state action pair value array, use double Q-Learning# otherwise use normal Q-Learningdef q_learning(q1, q2=None): state = STATE_START # track the # of action left in state A left_count = 0 while state != STATE_TERMINAL: if q2 is None: action = choose_action(state, q1) else: # derive(得到) a action form Q1 and Q2 # choose action的时候是结合两个Q综合选择的 action = choose_action(state, [item1 + item2 for item1, item2 in zip(q1, q2)]) if state == STATE_A and action == ACTION_A_LEFT: left_count += 1 reward = take_action(state, action) next_state = TRANSITION[state][action] if q2 is None: active_q = q1 target = np.max(active_q[next_state]) else: # 根据50%的概率来选择更新q1还是q2 if np.random.binomial(1, 0.5) == 1: active_q = q1 target_q = q2 else: active_q = q2 target_q = q1 # 从active_q中选择max value的action best_action = np.random.choice([action_ for action_, value_ in enumerate(active_q[next_state]) if value_ == np.max(active_q[next_state])]) # 从target_q中选择与active_q对应的target来更新Q target = target_q[next_state][best_action] # Q-Learning update active_q[state][action] += ALPHA * ( reward + GAMMA * target - active_q[state][action]) state = next_state return left_count 12345678910111213141516171819202122232425262728# Figure 6.7, 1,000 runs may be enough, the number of actions in state B will also affect the curvesdef figure_6_7(): # each independent run has 300 episodes episodes = 300 runs = 1000 left_counts_q = np.zeros((runs, episodes)) left_counts_double_q = np.zeros((runs, episodes)) for run in tqdm(range(runs)): q = copy.deepcopy(INITIAL_Q) q1 = copy.deepcopy(INITIAL_Q) q2 = copy.deepcopy(INITIAL_Q) for ep in range(0, episodes): left_counts_q[run, ep] = q_learning(q) left_counts_double_q[run, ep] = q_learning(q1, q2) left_counts_q = left_counts_q.mean(axis=0) left_counts_double_q = left_counts_double_q.mean(axis=0) plt.plot(left_counts_q, label='Q-Learning') plt.plot(left_counts_double_q, label='Double Q-Learning') plt.plot(np.ones(episodes) * 0.05, label='Optimal') plt.xlabel('episodes') plt.ylabel('% left actions from A') plt.legend() plt.savefig('./figure_6_7.png') plt.show()figure_6_7() 100%|██████████| 1000/1000 [00:44&lt;00:00, 23.10it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter06 Sarsa vs Q-Learning vs Expected Sarsa]]></title>
    <url>%2F2018%2F12%2F01%2FChapter06-Sarsa-vs-Q-Learning-vs-Expected-Sarsa%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter06/windy_grid_world.py、chapter06/cliff_walking.py 本篇包含了两份代码，第一个主要测试了on-policy Sarsa的性能，第二个对标题的三种算法性能进行了比较 一、问题描述和普通的grid-world不同之处是，在格子的中间区域，存在上升的wind，所以在该处的action会附加一个up的action。 引入模块并定义常量12345678910111213141516171819202122232425262728293031323334#Sarsa import numpy as npimport matplotlibimport matplotlib.pyplot as plt%matplotlib inline# world heightWORLD_HEIGHT = 7# world widthWORLD_WIDTH = 10# wind strength for each columnWIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]# possible actionsACTION_UP = 0ACTION_DOWN = 1ACTION_LEFT = 2ACTION_RIGHT = 3# probability for explorationEPSILON = 0.1# Sarsa step sizeALPHA = 0.5# reward for each stepREWARD = -1.0START = [3, 0]GOAL = [3, 7]ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT] 使用on-policy策略的Sarsa来更新state-action-vlaue12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 根据current-state和action对返回next statedef step(state, action): i, j = state if action == ACTION_UP: return [max(i - 1 - WIND[j], 0), j] elif action == ACTION_DOWN: return [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), j] elif action == ACTION_LEFT: return [max(i - WIND[j], 0), max(j - 1, 0)] elif action == ACTION_RIGHT: return [max(i - WIND[j], 0), min(j + 1, WORLD_WIDTH - 1)] else: assert False# play for an episodedef episode(q_value): # track the total time steps in this episode time = 0 # initialize state state = START # choose an action based on epsilon-greedy algorithm if np.random.binomial(1, EPSILON) == 1: action = np.random.choice(ACTIONS) else: values_ = q_value[state[0], state[1], :] action = np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) # keep going until get to the goal state while state != GOAL: next_state = step(state, action) if np.random.binomial(1, EPSILON) == 1: next_action = np.random.choice(ACTIONS) else: values_ = q_value[next_state[0], next_state[1], :] next_action = np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) # Sarsa update q_value[state[0], state[1], action] += \ ALPHA * (REWARD + q_value[next_state[0], next_state[1], next_action] - q_value[state[0], state[1], action]) state = next_state action = next_action time += 1 return time 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def figure_6_3(): q_value = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4)) episode_limit = 10000 q_values = [] steps = [] ep = 0 while ep &lt; episode_limit: steps.append(episode(q_value)) # time = episode(q_value) # episodes.extend([ep] * time) ep += 1 # q_values.append(q_value) # q_values = np.array(q_values) # q_values = np.add.accumulate(q_values) steps = np.add.accumulate(steps)# plt.plot(steps, np.arange(1, len(steps) + 1))# plt.xlabel('Time steps')# plt.ylabel('Episodes') plt.plot(np.arange(1,episode_limit+1),np.around(steps/np.arange(1,episode_limit+1))) plt.xlabel('episodes') plt.ylabel('time steps') plt.ylim(18,22) plt.savefig('./figure_6_3.png') plt.show() # display the optimal policy optimal_policy = [] for i in range(0, WORLD_HEIGHT): optimal_policy.append([]) for j in range(0, WORLD_WIDTH): if [i, j] == GOAL: optimal_policy[-1].append('G') continue #返回最大action的索引index bestAction = np.argmax(q_value[i, j, :]) if bestAction == ACTION_UP: optimal_policy[-1].append('U') elif bestAction == ACTION_DOWN: optimal_policy[-1].append('D') elif bestAction == ACTION_LEFT: optimal_policy[-1].append('L') elif bestAction == ACTION_RIGHT: optimal_policy[-1].append('R') print('Optimal policy is:') for row in optimal_policy: print(row) print('Wind strength for each column:\n&#123;&#125;'.format([str(w) for w in WIND])) figure_6_3() 可以看到随着多个episode的迭代，使用的step数目逐渐收敛。 Optimal policy is: [&apos;R&apos;, &apos;D&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;D&apos;] [&apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;L&apos;, &apos;R&apos;, &apos;D&apos;] [&apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;U&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;D&apos;] [&apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;G&apos;, &apos;R&apos;, &apos;D&apos;] [&apos;R&apos;, &apos;U&apos;, &apos;U&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;U&apos;, &apos;D&apos;, &apos;L&apos;, &apos;L&apos;] [&apos;R&apos;, &apos;D&apos;, &apos;R&apos;, &apos;U&apos;, &apos;R&apos;, &apos;U&apos;, &apos;U&apos;, &apos;D&apos;, &apos;R&apos;, &apos;L&apos;] [&apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;L&apos;] Wind strength for each column: [&apos;0&apos;, &apos;0&apos;, &apos;0&apos;, &apos;1&apos;, &apos;1&apos;, &apos;1&apos;, &apos;2&apos;, &apos;2&apos;, &apos;1&apos;, &apos;0&apos;] 二、问题描述这个问题在grid-world基础上做了一些改动，主要测试了3种算法的性能：Sarsa、Q-Learning、Expect Sarsa。 在一般区域(非阴影部分)action的reward=-1，如果action导致next state落到cliff区域，则reward=-100，而且agent会回到start state。到达goal state的return=0。 引入模块并定义常量12345678910111213141516171819202122232425262728293031import numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm# world heightWORLD_HEIGHT = 4# world widthWORLD_WIDTH = 12# probability for explorationEPSILON = 0.1# step sizeALPHA = 0.5# gamma for Q-Learning and Expected SarsaGAMMA = 1# all possible actionsACTION_UP = 0ACTION_DOWN = 1ACTION_LEFT = 2ACTION_RIGHT = 3ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]# initial state action pair valuesSTART = [3, 0]GOAL = [3, 11] function for taking action and choosing action12345678910111213141516171819202122232425262728293031# 根据所给state-action对给出next-state和对应的rewarddef step(state, action): i, j = state if action == ACTION_UP: next_state = [max(i - 1, 0), j] elif action == ACTION_LEFT: next_state = [i, max(j - 1, 0)] elif action == ACTION_RIGHT: next_state = [i, min(j + 1, WORLD_WIDTH - 1)] elif action == ACTION_DOWN: next_state = [min(i + 1, WORLD_HEIGHT - 1), j] else: # 如果action不在上述范围，直接触发异常 assert False reward = -1 if (action == ACTION_DOWN and i == 2 and 1 &lt;= j &lt;= 10) or ( action == ACTION_RIGHT and state == START): reward = -100 next_state = START return next_state, reward# choose an action based on epsilon greedy algorithmdef choose_action(state, q_value): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(ACTIONS) else: # greedy choose values_ = q_value[state[0], state[1], :] return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) 使用on-policy Sarsa方法训练1234567891011121314151617181920212223242526272829303132333435363738# an episode with Sarsa# @q_value: values for state action pair, will be updated# @expected: if True, will use expected Sarsa algorithm# @step_size: step size for updating# @return: total rewards within this episode# sarsa的方法可以按照这个循环来理解：choose action-&gt;take action-&gt;choose next-action-&gt;update，# 所以要把第一次choose action放到循环外面去。# 因为是on-policy方法，所以choose target使用的是基于Q的epsilon-greedy方法，take action更新的是Qdef sarsa(q_value, expected=False, step_size=ALPHA): state = START action = choose_action(state, q_value) rewards = 0.0 while state != GOAL: next_state, reward = step(state, action) next_action = choose_action(next_state, q_value) rewards += reward if not expected: target = q_value[next_state[0], next_state[1], next_action] else: # calculate the expected value of new state target = 0.0 q_next = q_value[next_state[0], next_state[1], :] best_actions = np.argwhere(q_next == np.max(q_next)) for action_ in ACTIONS: if action_ in best_actions: # 通过epsilon-greedy policy的π(a|s)计算期望 target += ((1.0 - EPSILON) / len(best_actions) + EPSILON / len(ACTIONS)) * q_value[next_state[0], next_state[1], action_] else: target += EPSILON / len(ACTIONS) * q_value[next_state[0], next_state[1], action_] target *= GAMMA q_value[state[0], state[1], action] += step_size * ( reward + target - q_value[state[0], state[1], action]) state = next_state action = next_action # rewards是所有action的reward总和 return rewards 使用off-policy Q-Learning方法训练123456789101112131415161718192021# an episode with Q-Learning# @q_value: values for state action pair, will be updated# @step_size: step size for updating# @return: total rewards within this episode# Q-Learning采用循环：choose action-&gt;take action-&gt;update，因为是off-policy的所以结构挺简单的# 个人觉得Q-Learning不是严格的off-policy的，因为target policy通过改变Q，其实也是间接的影响了behavior policy# 所以train data is not strictly off target policydef q_learning(q_value, step_size=ALPHA): state = START rewards = 0.0 while state != GOAL: action = choose_action(state, q_value) next_state, reward = step(state, action) rewards += reward # Q-Learning update q_value[state[0], state[1], action] += step_size * ( reward + GAMMA * np.max(q_value[next_state[0], next_state[1], :]) - q_value[state[0], state[1], action]) state = next_state return rewards 输出优化的action1234567891011121314151617181920# print optimal policydef print_optimal_policy(q_value): optimal_policy = [] for i in range(0, WORLD_HEIGHT): optimal_policy.append([]) for j in range(0, WORLD_WIDTH): if [i, j] == GOAL: optimal_policy[-1].append('G') continue bestAction = np.argmax(q_value[i, j, :]) if bestAction == ACTION_UP: optimal_policy[-1].append('U') elif bestAction == ACTION_DOWN: optimal_policy[-1].append('D') elif bestAction == ACTION_LEFT: optimal_policy[-1].append('L') elif bestAction == ACTION_RIGHT: optimal_policy[-1].append('R') for row in optimal_policy: print(row) 比较Sarsa和Q-Learning的rewards并给出两者的优化action123456789101112131415161718192021222324252627282930313233343536373839404142434445# Use multiple runs instead of a single run and a sliding window# With a single run I failed to present a smooth curve# However the optimal policy converges well with a single run# Sarsa converges to the safe path, while Q-Learning converges to the optimal pathdef figure_6_4(): # episodes of each run episodes = 500 # perform 40 independent runs runs = 50 rewards_sarsa = np.zeros(episodes) rewards_q_learning = np.zeros(episodes) for r in tqdm(range(runs)): q_sarsa = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4)) q_q_learning = np.copy(q_sarsa) for i in range(0, episodes): # cut off the value by -100 to draw the figure more elegantly # rewards_sarsa[i] += max(sarsa(q_sarsa), -100) # rewards_q_learning[i] += max(q_learning(q_q_learning), -100) rewards_sarsa[i] += sarsa(q_sarsa) rewards_q_learning[i] += q_learning(q_q_learning) # averaging over independt runs rewards_sarsa /= runs rewards_q_learning /= runs # draw reward curves plt.plot(rewards_sarsa, label='Sarsa') plt.plot(rewards_q_learning, label='Q-Learning') plt.xlabel('Episodes') plt.ylabel('Sum of rewards during episode') plt.ylim([-100, 0]) plt.legend() plt.savefig('./figure_6_4.png') plt.show() # display optimal policy print('Sarsa Optimal Policy:') print_optimal_policy(q_sarsa) print('Q-Learning Optimal Policy:') print_optimal_policy(q_q_learning)figure_6_4() 100%|██████████| 50/50 [00:56&lt;00:00, 1.15s/it] 最终训练结果Sarsa收敛到较安全的位于上部区域的路径(问题描述图片中的 safe path)，Q-Learning收敛到靠近cliff的路径(optimal path)，但是因为epsilon-greedy的behavior policy，所以会导致Q-Learning偶尔会落入cliff，所以最终的总的reward低于Sarsa。 Sarsa Optimal Policy: [&apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;D&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;D&apos;] [&apos;R&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;R&apos;, &apos;R&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;R&apos;, &apos;D&apos;] [&apos;U&apos;, &apos;R&apos;, &apos;R&apos;, &apos;L&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;L&apos;, &apos;U&apos;, &apos;U&apos;, &apos;R&apos;, &apos;D&apos;] [&apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;G&apos;] Q-Learning Optimal Policy: [&apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;R&apos;, &apos;R&apos;, &apos;U&apos;, &apos;D&apos;, &apos;R&apos;, &apos;R&apos;, &apos;D&apos;, &apos;D&apos;, &apos;D&apos;] [&apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;D&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;D&apos;] [&apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;R&apos;, &apos;D&apos;] [&apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;U&apos;, &apos;G&apos;] Interim and asymptotic performance of TD control methods as a function of α12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# Due to limited capacity of calculation of my machine, I can't complete this experiment# with 100,000 episodes and 50,000 runs to get the fully averaged performance# However even I only play for 1,000 episodes and 10 runs, the curves looks still good.def figure_6_6(): # α step_sizes = np.arange(0.1, 1.1, 0.1) episodes = 1000 runs = 10 # Asymptotic: 渐进的 ASY_SARSA = 0 ASY_EXPECTED_SARSA = 1 ASY_QLEARNING = 2 # Interim: 暂时的 INT_SARSA = 3 INT_EXPECTED_SARSA = 4 INT_QLEARNING = 5 methods = range(0, 6) performace = np.zeros((6, len(step_sizes))) for run in range(runs): for ind, step_size in tqdm(list(zip(range(0, len(step_sizes)), step_sizes))): q_sarsa = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4)) q_expected_sarsa = np.copy(q_sarsa) q_q_learning = np.copy(q_sarsa) for ep in range(episodes): sarsa_reward = sarsa(q_sarsa, expected=False, step_size=step_size) expected_sarsa_reward = sarsa(q_expected_sarsa, expected=True, step_size=step_size) q_learning_reward = q_learning(q_q_learning, step_size=step_size) performace[ASY_SARSA, ind] += sarsa_reward performace[ASY_EXPECTED_SARSA, ind] += expected_sarsa_reward performace[ASY_QLEARNING, ind] += q_learning_reward if ep &lt; 100: performace[INT_SARSA, ind] += sarsa_reward performace[INT_EXPECTED_SARSA, ind] += expected_sarsa_reward performace[INT_QLEARNING, ind] += q_learning_reward performace[:3, :] /= episodes * runs performace[3:, :] /= 100 * runs labels = ['Asymptotic Sarsa', 'Asymptotic Expected Sarsa', 'Asymptotic Q-Learning', 'Interim Sarsa', 'Interim Expected Sarsa', 'Interim Q-Learning'] for method, label in zip(methods, labels): plt.plot(step_sizes, performace[method, :], label=label) plt.xlabel('alpha') plt.ylabel('reward per episode') plt.legend() plt.savefig('./figure_6_6.png') plt.show()figure_6_6() 100%|██████████| 10/10 [00:41&lt;00:00, 4.57s/it] 100%|██████████| 10/10 [00:41&lt;00:00, 4.53s/it] 100%|██████████| 10/10 [00:41&lt;00:00, 4.55s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.35s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.36s/it] 100%|██████████| 10/10 [00:41&lt;00:00, 4.40s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.33s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.27s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.37s/it] 100%|██████████| 10/10 [00:40&lt;00:00, 4.28s/it]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter06 TD(0) vs constant-alpha-MC]]></title>
    <url>%2F2018%2F12%2F01%2FChapter06-TD-0-vs-constant-alpha-MC%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter06/random_walk.py 通过一个例子比较了TD(0)和constant-α MC方法的训练性能 问题描述通过一个MRP来比较constant-α MC Method和TD(0)方法之间的训练性能。 MRP的状态转移示意图： 其中state C 是起始状态，向右达到终点return=1，向左return=0，其余每一步reward=0 所以每个state的true value就是该state到达最右端端点的概率。A-E：1/6,2/6,..5/6 计算也很容易，通过迭代计算列出5个式子，P(s)代表从改点到达右端点的概率： P(E)=1/2 + 1/2 * P(D) P(D)=1/2 P(C) + 1/2 P(E) P(C)=1/2 P(B) + 1/2 P(D) P(B)=1/2 P(A) + 1/2 P(C) P(A)=1/2 * P(B) 计算即可得到上面提到的答案。 引入模块并定义常量12345678910111213141516171819202122232425# 6.2 Advantages of TD Prediction Methodsimport numpy as npimport matplotlib%matplotlib inlineimport matplotlib.pyplot as pltfrom tqdm import tqdm# 0 is the left terminal state# 6 is the right terminal state# 1 ... 5 represents A ... EVALUES = np.zeros(7)VALUES[1:6] = 0.5# For convenience, we assume all rewards are 0# and the left terminal state has value 0, the right terminal state has value 1# This trick has been used in Gambler's ProblemVALUES[6] = 1# set up true state valuesTRUE_VALUE = np.zeros(7)TRUE_VALUE[1:6] = np.arange(1, 6) / 6.0TRUE_VALUE[6] = 1ACTION_LEFT = 0ACTION_RIGHT = 1 使用TD(0)方法update state-value123456789101112131415161718192021222324# @values: current states value, will be updated if @batch is False# @alpha: step size# @batch: whether to update @valuesdef temporal_difference(values, alpha=0.1, batch=False): state = 3 trajectory = [state] # TD方法的rewards array是每一步action产生的reward rewards = [0] while True: old_state = state if np.random.binomial(1, 0.5) == ACTION_LEFT: state -= 1 else: state += 1 # Assume all rewards are 0 reward = 0 trajectory.append(state) # TD update if not batch: values[old_state] += alpha * (reward + values[state] - values[old_state]) if state == 6 or state == 0: break rewards.append(reward) return trajectory, rewards 使用Monte Carlo方法update state-value1234567891011121314151617181920212223242526272829# @values: current states value, will be updated if @batch is False# @alpha: step size# @batch: whether to update @valuesdef monte_carlo(values, alpha=0.1, batch=False): state = 3 trajectory = [3] # if end up with left terminal state, all returns are 0 # if end up with right terminal state, all returns are 1 while True: if np.random.binomial(1, 0.5) == ACTION_LEFT: state -= 1 else: state += 1 trajectory.append(state) if state == 6: # monte carlo方法的return是指从state s开始到结束产生的收益,即G_t returns = 1.0 break elif state == 0: returns = 0.0 break if not batch: for state_ in trajectory[:-1]: # MC update values[state_] += alpha * (returns - values[state_]) # 因为中间状态的reward=0，所以所以trajectory上的state的return都是一样的，并且取决于最终结束状态 return trajectory, [returns] * (len(trajectory) - 1) 通过使用TD(0)方法建立state-value，并绘制收敛图像1234567891011121314151617# Example 6.2 leftdef compute_state_value(): episodes = [0, 1, 10, 100] current_values = np.copy(VALUES) plt.figure(1) for i in tqdm(range(episodes[-1] + 1)): if i in episodes: plt.plot(current_values, label=str(i) + ' episodes') temporal_difference(current_values) plt.plot(TRUE_VALUE, label='true values') plt.xlabel('state') plt.ylabel('estimated value') plt.legend()# test# 可以看到结果逐渐收敛了# compute_state_value() 计算并比较TD(0)和constant α-MC method的rms(均方差误差)1234567891011121314151617181920212223242526272829303132333435# Example 6.2 rightdef rms_error(): # Same alpha value can appear in both arrays td_alphas = [0.15, 0.1, 0.05] mc_alphas = [0.01, 0.02, 0.03, 0.04] episodes = 100 + 1 runs = 100 # list 相加相当于将两个list首尾相接 for i, alpha in enumerate(td_alphas + mc_alphas): total_errors = np.zeros(episodes) if i &lt; len(td_alphas): method = 'TD' linestyle = 'solid' else: method = 'MC' linestyle = 'dashdot' for r in tqdm(range(runs)): errors = [] current_values = np.copy(VALUES) for i in range(0, episodes): errors.append(np.sqrt(np.sum(np.power(TRUE_VALUE - current_values, 2)) / 5.0)) if method == 'TD': temporal_difference(current_values, alpha=alpha) else: monte_carlo(current_values, alpha=alpha) total_errors += np.asarray(errors) total_errors /= runs plt.plot(total_errors, linestyle=linestyle, label=method + ', alpha = %.02f' % (alpha)) plt.xlabel('episodes') plt.ylabel('RMS') plt.legend()# test# 可以看到TD方法收敛的更快，rms更低# rms_error() 使用一般的方法进行state-value收敛并绘制图像12345678910111213def example_6_2(): plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) compute_state_value() plt.subplot(2, 1, 2) rms_error() plt.tight_layout() plt.savefig('./example_6_2.png') plt.show()example_6_2() 100%|██████████| 101/101 [00:00&lt;00:00, 7677.14it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 173.89it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 179.95it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 169.18it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 247.49it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 239.68it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 236.73it/s] 100%|██████████| 100/100 [00:00&lt;00:00, 229.08it/s] 使用batch-update方法优化state-value的收敛过程并绘制图像123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# Figure 6.2(Example 6.3)# @method: 'TD' or 'MC'def batch_updating(method, episodes, alpha=0.001): # perform 100 independent runs runs = 100 total_errors = np.zeros(episodes) for r in tqdm(range(0, runs)): current_values = np.copy(VALUES) errors = [] # track shown trajectories and reward/return sequences trajectories = [] rewards = [] for ep in range(episodes): # batch=True将导致state-value不被更新 if method == 'TD': trajectory_, rewards_ = temporal_difference(current_values, batch=True) else: trajectory_, rewards_ = monte_carlo(current_values, batch=True) trajectories.append(trajectory_) rewards.append(rewards_) while True: # keep feeding our algorithm with trajectories seen so far until state value function converges updates = np.zeros(7) for trajectory_, rewards_ in zip(trajectories, rewards): # len(trajectory)-1表明不考虑终止状态 for i in range(0, len(trajectory_) - 1): if method == 'TD': updates[trajectory_[i]] += rewards_[i] + current_values[trajectory_[i + 1]] - current_values[trajectory_[i]] else: updates[trajectory_[i]] += rewards_[i] - current_values[trajectory_[i]] updates *= alpha if np.sum(np.abs(updates)) &lt; 1e-3: break # perform batch updating current_values += updates # calculate rms error errors.append(np.sqrt(np.sum(np.power(current_values - TRUE_VALUE, 2)) / 5.0)) total_errors += np.asarray(errors) total_errors /= runs return total_errorsdef figure_6_2(): episodes = 100 + 1 td_erros = batch_updating('TD', episodes) mc_erros = batch_updating('MC', episodes) plt.plot(td_erros, label='TD') plt.plot(mc_erros, label='MC') plt.xlabel('episodes') plt.ylabel('RMS error') plt.legend() plt.savefig('./figure_6_2.png') plt.show() figure_6_2() 100%|██████████| 100/100 [00:45&lt;00:00, 2.29it/s] 100%|██████████| 100/100 [00:37&lt;00:00, 2.49it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter06 Temporal-Difference Learning]]></title>
    <url>%2F2018%2F12%2F01%2FChapter06-Temporal-Difference-Learning%2F</url>
    <content type="text"><![CDATA[TD methodTemporal Difference method 可以说是强化学习算法的代表，它结合了Monte Carlo方法和DP方法的优点：TD方法不需要借助环境的信息(model-free)，可以直接通过模拟得到经验进行学习；同时它像DP算法一样，一个state的估计(estimate)可以通过其它部分的估计来进行更新，而不需要等待完整的episode结束。 TD PredictionTD方法和MC方法都是根据生成的经验来学习的，但是MC方法需要在episode结束后才能更新，比如对一个非平稳问题有如下更新规则的every-visit MC方法： TD methods need to wait only until the next time step. At time t+1 they immediately form a target and make a useful update using the observed rewardR_{t+1} and the estimate V(S_{t+1}). The simplest TD method makes the update, which we call it as TD(0): 接着给出TD(0)方法预测(prediction)的伪代码： 关于TD方法的理论基础，我们需要分析一下target的由来。target的概念在第二章Bandit的增量实现那一部分提到过，在强化学习中使用的更新通式： 从第三章我们推导出这样的公式： 可以看到Monte Carlo方法是使用(6.3)的近似来作为target，使用样本返回的reward来近似；DP方法使用(6.4)的近似作为target。TD方法将两者结合起来，首先它并没有使用R_{t+1}的期望，而是使用了一次抽样的reward来近似，其次没有使用基于policy π的expect value，而是使用了它当前的近似V来更新。所以可以认为TD方法结合了Monte Carlo的抽样原理，使它不必基于model，同时基于DP的原理使得TD可以快速迭代更新。 这一部分关于TD方法的预测基本完成了，但是书中还提到了一个量δ_t，这个值是update rule右边括号里的值，是V(s)更新到更好的值所使用的偏差。因为需要采样来进行更新，所以δ_t只有到t+1时刻才可以知道。 如果假设V array的值不变，或者因为step size的值α很小，认为V array的更新比较缓慢，则有下述等式近似成立： Generalizations of this identity play an important role in the theory and algorithms of temporal-difference learning. Advantages of TD Prediction Methods这部分回答了几个问题： 1、TD方法比DP好吗？比MC好吗？ TD vs DP: Obviously, TD methods have an advantage over DP methods in that they do not require a model ofthe environment, of its reward and next-state probability distributions. TD vs MC: (1)One of most obvious advantage of TD methods over Monte Carlo methods is that they are naturallyimplemented in an on-line, fully incremental fashion. (2)With Monte Carlo methods one must wait untilthe end of an episode, because only then is the return known, whereas with TD methods one need waitonly one time step. Surprisingly often this turns out to be a critical consideration. Some applicationshave very long episodes, so that delaying all learning until the end of the episode is too slow. Otherapplications are continuing tasks and have no episodes at all. (3)Finally, as we noted in the previouschapter, some Monte Carlo methods must ignore or discount episodes on which experimental actionsare taken, which can greatly slow learning. TD methods are much less susceptible to these problemsbecause they learn from each transition regardless of what subsequent actions are taken. 2、TD方法收敛吗？ 答案是肯定的。对于任意给定的policy π，TD(0)方法的结果被证明会收敛到v_π，如果使用一个小的固定step size那么收敛情况是平均的(我觉得应该是在附近很小的振荡吧)或者变化的，如果使用满足如下条件的递减step size则一定会收敛v_π。 3、TD方法和MC方法那个训练起来更快？ 这个目前还没有人能通过数学推导出两者的速度孰快孰慢，但是一般来说TD方法总是比constant-α MC方法快一些。 这部分的实验Example6.2 Random Walk的代码可以看这个。 Optimality of TD(0)这一部分主要讲了batch updating的优化方法，这个方法不同于之前的方法,V(S_t)不是在得到V(S_{t+1})后就直接更新了,而是先模拟了n个完整的episode,然后再用每个episode计算相应的target值,相当于原来增量以episode为单位的总和，再用这个总和进行更新，如果value收敛了，这个episode就算用完了，再换下一个episode继续操作。这种方法适用于训练样本比较少的情况，在相同样本数量下这种方法训练速度较慢，而且训练结果从和理论值计算得到的rms error来看并没有很大的优化。 使用batch updating方法更新的python代码其中的temporal_difference(value,batch)函数是使用TD方法更新的函数，如果batch=True则不执行数据更新，只返回trajectory和reward的两个array； monte_carlo(value,batch)函数是使用MC方法更新的函数，如果batch=True，只返回trajectory和return的两个array。 同时因为这里使用的是MRP(example6.3)，所以可以直接计算state-value(model-based)。 12345678910111213141516171819202122232425262728293031323334353637for r in tqdm(range(0, runs)): # current_value是当前对v_π的近似(estimate) current_values = np.copy(VALUES) trajectories = [] rewards = [] for ep in range(episodes): if method == 'TD': # trajectory_ 是单次episode的轨迹，包括了(1)中提到的S0,S1,S2,...,S_n # rewards在TD方法里指的是每一步action引入的reward。 trajectory_, rewards_ = temporal_difference(current_values, batch=True) else: # trajectory_ 是单次episode的轨迹，包括了(1)中提到的S0,S1,S2,...,S_n # rewards在MC方法里指的是每个state的return trajectory_, rewards_ = monte_carlo(current_values, batch=True) # 将每一个episode产生的trajectory还有rewards都推进宏观的trajectories和rewards # 这两个list随着实验进行长度迅速增长 trajectories.append(trajectory_) rewards.append(rewards_) while True: # update相当于累加版的target update = np.zeros(7) for trajectory_, rewards_ in zip(trajectories, rewards): # len(trajectory)-1表明不考虑终止状态 # 使用所有的trajectory和reward计算所以trajectory上的state的update for i in range(0, len(trajectory_) - 1): if method == 'TD': updates[trajectory_[i]] += rewards_[i] + current_values[trajectory_[i + 1]] - current_values[trajectory_[i]] else: updates[trajectory_[i]] += rewards_[i] - current_values[trajectory_[i]] updates *= alpha if np.sum(np.abs(updates)) &lt; 1e-3: break # 使用update进行一次batch update current_values += updates 现在来讨论一下Example6.4并对TD和MC性能差异给出解释首先给出8个episode： A, 0, B, 0 B, 1 B, 1 B, 1 B, 1 B, 1 B, 1 B, 0 这是一条参数未知的MRP，如何根据给出的episode来估计其中的参数？ 首先来讨论一下value(B)，因为TD(0)method 和MC method更新最后一个state的方法一致，所有最终会收敛到最大似然估计值：0.75。 最大似然估计值在《统计学习方法》里经常可以看到，抛开详细的证明，根据频率来逼近概率的思路就是最大似然估计，所以这里对v(B)的最大似然估计就是出现1的概率：0.75。 但是在v(A)的计算中，TD和MC方法出现了分歧： MC方法直接从train data中来，所有只有一次A出现，而且G=0，所以很自然v(A)=0; 但是如果用TD方法来估计，因为reward(action:A-&gt;B)=0，所以最终v(A)会和v(B)一起收敛到0.75。 比较这两个方法可以看到，MC方法在train data上计算出来的rms error无疑更小，但是我们仍然认为TD方法更好，因为TD方法在未来或者说未知数据上可能会有更好的近似性。PS:这个问题比较像欠拟合和过拟合的辨析。 从Example6.4总结得到结论batch MC方法总是找到使训练集上的均方误差(rms error)最小的estimate，而batch-TD(0)总是找到对马尔可夫过程的最大似然模型精确的估计。 通常，参数的最大似然估计是其生成数据的概率最大的参数值。在这种情况下，最大似然估计得到的是马尔可夫过程的模型，该模型以明显的方式由观测事件形成：从i到j的转移概率的估计是从i到j的观测转移的频率，并且关联的期望报酬是在这些转变中观察到的回报。给定这个模型，我们可以计算值函数的估计，如果模型是正确的，则该估计将完全正确。这被称为确定性等价性估计(certainty-equivalence estimate)，因为它等价于假定基础过程的估计是确定的，而不是近似的。总体上，批处理TD(0)收敛于确定性等价估计。 总体来说，TD方法得到的estimate是最大似然估计(maximum-likelihood)，得到的是确定性等价评估；而MC方法则太过于注重样本的结果，得到的是训练样本的无差估计。虽然这个结论是我们从batch updating得到的，但是nonbatch算法的收敛趋势和batch updating是一致的，即总的方向是粗略的朝着batch updating的方向的。从而我们证明了TD方法训练效果和收敛速度快于constant-α MC。 Sarsa: On-policy TD Control从预测算法得到控制算法或者提升算法(chapter04)，重要的是引入greedy-policy。在DP方法那一章引入的GPI方法几乎给出了控制算法的”通解”。所以可以仿照上一章给出的基于MC方法的on-policy控制算法，给出TD方法的on-policy控制算法——Sarsa: target-policy:ε-greedy or ε-soft policies. convergence properties of the Sarsa algorithm: depend on the nature of the policy’s dependence on Q, ε-greedy or ε-soft policies are OK. Sarsa converges with probability 1 to an optimal policy and action-value function as long as all state–action pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy (which can be arranged, for example, with ε-greedy policies by setting ε = 1/t). Q-learning: Off-policy TD ControlQ-learning 是off-policy的控制方法，因为在update Q的时候，使用的不是behavior policy(比如ε-greedy)，而是使用的next-state对应的maximization state-action-value。伪代码如下： Expected Sarsa如果把Sarsa的update规则更换为如下： 就是Expected sarsa算法。 Expected sarsa算法相比Sarsa方法表现更好，因为引入了期望来更新Q，一定程度上消除了因为随机选择Action带来的方差(variance)。从cliff-walk的例子中我们可以看到，state的转换都是确定的，随机性或者说不确定性都是policy引入的，因为Expected Sarsa引入了期望，所以可以设置α=1，并且不会降低渐近性能(asymptotic performance)，而Sarsa只能把α设置的低一些才能有效的运行，并且只有长期训练才有好的效果。关于渐近性能和临时性能的比较，可以参考post。 Maximization Bias and Double Learning所谓的最大正偏差，产生的原因就是在算法中使用的maximization操作。这种maximization操作无形中将估计Q值中的最大值用作了对最大q*的估计值： In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a significant positive bias. 比如，假设只有一个state，并且它有很多对q(s,a)=0。但是estimate of q(s,a)有大于0的也有小于0的。如果把估计值中的最大值用作了estimate of maximization q*(s,a)，那么就会产生一个正偏差。 那么如何解决这个问题那？ One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. 意思就是维护两个q(s,a)的估计Q1和Q2，当估计max q*的时候，可以先找到对应最大Q1的A*，然后使用Q2(A*)来估计max q*。原理就是E[Q2(A*)] = q(A*)这个期望公式是无偏的。同理也可以反过来更新Q1，二者谁更新的问题就好比抛硬币正反面一样，可以使用二项分布来决定。这种算法和Q-learning结合起来就可以得到避免最大正向偏差的控制算法：Double Q-learning. 更新规则下例，反过来同理: Example6.7代码可以参考这个post Games, Afterstates, and Other Special Cases算法根据不同的情况也需要进行一些调整，给出的方法只是一种参考格式，并不一定适应所有问题。 一个例子就是在导论中学习的tic-tac-toe问题，可以看到我们在这里学习的value function并不每一步的state-action-value，而是采取action后当前棋盘的所有棋子分布的value。因为只考虑action的话，就无法将对手的走子情况考虑进去，作为一个竞技游戏很明显是不合适的。比如下面的两种棋盘，虽然所处state和采取action都不同，但是结果是一样的，所以这种afterstate的更新方法更加有效可靠。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter05 infinite_variance]]></title>
    <url>%2F2018%2F11%2F28%2FChapter05-infinite-variance%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter05/infinite_variance.py 通过一个例子论证了ordinary importance sampling的不稳定性 问题描述这个程序通过一个简单的例子证明了ordinary importance sampling 的方差经常会发生不收敛的问题。 本例使用了一个只有一个状态s和两个状态left和right，以及一个terminate state的MDP问题，详细的Reward和转移概率如下所示： 选择的target_policy是：π(left|s)=1,π(right|s)=0； 选择生成episode的behavior policy是:b(left|s)=b(right|s)=0.5 满足π cover b的要求，并根据target_policy可以估计出v_π(s)=1，接下来看看代码运行结果，看看通过behavior policy预测出来的可以收敛到什么情况。 引入模块并定义常量，其中action_back=left，action_end=right12345678import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom tqdm import tqdm %matplotlib inlineACTION_BACK = 0ACTION_END = 1 定义behavior-policy和target-policy并开始训练1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# behavior policydef behavior_policy(): return np.random.binomial(1, 0.5)# target policydef target_policy(): return ACTION_BACK# one turn# 返回reward和action trajectory，因为state已知，所以不用指定def play(): # track the action for importance ratio trajectory = [] while True: action = behavior_policy() trajectory.append(action) if action == ACTION_END: return 0, trajectory if np.random.binomial(1, 0.9) == 0: return 1, trajectorydef figure_5_4(): runs = 10 episodes = 100000 for run in tqdm(range(runs)): # 每轮run之间都是独立的 rewards = [] for episode in range(0, episodes): reward, trajectory = play() if trajectory[-1] == ACTION_END: rho = 0 else: rho = 1.0 / pow(0.5, len(trajectory)) rewards.append(rho * reward) rewards = np.add.accumulate(rewards) estimations = np.asarray(rewards) / np.arange(1, episodes + 1) plt.plot(estimations) plt.plot(np.ones(episodes+1)) plt.xlabel('Episodes (log scale)') plt.ylabel('Ordinary Importance Sampling') plt.xscale('log') plt.ylim(0,2) plt.savefig('./figure_5_4.png') plt.show()figure_5_4() 100%|██████████| 10/10 [00:06&lt;00:00, 1.59it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter05 blackjack]]></title>
    <url>%2F2018%2F11%2F28%2FChapter05-blackjack%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter02/blackjack.py 实现了基于Monte Carlo方法的三种算法：1、基于Monte Carlo方法的策略预测，根据给出的策略计算state-value 2、使用exploring starts的训练方法，得出state-action-value，以及对应的优化policy π 3、使用off-policy的重要取样方法预测state-value，并和期望值对比 问题描述blackjack是一个常见的赌场游戏，规则如下： 牌组和牌值：总的牌堆由7到8组牌去掉大小王组成，所以不用考虑牌数量问题；其中2-10是原值，J-K当做10使用，A分两种情况，可以等于1 or 11。 规则：游戏开始，发牌的兄弟给玩家和庄家各发2张牌，玩家的牌是亮出来的明牌，庄家的是一张明牌一张暗牌，如果玩家当场数值和到21了(natural)，即拿到了一张A和一张10(或者J-K)，如果庄家没有达到21，那么判庄家输(这里不考虑赌金什么的)，反正则平局；如果玩家没有到21，可以选择继续要牌(hit)或者放弃要牌(strick or stand)，如果超过21了就叫做爆牌(go bust)，玩家就被判负；如果玩家strick1了，就轮到庄家发牌了，庄家一般会按照这样的方法来决策(也可以不这样，这里是一种规定吧)：如果牌值&lt;17，就hit，在17-21之间stand，如果庄家goes bust，庄家就被判负；如果庄家stand了，就比较双方的总牌值，大的一方为胜方。 这里使用policy：玩家hit当牌值&lt;20，20-21就stand。 monte carlo算法本身不难理解，但是这个blackjack问题可以说是目前接触到的最复杂的强化学习问题了。首先是state的理解，state=[usable_ace_player, player_sum, dealer_card1]，其中usable_ace_player指的是player是否将A用作11，player_sum指的是Player牌组总值，dealer_card1指的是庄家亮出的明牌。 算法中使用了很多blackjack游戏的游戏技巧，比如上面提到的一些策略，还有state的选取，增加了理解难度。 所以这个问题主要理解monte carlo算法工作原理，具体的技巧可以选择性忽略。 引入模块，并定义action常量，hit=继续抽，stand=停止1234567891011import numpy as npimport matplotlibimport matplotlib.pyplot as pltimport seaborn as snsfrom tqdm import tqdm%matplotlib inline# actions: hit or standACTION_HIT = 0ACTION_STAND = 1 # "strike" in the bookACTIONS = [ACTION_HIT, ACTION_STAND] 为player定义policy，属于游戏技巧，也是本例的一个参考，通过最终学到的policy和这里的POLICY_PLAYER对比来比较算法的优劣123456# policy for playerPOLICY_PLAYER = np.zeros(22)for i in range(12, 20): POLICY_PLAYER[i] = ACTION_HITPOLICY_PLAYER[20] = ACTION_STANDPOLICY_PLAYER[21] = ACTION_STAND 两个待定函数，这里的target_policy_player和behavior_policy_player的工作方式是理解off-policy Monte Carlo算法的关键1234567891011# use for off-policy method# function form of target policy of playerdef target_policy_player(usable_ace_player, player_sum, dealer_card): return POLICY_PLAYER[player_sum]# function form of behavior policy of playerdef behavior_policy_player(usable_ace_player, player_sum, dealer_card): # probality = 0.5 according to binomial distruction if np.random.binomial(1, 0.5) == 1: return ACTION_STAND return ACTION_HIT 为庄家定义policy，属于游戏技巧123456# policy for dealerPOLICY_DEALER = np.zeros(22)for i in range(12, 17): POLICY_DEALER[i] = ACTION_HITfor i in range(17, 22): POLICY_DEALER[i] = ACTION_STAND Monte Carlo方法的关键一步，通过模拟游戏来获得sample123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157# get a new card# 根据游戏规则发牌并定义牌值def get_card(): card = np.random.randint(1, 14) card = min(card, 10) return card# play a game# @policy_player: 为Player制定的policy，也是训练的目标？# @initial_state: 初始状态[whether player has a usable Ace, sum of player's cards, one card of dealer]# @initial_action: 初始行为 the initial action# return:(state,reward,player_trajectory)# 返回变量解释：# state：就是得到的初始状态，如果initial_state=None，就是随机产生的，反正其实就是initial_state的deep copy# reward：是相应的初始状态的最终结果，Player win=1，Player lost=-1，平局=0# player_trajectory：以[state,action]为元素的list，记录整个实验过程中的state和对应的actiondef play(policy_player, initial_state=None, initial_action=None): # player status # sum of player player_sum = 0 # trajectory of player player_trajectory = [] # whether player uses Ace as 11 usable_ace_player = False # dealer status dealer_card1 = 0 dealer_card2 = 0 usable_ace_dealer = False if initial_state is None: # generate a random initial state num_of_ace = 0 # initialize cards of player # 这里是一个理解的难点，主要是因为对游戏不太理解 # 游戏规定是要一开始给玩家和庄家发2张牌，这里没给初始状态，所以随机抽呗 # 但是问题是，这个循环把Player的牌值一直抽到11才跳出，也就是说其中大概率不止抽了2张，怎么回事？ # 因为反正发完牌也是让Player先搞，而且前面按照给的经验玩法，在12到20之间都是无脑hit的，所以直接在这抽到12算了。 while player_sum &lt; 12: # for _ in range(2): # if sum of player is less than 12, always hit card = get_card() # if get an Ace, use it as 11 if card == 1: num_of_ace += 1 card = 11 usable_ace_player = True player_sum += card # if player's sum is larger than 21, he must hold at least one Ace, two Aces are possible # 这里的理解也很有意思，就是如果超过21了，那么考虑前一个while循环，数值肯定是&lt;=11，而最后一次抽最大是11(A)，所以超过21 # 只有一种情况，就是前一次11，这次又抽到了A，所以肯定至少有一个A，当然两个A也是有可能的。 if player_sum &gt; 21: # use the Ace as 1 rather than 11 player_sum -= 10 # if the player only has one Ace, then he doesn't have usable Ace any more if num_of_ace == 1: usable_ace_player = False # initialize cards of dealer, suppose dealer will show the first card he gets # 这里也验证了前面的推测，因为下一次不论到庄家抽卡，所以庄家这里就老老实实抽了2张 # 不过其中card1是可见的，card2是暗牌看不到，所以card1也是会影响决策的(怎么影响就是游戏经验了) dealer_card1 = get_card() dealer_card2 = get_card() else: # use specified initial state usable_ace_player, player_sum, dealer_card1 = initial_state dealer_card2 = get_card() # initial state of the game state = [usable_ace_player, player_sum, dealer_card1] # initialize dealer's sum # 计算得到庄家的牌值总值，当然这个对玩家是不可见的。 dealer_sum = 0 if dealer_card1 == 1 and dealer_card2 != 1: dealer_sum += 11 + dealer_card2 usable_ace_dealer = True elif dealer_card1 != 1 and dealer_card2 == 1: dealer_sum += dealer_card1 + 11 usable_ace_dealer = True elif dealer_card1 == 1 and dealer_card2 == 1: dealer_sum += 1 + 11 usable_ace_dealer = True else: dealer_sum += dealer_card1 + dealer_card2 # game starts! # player's turn while True: # 第一次会对玩家的行为套用初始action，以后不会用了 if initial_action is not None: action = initial_action initial_action = None else: # get action based on current sum # 写了这么变量，其实就是根据state和给定的policy给出action action = policy_player(usable_ace_player, player_sum, dealer_card1) # track player's trajectory for importance sampling # 将state-action对append进player_trajectory player_trajectory.append([(usable_ace_player, player_sum, dealer_card1), action]) if action == ACTION_STAND: break # if hit, get new card player_sum += get_card() # player busts if player_sum &gt; 21: # if player has a usable Ace, use it as 1 to avoid busting and continue if usable_ace_player == True: player_sum -= 10 usable_ace_player = False else: # otherwise player loses return state, -1, player_trajectory # dealer's turn while True: # get action based on current sum action = POLICY_DEALER[dealer_sum] if action == ACTION_STAND: break # if hit, get a new card new_card = get_card() if new_card == 1 and dealer_sum + 11 &lt; 21: dealer_sum += 11 usable_ace_dealer = True else: dealer_sum += new_card # dealer busts if dealer_sum &gt; 21: if usable_ace_dealer == True: # if dealer has a usable Ace, use it as 1 to avoid busting and continue dealer_sum -= 10 usable_ace_dealer = False else: # otherwise dealer loses return state, 1, player_trajectory # compare the sum between player and dealer if player_sum &gt; dealer_sum: return state, 1, player_trajectory elif player_sum == dealer_sum: return state, 0, player_trajectory else: return state, -1, player_trajectory 使用on-policy策略训(yu)练(ce)算法伪代码： 其实给的target_policy才是last boss，这里只是使用Monte Carlo的方法来预测了一下，看看结果是不是符合的。 这里有一个比较有意思的地方感觉可以讨论一下： 算法的最后一部分，也就是循环的最小部分，即对value的更新，因为blackjack问题本身就是state不重复的，所以first-visit和every-visit是一样的；其次就是G，这个G是被累加了，因为这里S_t是按照索引减小的方向移动的，而且注意符号，这里累加了R_{t+1}，这个形式和上一章的MDP问题中的表达式是一致的，关于这里R的索引是n还是n+1，我觉得也有必要提一下。因为在第二章Bandit问题里，R是用的n： 而且这里对value的更新也是从前往后的，state是有限的； 而第4章DP的时候，R使用的就变成n+1了： 而且也变成反向更新了（虽然形式上仍是正向写的code，但是事实上最先确定的是最终的state-value） 仔细看的话会发现，其实这两个问题还是差别挺大的，首先说下bandit问题把。Bandit问题是通过学习找到最优的平均收益，所以n控制的试验次数，所以其实本质上它只计算了一个value值，就是最终目标是为了收敛到最优的action，即找到reward期望最大的action。 MDP问题中的t代表的是state随step出现的时刻，即使一般问题可能不满足MDP的马尔科夫性，但是对于一个普遍的多状态序列决策问题，t时刻的state对应的value肯定是其后时刻states的value的一种和的形式(通常会考虑discount如果问题是continuing task)。所以两个问题本质就是不同的，不能混淆。 再谈谈另一个问题，就是伪算法最里层计算state-value的时候，计算顺序是沿着t减小的方向的，因为这样有利于迭代，其实也是一种DP的思路。但是这个blackjack问题有它的特殊性，就是每一步action的reward是设为0的，只有最后的结果才是有reward的。这样设计是有道理的，避免达到次优点嘛。 看到这里我不禁想到，Monte Carlo方法和MDP其实原理是相通的，都利用的DP的思路来解决问题，我记得在MDP那一章也讲过这个问题，在原书46页： 123456789101112131415161718192021222324252627282930# Monte Carlo Sample with On-Policydef monte_carlo_on_policy(episodes): states_usable_ace = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided states_usable_ace_count = np.ones((10, 10)) states_no_usable_ace = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided states_no_usable_ace_count = np.ones((10, 10)) for i in tqdm(range(0, episodes)): _, reward, player_trajectory = play(target_policy_player) for (usable_ace, player_sum, dealer_card), _ in player_trajectory: # 这里也是一个理解的难点，一点一点分析： # 1、“player_sum-=12”这个操作，通过play函数我发现，append进player_trajectory # 中的state的player_sum一定是&lt;=21的，即state中的action是在player_sum基础上的，player_sum # 的值是在action之前的值。那么可以知道player_sum-12&lt;9 # 接着分析，-12的操作结果是否会向下溢出，根据play函数中的处理，player_sum一定是从&gt;=12的值开始的， # 这一点可以从play函数一开始的初始状态的处理看出来，while循环只有当player_sum&gt;=12才会跳出 # 所以player_sum-12 ~ [0,9]，刚好符合states_usable_ace等4个np.ndarray的范围 # 同理，dealer_card-1也是一样的道理，将[1,10]的范围换算到[0,9] # 那dealer_card就不可能是11吗？根据play函数的分析，dealer_card在计算dealer_sum的时候 # 需要根据另一张暗牌的牌值来计算，所以对于player来说只能认为它是1（暗牌不可见） player_sum -= 12 dealer_card -= 1 if usable_ace: states_usable_ace_count[player_sum, dealer_card] += 1 states_usable_ace[player_sum, dealer_card] += reward else: states_no_usable_ace_count[player_sum, dealer_card] += 1 states_no_usable_ace[player_sum, dealer_card] += reward return states_usable_ace / states_usable_ace_count, states_no_usable_ace / states_no_usable_ace_count 绘制图表1234567891011121314151617181920212223242526272829def figure_5_1(): states_usable_ace_1, states_no_usable_ace_1 = monte_carlo_on_policy(10000) states_usable_ace_2, states_no_usable_ace_2 = monte_carlo_on_policy(500000) states = [states_usable_ace_1, states_usable_ace_2, states_no_usable_ace_1, states_no_usable_ace_2] titles = ['Usable Ace, 10000 Episodes', 'Usable Ace, 500000 Episodes', 'No Usable Ace, 10000 Episodes', 'No Usable Ace, 500000 Episodes'] _, axes = plt.subplots(2, 2, figsize=(40, 30)) plt.subplots_adjust(wspace=0.1, hspace=0.2) axes = axes.flatten() for state, title, axis in zip(states, titles, axes): fig = sns.heatmap(np.flipud(state), cmap="YlGnBu", ax=axis, xticklabels=range(1, 11), yticklabels=list(reversed(range(12, 22)))) fig.set_ylabel('player sum', fontsize=30) fig.set_xlabel('dealer showing', fontsize=30) fig.set_title(title, fontsize=30) plt.savefig('./figure_5_1.png') plt.show() figure_5_1() 100%|██████████| 10000/10000 [00:00&lt;00:00, 51795.15it/s] 100%|██████████| 500000/500000 [00:08&lt;00:00, 55705.87it/s] 使用exploring-start训练算法的伪代码如下： 注意其中几个要点： 在Monte Carlo模拟的时候，即play函数里，使用的player_policy是greey的； 初始状态是explore的，并保证每个初始state和action组合都有概率出现 1234567891011121314151617181920212223242526272829303132333435363738394041# Monte Carlo with Exploring Startsdef monte_carlo_es(episodes): # (playerSum, dealerCard, usableAce, action) state_action_values = np.zeros((10, 10, 2, 2)) # initialze counts to 1 to avoid division by 0 state_action_pair_count = np.ones((10, 10, 2, 2)) # behavior policy is greedy def behavior_policy(usable_ace, player_sum, dealer_card): usable_ace = int(usable_ace) player_sum -= 12 dealer_card -= 1 # get argmax of the average returns(s, a) values_ = state_action_values[player_sum, dealer_card, usable_ace, :] / \ state_action_pair_count[player_sum, dealer_card, usable_ace, :] # np.random.choice 函数通过给定一个list或者np.ndarray或者整数n，以及一个表示选择概率的list来从一系列数中 # 按照概率选出相应的值 # 如果第一个参数是n，其实是相当于np.arange(n)的，第二个概率如果不指定就默认uniform distribution(均匀分布) # 这里是从最大的value的action中选出任一个的 return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) # play for several episodes for episode in tqdm(range(episodes)): # for each episode, use a randomly initialized state and action # 使用exploring start-action对 initial_state = [bool(np.random.choice([0, 1])), np.random.choice(range(12, 22)), np.random.choice(range(1, 11))] initial_action = np.random.choice(ACTIONS) # 这个地方挺有意思的，第一次先使用target_policy，免得state_action_value一直是0，陷入死循环 current_policy = behavior_policy if episode else target_policy_player _, reward, trajectory = play(current_policy, initial_state, initial_action) for (usable_ace, player_sum, dealer_card), action in trajectory: usable_ace = int(usable_ace) player_sum -= 12 dealer_card -= 1 # update values of state-action pairs state_action_values[player_sum, dealer_card, usable_ace, action] += reward state_action_pair_count[player_sum, dealer_card, usable_ace, action] += 1 return state_action_values / state_action_pair_count 绘制图表1234567891011121314151617181920212223242526272829303132333435def figure_5_2(): state_action_values = monte_carlo_es(500000) state_value_no_usable_ace = np.max(state_action_values[:, :, 0, :], axis=-1) state_value_usable_ace = np.max(state_action_values[:, :, 1, :], axis=-1) # get the optimal policy action_no_usable_ace = np.argmax(state_action_values[:, :, 0, :], axis=-1) action_usable_ace = np.argmax(state_action_values[:, :, 1, :], axis=-1) images = [action_usable_ace, state_value_usable_ace, action_no_usable_ace, state_value_no_usable_ace] titles = ['Optimal policy with usable Ace', 'Optimal value with usable Ace', 'Optimal policy without usable Ace', 'Optimal value without usable Ace'] _, axes = plt.subplots(2, 2, figsize=(40, 30)) plt.subplots_adjust(wspace=0.1, hspace=0.2) axes = axes.flatten() for image, title, axis in zip(images, titles, axes): fig = sns.heatmap(np.flipud(image), cmap="YlGnBu", ax=axis, xticklabels=range(1, 11), yticklabels=list(reversed(range(12, 22)))) fig.set_ylabel('player sum', fontsize=30) fig.set_xlabel('dealer showing', fontsize=30) fig.set_title(title, fontsize=30) plt.savefig('./figure_5_2.png') plt.show() figure_5_2() 100%|██████████| 500000/500000 [00:31&lt;00:00, 15708.47it/s] 使用off-policy策略训练(预测)通过计算Ordinary Importance Sampling 和 Weighted Importance Sampling并和target_policy训练的结果比对，分别计算均方误差来测试算法的性能。 重点是两个重要取样的意义和计算： 重要取样(Importance Sampling)的推导是通过MDP得到的，它是一种off-policy方法中通过behavior policy来训练target policy的重要途径，其中一个重要的概念就是重要取样比率(importance-sampling ratio)，它的定义式是target policy下的状态轨迹的π(a|s)和behavior policy下的状态轨迹的b(a|s)的比值。具体推导如下： 首先定义通过behavior policy取样得到的states-chain的联合概率，这里的进一步计算使用了马尔科夫独立性假设： 然后给出重要取样比率的定义： 其中的状态转移概率p被消去了，可以证明这个结论虽然是通过马尔科夫独立性推出的，却具有一般性。 好了，现在我们基本理解了重要采样比率的概念，那么它到底有什么用那？下面的公式解释了这个问题（具体推导不清楚QAQ）： 这个值是联系episode的return和target policy π下的v_π(s)的关键！ 所以很直观的从这个期望公式引出ordinary importance sampling： 但是有一个问题，就是如果重要采样因子有可能是方差无限的，这时我们近似得到的state-value就会发散而无法收敛，这个问题很严重，随后也会通过代码来解释。 所以通过对采样因子做归一化，引出了Weighted importance sampling的概念： 和前者相比，虽然weighted importance sampling是有偏的(bias)，因为它不是直接从上面的期望公式来的，但是经过多次迭代bias会趋于0。但是如果采样因子的方差不是有限的，ordinary importance sampling就很有可能无法收敛，也就是说它的方差(variance)是高于weighted importance sampling的，所以总的来说后者更常用。 12345678910111213141516171819202122232425262728293031323334353637# Monte Carlo Sample with Off-Policydef monte_carlo_off_policy(episodes): # 不用exploring start了，所以可以直接指定起始状态 initial_state = [True, 13, 2] rhos = [] returns = [] for i in range(0, episodes): _, reward, player_trajectory = play(behavior_policy_player, initial_state=initial_state) # get the importance ratio numerator = 1.0 denominator = 1.0 for (usable_ace, player_sum, dealer_card), action in player_trajectory: if action == target_policy_player(usable_ace, player_sum, dealer_card): denominator *= 0.5 else: numerator = 0.0 break rho = numerator / denominator rhos.append(rho) returns.append(reward) rhos = np.asarray(rhos) returns = np.asarray(returns) weighted_returns = rhos * returns weighted_returns = np.add.accumulate(weighted_returns) rhos = np.add.accumulate(rhos) ordinary_sampling = weighted_returns / np.arange(1, episodes + 1) with np.errstate(divide='ignore',invalid='ignore'): weighted_sampling = np.where(rhos != 0, weighted_returns / rhos, 0) return ordinary_sampling, weighted_sampling 绘制图表12345678910111213141516171819202122232425def figure_5_3(): true_value = -0.27726 episodes = 10000 runs = 100 error_ordinary = np.zeros(episodes) error_weighted = np.zeros(episodes) for i in tqdm(range(0, runs)): ordinary_sampling_, weighted_sampling_ = monte_carlo_off_policy(episodes) # get the squared error error_ordinary += np.power(ordinary_sampling_ - true_value, 2) error_weighted += np.power(weighted_sampling_ - true_value, 2) error_ordinary /= runs error_weighted /= runs plt.plot(error_ordinary, label='Ordinary Importance Sampling') plt.plot(error_weighted, label='Weighted Importance Sampling') plt.xlabel('Episodes (log scale)') plt.ylabel('Mean square error') plt.xscale('log') plt.legend() plt.savefig('./figure_5_3.png') plt.show() figure_5_3() 100%|██████████| 100/100 [00:20&lt;00:00, 4.87it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter05 Monte Carlo Methods]]></title>
    <url>%2F2018%2F11%2F28%2FChapter05-Monte-Carlo-Methods%2F</url>
    <content type="text"><![CDATA[Monte Carlo MethodUnlike the previous chapter, here we do not assume complete knowledge of the environment. Monte Carlo methods require only experience—sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Learning from actual experience is striking because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior. Monte Carlo Prediction蒙特卡洛预测是利用蒙特卡洛方法的最简单的强化学习算法，通过给定的policy进行模拟，并根据模拟episodes来进行state-value期望计算。 这里要注意的是，预测分为first-visit和every-visit，前者是本章讲述的重点，后面其他的算法也都是first-visit的，它的意思是指模拟过程中只对一次状态s计算return，换句话说episode中只能存在一个s，如果存在多个s的episode这种方法就会直接舍弃； 后者会在后面章节讲述，every-visit会对episode中所有s的return进行计算。 first-visit的Monte Carlo预测伪代码： Monte Carlo Estimation of Action Values看了网上的一些解释，如果是model-free的强化学习问题，都是要学习Q(s,a)的，只有model-based的方法可以直接学习v(s)。 其实感觉model-free是一种更普适的方法，因为一个一般的决策问题如果知道行为带来的value的话决策起来就会很舒服，直接greedy action嘛。 但是如果我对环境有一些了解，比如像MDP中那样使用model-based的学习方法，知道状态间的转移概率p(s,a)，那么我在训练的过程中不同step对应的state可以通过动态规划联系在一起，即state和following states之间并没有通过action来连接。那么我在学习的时候就不用去考虑不同的action导致的Q(s,a)，具体的算法可以参考前面MDP的post。那我自然只通过建立v(s)就够了。 这里关于如何建立action-value有一点需要注意：在MDP问题中，每个state都会被更新，所以是不需要maintain exploring的；但是对于action来说，如果采用完全greedy的policy的话，所有的action不一定都会选中，这个问题和第2章的Bandit问题一样，所以需要在greedy policy之外保持一种exploring的策略。于是这里便引入了exploring start的方法，即每次episode的初始state-action会保证所有的state-action对都有概率被选中。 Monte Carlo Control蒙特卡洛控制通过上一章提到的GPI(generalized policy iteration)的思路来解决的。就是控制环保有一个近似的value function和一个近似的policy，the value function is repeatedly altered(改变) to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to thecurrent value function.GPI的流程可以简单理解为下图： 这个地方和上一章DP算法提到的policy Iteration和value Iteration原理是一致的就不多赘述了。下面看一下基于ES(exploring start)的Control方法： Monte Carlo Control without Exploring StartExploring start的假设在实际问题中不是很常见，所以这一部分提出了两个without exploring start的控制方法：on-policy和off-policy的。 首先需要了解on和off-policy之间的区别和联系： off-policy是通过behavior policy来模拟并产生数据(episode)，但是学习得到的是target policy π，即使用的episode is “off” the target policy π； on-policy则在模拟数据和学习时都使用同一个policy。一般来说on-policy更简单易用，off-policy则会有更多的参数和更复杂的学习过程，收敛起来也比较慢，但是off-policy可以适的范围更广，可以更好的解决问题。而且如果off-policy的behavior policy = target policy，二者就一致了。 这一部分主要讨论了on-policy的控制方法，先看一下算法的伪代码： 算法提升原理如下所示，这里的q(s,π’(s))指的是在π’策略下的s的state-value。π’是上面伪代码中的ε-greedy策略。 Off-policy Prediction via Importance Samplingbehavior policy vs target policy关于target policy π cover behavior policy b的概念，这里什么才能被称为cover(覆盖)那？ 文中解释是这样的：通过π选取的action一定有概率通过b选取，即通过π(a|s)&gt;0一定可以推出b(a|s)&gt;0，前者是后者的充分条件。 还有就是这里提一下off-policy和on-policy之间的关系： what is importance sampling重要取样(Importance Sampling)的推导是通过MDP得到的，它是一种off-policy方法中通过behavior policy来训练target policy的重要途径，其中一个重要的概念就是重要取样比率(importance-sampling ratio)，它的定义式是target policy下的状态轨迹的π(a|s)和behavior policy下的状态轨迹的b(a|s)的比值。具体推导如下： 首先定义通过behavior policy取样得到的states-chain的联合概率，这里的进一步计算使用了马尔科夫独立性假设： 然后给出重要取样比率的定义： 其中的状态转移概率p被消去了，可以证明这个结论虽然是通过马尔科夫独立性推出的，却具有一般性。 好了，现在我们基本理解了重要采样比率的概念，那么它到底有什么用那？下面的公式解释了这个问题（具体推导不清楚QAQ）： 这个值是联系episode的return和target policy π下的v_π(s)的关键！ 所以很直观的从这个期望公式引出ordinary importance sampling： 但是有一个问题，就是如果重要采样因子有可能是方差无限的，这时我们近似得到的state-value就会发散而无法收敛，这个问题很严重，随后也会通过代码来解释。 所以通过对采样因子做归一化，引出了Weighted importance sampling的概念： 和前者相比，虽然weighted importance sampling是有偏的(bias)，因为它不是直接从上面的期望公式来的，但是经过多次迭代bias会趋于0。但是如果采样因子的方差不是有限的，ordinary importance sampling就很有可能无法收敛，也就是说它的方差(variance)是高于weighted importance sampling的，所以总的来说后者更常用。 Incremental Implementation for Off-policy Prediction增量算法实现和第2章的增量实现原理类似，先给出简单的证明： 设V_n是模拟过程中第n对(s,a)，G1，G2,…Gn是behavior policy产生的对应第n对(s,a)的return，则有下述表达式成立: 额感觉翻译的不够好，粘一下原文吧： Suppose we have a sequence of returns G1 , G2 , . . . , Gn−1 , all starting in the same state and each with a corresponding random weight Wi (e.g., Wi = ρ_{t:T(t)−1} ). We wish to form the estimate: 增量形式表达式: 其中： 紧接着给出预测的增量形式伪代码： Off-policy Monte Carlo Control这一章脉络很清晰啊，基本是先阐述预测算法，再讨论控制算法。这种方法在本书中很常见，因为policy的improve，还是control算法都是建立在predict(or evalution)的基础上的，再加上greey的policy提升方法得到的。下面给出off-policy的增量控制算法： 针对这个算法有几点想讨论的： 1、b是soft-policy的，soft是指b在生成模拟过程的时候对所有的状态动作对的发生概率都不为0，即可以尽可能多的产生不同的过程，相当于一种exploring吧。原文解释是： policy π is soft, meaning that π(a|s)&gt;0 for all s and a. ε-soft policy is defined as policies for which π(a|s) ≥ ε/|A(s)| for all states and actions, for some ε &gt; 0 2、如果使用π(St)得到的action和episode中的At不同，那么重要采样因子的分子是为0的，此时不对W进行更新。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter04 gamblers_problem]]></title>
    <url>%2F2018%2F11%2F23%2FChapter04-gamblers-problem%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter04/gamblers_problem.py 赌徒问题问题描述一个赌徒可以在每轮赌博中决定将自己手里的钱拿来赌硬币的正反，如果硬币向上，则可以获得押金一样的奖励，但是向下的话押金就没了。结束条件是赌徒手里的钱增长到100，或者把钱输光。 这个问题可以定义为state为赌徒手里的钱，action为每次拿去赌的钱，discount=1的MDP问题。 引入模块并定义全局变量12345678910111213import numpy as npimport matplotlibimport matplotlib.pyplot as plt%matplotlib inline# goalGOAL = 100# all states, including state 0 and state 100STATES = np.arange(GOAL + 1)# probability of head, which is the probability win moneyHEAD_PROB = 0.4 Value Iteration需要注意几点： 初始化value-state的时候，除了100的状态为1，其余都为0，可以理解为除了到达100可以获得reward=1，其余action对应reward=0，即利用value-state initialize来实现reward。 训练的时候把action=0去掉，是因为aciton=0会导致agent陷入局部最优，所以需要跳出这个点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def figure_4_3(): # state value initialize state_value = np.zeros(GOAL + 1) state_value[GOAL] = 1.0 # value iteration while True: delta = 0.0 for state in STATES[1:GOAL]: # get possilbe actions for current state actions = np.arange(1,min(state, GOAL - state) + 1) action_returns = [] for action in actions: action_returns.append( HEAD_PROB * state_value[state + action] + (1 - HEAD_PROB) * state_value[state - action]) new_value = np.max(action_returns) delta += np.abs(state_value[state] - new_value) # update state value state_value[state] = new_value if delta &lt; 1e-9: break # compute the optimal policy policy = np.zeros(GOAL + 1) for state in STATES[1:GOAL]: actions = np.arange(1,min(state, GOAL - state) + 1) action_returns = [] for action in actions: action_returns.append( HEAD_PROB * state_value[state + action] + (1 - HEAD_PROB) * state_value[state - action]) # round to resemble the figure in the book, see # https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/issues/83# policy[state] = actions[np.argmax(np.round(action_returns[1:], 5)) + 1] policy[state] = actions[np.argmax(np.round(action_returns,5))] plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) plt.plot(state_value) plt.xlabel('Capital') plt.ylabel('Value estimates') plt.subplot(2, 1, 2) plt.scatter(STATES, policy) plt.xlabel('Capital') plt.ylabel('Final policy (stake)') plt.savefig('./figure_4_3.png') plt.show()figure_4_3() 中奖率=0.4 中奖率=0.1 中奖率=0.8(所以说中奖率太高也不能浪吗。。。)]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter04 car_rental]]></title>
    <url>%2F2018%2F11%2F23%2FChapter04-car-rental%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter04/car_rental.py car rental 问题问题描述这个问题就比较复杂了。。。说是Jack是两个汽车租赁公司的老板，他收入是靠租车出去，租出一辆赚10刀，每次有人还车，那么第二天这车就可以租出去了；每天夜里可以将一个地方的车运到另一个地方，不过每运一辆车要花2刀。关于业务，Jack发现了一些斯巴拉西的规律：每个地方每天汽车租借和归还的数量都遵循泊松分布： 我们就把两个位置称为first和second吧。 first的汽车每天借出去λ=3，归还λ=3； second每天借出去λ=4，归还λ=2； 并且每个地方汽车库存不能超过20辆，超过了总公司就会回收； 夜里从一个地方运到另一个地方的汽车数量不能超过5辆。 这个问题我们把它设计成discount因子=0.9的MDP问题，step是每一天，action是每晚运的车，并设从first运到second为正，从second运到first为负，state是first和second可以租赁车的总数量。同时做一个简化，就是如果泊松分布n&gt;10，就把概率人为截断为0。 引入模块并定义常量123456789101112131415161718192021222324252627282930313233343536373839import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom math import exp, factorialimport seaborn as sns%matplotlib inline# maximum # of cars in each locationMAX_CARS = 20# maximum # of cars to move during nightMAX_MOVE_OF_CARS = 5# expectation for rental requests in first locationRENTAL_REQUEST_FIRST_LOC = 3# expectation for rental requests in second locationRENTAL_REQUEST_SECOND_LOC = 4# expectation for # of cars returned in first locationRETURNS_FIRST_LOC = 3# expectation for # of cars returned in second locationRETURNS_SECOND_LOC = 2DISCOUNT = 0.9# credit earned by a carRENTAL_CREDIT = 10# cost of moving a carMOVE_CAR_COST = 2# all possible actionsactions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)# An up bound for poisson distribution# If n is greater than this value, then the probability of getting n is truncated to 0POISSON_UPPER_BOUND = 11 进行policy evaluation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Probability for poisson distribution# @lam: lambda should be less than 10 for this functionpoisson_cache = dict()def poisson(n, lam): global poisson_cache key = n * 10 + lam if key not in poisson_cache.keys(): poisson_cache[key] = exp(-lam) * pow(lam, n) / factorial(n) return poisson_cache[key]# @state: [# of cars in first location, # of cars in second location]# @action: positive if moving cars from first location to second location,# negative if moving cars from second location to first location# @stateValue: state value matrix# @constant_returned_cars: if set True, model is simplified such that# the # of cars returned in daytime becomes constant# rather than a random value from poisson distribution, which will reduce calculation time# and leave the optimal policy/value state matrix almost the samedef expected_return(state, action, state_value, constant_returned_cars): # initailize total return returns = 0.0 # cost for moving cars returns -= MOVE_CAR_COST * abs(action) # go through all possible rental requests for rental_request_first_loc in range(0, POISSON_UPPER_BOUND): for rental_request_second_loc in range(0, POISSON_UPPER_BOUND): # moving cars num_of_cars_first_loc = int(min(state[0] - action, MAX_CARS)) num_of_cars_second_loc = int(min(state[1] + action, MAX_CARS)) # valid rental requests should be less than actual # of cars real_rental_first_loc = min(num_of_cars_first_loc, rental_request_first_loc) real_rental_second_loc = min(num_of_cars_second_loc, rental_request_second_loc) # get credits for renting reward = (real_rental_first_loc + real_rental_second_loc) * RENTAL_CREDIT num_of_cars_first_loc -= real_rental_first_loc num_of_cars_second_loc -= real_rental_second_loc # probability for current combination of rental requests # possion(n,lam) # P(AB) = P(A)*P(B) prob = poisson(rental_request_first_loc, RENTAL_REQUEST_FIRST_LOC) * \ poisson(rental_request_second_loc, RENTAL_REQUEST_SECOND_LOC) if constant_returned_cars: # get returned cars, those cars can be used for renting tomorrow returned_cars_first_loc = RETURNS_FIRST_LOC returned_cars_second_loc = RETURNS_SECOND_LOC num_of_cars_first_loc = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS) num_of_cars_second_loc = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS) returns += prob * (reward + DISCOUNT * state_value[num_of_cars_first_loc, num_of_cars_second_loc]) else: for returned_cars_first_loc in range(0, POISSON_UPPER_BOUND): for returned_cars_second_loc in range(0, POISSON_UPPER_BOUND): num_of_cars_first_loc_ = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS) num_of_cars_second_loc_ = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS) prob_ = poisson(returned_cars_first_loc, RETURNS_FIRST_LOC) * \ poisson(returned_cars_second_loc, RETURNS_SECOND_LOC) * prob returns += prob_ * (reward + DISCOUNT * state_value[num_of_cars_first_loc_, num_of_cars_second_loc_]) return returns 进行policy iteration123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def figure_4_2(constant_returned_cars=True): value = np.zeros((MAX_CARS + 1, MAX_CARS + 1)) policy = np.zeros(value.shape, dtype=np.int) iterations = 0 _, axes = plt.subplots(2, 3, figsize=(40, 20)) plt.subplots_adjust(wspace=0.1, hspace=0.2) axes = axes.flatten() while True: fig = sns.heatmap(np.flipud(policy), cmap="YlGnBu", ax=axes[iterations]) fig.set_ylabel('# cars at first location', fontsize=30) fig.set_yticks(list(reversed(range(MAX_CARS + 1)))) fig.set_xlabel('# cars at second location', fontsize=30) fig.set_title('policy %d' % (iterations), fontsize=30) # policy evaluation (in-place) while True: new_value = np.copy(value) for i in range(MAX_CARS + 1): for j in range(MAX_CARS + 1): new_value[i, j] = expected_return([i, j], policy[i, j], new_value, constant_returned_cars) value_change = np.abs((new_value - value)).sum() print('value change %f' % (value_change)) value = new_value if value_change &lt; 1e-4: break # policy improvement new_policy = np.copy(policy) for i in range(MAX_CARS + 1): for j in range(MAX_CARS + 1): action_returns = [] for action in actions: if (action &gt;= 0 and i &gt;= action) or (action &lt; 0 and j &gt;= abs(action)): action_returns.append(expected_return([i, j], action, value, constant_returned_cars)) else: action_returns.append(-float('inf')) new_policy[i, j] = actions[np.argmax(action_returns)] policy_change = (new_policy != policy).sum() print('policy changed in %d states' % (policy_change)) policy = new_policy if policy_change == 0: fig = sns.heatmap(np.flipud(value), cmap="YlGnBu", ax=axes[-1]) fig.set_ylabel('# cars at first location', fontsize=30) fig.set_yticks(list(reversed(range(MAX_CARS + 1)))) fig.set_xlabel('# cars at second location', fontsize=30) fig.set_title('optimal value', fontsize=30) break iterations += 1 plt.savefig('./figure_4_2.png') plt.show() # figure_4_2() 注意图片，总共经过4次迭代最终收敛，前5个图片是policy的温度分布图，最后一个是value的分布图。还有一点需要注意，就是这个任务没有终止状态，所以是属于continuing task，discount因子&lt;1，而本章第一个和第三个都是有终止状态的，所以是undiscount的。]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter04 gird-world using DP]]></title>
    <url>%2F2018%2F11%2F23%2FChapter04-gird-world-using-DP%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter04/gird_world.py grid_world(policy evaluation)问题描述4X4 网格： 左上角和右下角是终止状态(terminal state)，如果action使得state跳转到外面了，就返回上次位置，所有的action造成的reward都是-1。 引入模块并定义常量1234567891011121314import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom matplotlib.table import Table%matplotlib inlineWORLD_SIZE = 4# left, up, right, downACTIONS = [np.array([0, -1]), np.array([-1, 0]), np.array([0, 1]), np.array([1, 0])]ACTION_PROB = 0.25 action控制代码12345678910111213141516# judge whether comes to terminal statedef is_terminal(state): x, y = state return (x == 0 and y == 0) or (x == WORLD_SIZE - 1 and y == WORLD_SIZE - 1)# return the next_state s' and reward rdef step(state, action): state = np.array(state) next_state = (state + action).tolist() x, y = next_state if x &lt; 0 or x &gt;= WORLD_SIZE or y &lt; 0 or y &gt;= WORLD_SIZE: next_state = state.tolist() reward = -1 return next_state, reward 绘制方格图123456789101112131415161718192021222324252627def draw_image(image): fig, ax = plt.subplots() ax.set_axis_off() tb = Table(ax, bbox=[0, 0, 1, 1]) nrows, ncols = image.shape width, height = 1.0 / ncols, 1.0 / nrows # Add cells for (i,j), val in np.ndenumerate(image): # Index either the first or second item of bkg_colors based on # a checker board pattern idx = [j % 2, (j + 1) % 2][i % 2] color = 'white' tb.add_cell(i, j, width, height, text=val, loc='center', facecolor=color) # Row Labels... for i, label in enumerate(range(len(image))): tb.add_cell(i, -1, width, height, text=label+1, loc='right', edgecolor='none', facecolor='none') # Column Labels... for j, label in enumerate(range(len(image))): tb.add_cell(-1, j, width, height/2, text=label+1, loc='center', edgecolor='none', facecolor='none') ax.add_table(tb) 进行policy evaluation，即计算value-state123456789101112131415161718192021222324def compute_state_value(in_place=False): new_state_values = np.zeros((WORLD_SIZE, WORLD_SIZE)) state_values = new_state_values.copy() iteration = 1 while True: # in place algorithm is faster than 2-array edition src = new_state_values if in_place else state_values for i in range(WORLD_SIZE): for j in range(WORLD_SIZE): if is_terminal([i, j]): continue value = 0 for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) value += ACTION_PROB * (reward + src[next_i, next_j]) new_state_values[i, j] = value if np.sum(np.abs(new_state_values - state_values)) &lt; 1e-4: state_values = new_state_values.copy() break state_values = new_state_values.copy() iteration += 1 return state_values, iteration 运行并显示评估结果123456789101112def figure_4_1(): values, sync_iteration = compute_state_value(in_place=False) _, asycn_iteration = compute_state_value(in_place=True) # show the sync DP evaluation draw_image(np.round(values, decimals=2)) print('In-place: %d iterations' % (asycn_iteration)) print('Synchronous: %d iterations' % (sync_iteration)) plt.savefig('./figure_4_1.png') plt.show()figure_4_1() In-place: 142 iterations Synchronous: 218 iterations]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter04 Dynamic Programming]]></title>
    <url>%2F2018%2F11%2F23%2FChapter04-Dynamic-Programming%2F</url>
    <content type="text"><![CDATA[上一章讲了马尔科夫决策过程的概念，末尾提出了针对MDP的value-state、value-action-state建立方法，很明显可以看出是使用了动态规划的方法，这一章就在上一章基础上，进一步讲述如何使用动态规划来训练以及优化MDP问题的强化学习算法。 4.1 Policy Evaluation首先理解什么是策略的评估策略的评估也可以认为是一种预测行为，是解决MDP问题的必要环节。通过评估，我们使用已有的policy选择action（大部分问题policy是一个随机函数，即按照一定的概率分布产生action），使用动态规划的方法更新整个state-set的value并达到收敛。评估结果即value-table，是我们决策的重要依据，所以可以为未知的player如何行动提供预测的参考。 评估模型经常使用迭代运算的方式，迭代同时分为原地迭代和旧值迭代，区别仅是在动态规划使用未来状态来更新当前value的时候，是否使用更新后的值，运算通式是： 评估算法的伪代码： 4.2 Policy Improvement一开始看的时候，一直没明白上一步评估的作用何在，在策略提升这部分就讲了，策略评估是为了改进用的，因为如果在某个state s上，我改变了action a-&gt;a’，那么可以认为我采用了一种新的策略π’，即这一步我采取的行为是π’(s)，如果有： 则可以推出新的策略π’一定不差于原来的π，即： 推导过程： 顺势我们推出一种greedy的提升方法，就是直接获得最大reward的action： 这种方法也为下一部分的Policy迭代优化提供一种优化的思路。 4.3 Policy Iteration直接给出迭代优化的伪代码： 可以看出来算法其实就是评估和提升交替进行的，迭代终止条件就是没办法对当前的v_π(s)进一步优化了。 4.4 Value Iteration上面的策略迭代优化的方法很明显有个问题，就是每次提升的前提是需要对策略进行评估，value Iteration提出一种将提升和评估放在一起进行的方法，这个思路比较像4.1中评估的时候使用的in-place方法。 算法伪代码如下： 算法结束条件就是当value-table更新幅度小于阈值Θ时停止。 4.5 4.6 4.7因为后面三部分都很短，也没有给出具体的解释，所以我就放一块写。 4.5讲的是异步动态规划，其实前面讲的value iteration就是异步动态规划的一种，主要是希望可以改进DP算法会遍历整个state set的问题，有的value state没必要多次更新，而有的可以多次更新，具体的算法会在第8章提出。 4.6讲的是统一的策略迭代(GPI)，意指evaluation和improvement是相互竞争合作的，大部分强化学习都是这两者相互作用达到最优的policy和state-value。 4.7讲的是动态规划算法的效率，大致意思就是动规对于large-state的问题计算量仍旧很大，但是可以用异步动规来解决，policy iteration和value iteration现在仍很常用。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter03 gird-world]]></title>
    <url>%2F2018%2F11%2F22%2FChapter03-gird-world%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter03/gird_world.py 使用MDP的强化学习算法解决Grid-world问题任务解释(example 3.5 in chapter 03)grid_world代表了一个网格，网格中每个小格子代表一种状态。其中每个状态可以有4种action:left、right、up、down。对应reward规则如下： 如果action导致agent跑到网格外面去，则reward=-1； 如果agent从A出发，则reward=10，下个状态是固定的A_PRIME_POS；从B出发，reward=5，下个状态时固定的B_PRIME_POS； 其他的state上的action均为0。 示意图如下： 引入模块，定义常量12345678910111213141516171819import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom matplotlib.table import Table%matplotlib inlineWORLD_SIZE = 5A_POS = [0, 1]A_PRIME_POS = [4, 1]B_POS = [0, 3]B_PRIME_POS = [2, 3]DISCOUNT = 0.9# left, up, right, downACTIONS = [np.array([0, -1]), np.array([-1, 0]), np.array([0, 1]), np.array([1, 0])]ACTION_PROB = 0.25 获取next_state（base on action），和对应的reward123456789101112131415def step(state, action): if state == A_POS: return A_PRIME_POS, 10 if state == B_POS: return B_PRIME_POS, 5 state = np.array(state) next_state = (state + action).tolist() x, y = next_state if x &lt; 0 or x &gt;= WORLD_SIZE or y &lt; 0 or y &gt;= WORLD_SIZE: reward = -1.0 next_state = state else: reward = 0 return next_state, reward 根据给定的np.array数据类型绘制方格图1234567891011121314151617181920212223242526272829303132def draw_image(image): fig, ax = plt.subplots() ax.set_axis_off() tb = Table(ax, bbox=[0, 0, 1, 1]) nrows, ncols = image.shape width, height = 1.0 / ncols, 1.0 / nrows # Add cells # np.ndenumerate() return an iterator yielding pairs of array coordinates and values. for (i,j), val in np.ndenumerate(image): # Index either the first or second item of bkg_colors based on # a checker board pattern idx = [j % 2, (j + 1) % 2][i % 2] color = 'white' tb.add_cell(i, j, width, height, text=val, loc='center', facecolor=color) # Row Labels... for i, label in enumerate(range(len(image))): tb.add_cell(i, -1, width, height, text=label+1, loc='right', edgecolor='none', facecolor='none') # Column Labels... for j, label in enumerate(range(len(image))): tb.add_cell(-1, j, width, height/2, text=label+1, loc='center', edgecolor='none', facecolor='none') ax.add_table(tb) # test draw_imagetest_image = np.array([[1.2,2.1],[3.5,4.7]])draw_image(test_image) Policies and Value Functions123456789101112131415161718192021def figure_3_2(): value = np.zeros((WORLD_SIZE, WORLD_SIZE)) while True: # keep iteration until convergence new_value = np.zeros(value.shape) for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) # bellman equation # each action is random chosen new_value[i, j] += ACTION_PROB * (reward + DISCOUNT * value[next_i, next_j]) if np.sum(np.abs(value - new_value)) &lt; 1e-4: draw_image(np.round(new_value, decimals=2)) plt.savefig('./figure_3_2.png') plt.show() plt.close() break value = new_valuefigure_3_2() Optimal Policies and Value Functions(Greedy choose action)123456789101112131415161718192021def figure_3_5(): value = np.zeros((WORLD_SIZE, WORLD_SIZE)) while True: # keep iteration until convergence new_value = np.zeros(value.shape) for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): values = [] for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) # value iteration values.append(reward + DISCOUNT * value[next_i, next_j]) new_value[i, j] = np.max(values) if np.sum(np.abs(new_value - value)) &lt; 1e-4: draw_image(np.round(new_value, decimals=2)) plt.savefig('./figure_3_5.png') plt.show() plt.close() break value = new_valuefigure_3_5() 后记这是关于马尔科夫决策过程一个相对简单的过程，可以看到这里一旦确定了previous state和action，current state和对应的reward就确定了。即Pr{s’,r|s,a}=1，事实上MDP的应用包括优化过程都是有相当的局限性的： （1）首先我们需要准确的知道环境的动态变化； （2）我们要有足够的算力来推出所有需要的state和value； （3）我们需要保证问题的马尔科夫性。 然而这里的（2）则很大程度上局限了MDP的推广，因为事实上如果需要在所有的state上做出action并update value，对于state数量庞大的任务几乎不可能完成的。]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter03 Finite Markov Decision Processes]]></title>
    <url>%2F2018%2F11%2F22%2FChapter03-Finite-Markov-Decision-Processes%2F</url>
    <content type="text"><![CDATA[马尔可夫决策过程(Markov Decision Process, MDP)，是指具有马尔可夫性的一类强化学习问题，即系统下个状态和当前的状态有关，以及当前采取的动作有关。 some notation in MDP1.state : S_t 2.action: A_t 3.reward: R_t 4.a probability of those values occurring at time t, given particular values of the preceding state and action: 上述概率对s’和r分别累加=1 5.state-transition probability 6.expect reward 7.expected reward for state-action-next-state return of episode tasks and continuing taskswhat is return of task?if we define each step reward as R_i(i=0,1,2,3…), then expect return is defined as some specific function of the reward sequence. what is episode task?Such as plays of a game, trips through a maze, or any sort of repeated interaction. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. And the next episode begins independently of how the previous one ended. what is continuing taks?agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. the return of above two taskepisode task continuing task Unified notationthrough add an absorbing state in episode task(solid square): unified notation: update value function in mdp methodstate-value function for policy π: state-action-value function for policy π: how to update the state in policy π?Bellman equation for v π: Instead of getting average return in some random samples(Monte Carlo method), use some parameters to describle. π(a|s) is the policy π: decide action due to state s p(s’,r|s,a) is the transition probability of previous state s and action a, which follows the Markov property get the optimal policy π and optimal policy value-statechoose the max value in the actions of state s:]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter02 ten-armed-Testbed]]></title>
    <url>%2F2018%2F11%2F16%2FChapter02-ten-armed-Testbed%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter02/ten_armed_testbed.py 通过建立10-armed-Testbed来仿真第二章讲的几种Bandit算法 1、引入模块123456import matplotlibimport matplotlib.pyplot as plt%matplotlib inlineimport numpy as np# use for showing process barfrom tqdm import tqdm 2、创建Testbed类，实现基本的action和update value方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class Bandit: # @k_arm: # of arms # @epsilon: probability for exploration in epsilon-greedy algorithm # @initial: initial estimation for each action # @step_size: constant step size for updating estimations # @sample_averages: if True, use sample averages to update estimations instead of constant step size # @UCB_param: if not None, use UCB algorithm to select action # @gradient: if True, use gradient based bandit algorithm # @gradient_baseline: if True, use average reward as baseline for gradient based bandit algorithm def __init__(self, k_arm=10, epsilon=0., initial=0., step_size=0.1, sample_averages=False, UCB_param=None, gradient=False, gradient_baseline=False, true_reward=0.): self.k = k_arm self.step_size = step_size self.sample_averages = sample_averages self.indices = np.arange(self.k) self.time = 0 self.UCB_param = UCB_param self.gradient = gradient self.gradient_baseline = gradient_baseline self.average_reward = 0 self.true_reward = true_reward self.epsilon = epsilon self.initial = initial def reset(self): # real reward for each action # true_reward is baseline can be configed # np.random.randn(_size()) return standard normal array in the size of _size() self.q_true = np.random.randn(self.k) + self.true_reward # estimation for each action # here we can modify the serf.initial to export optimistic initial value self.q_estimation = np.zeros(self.k) + self.initial # of chosen times for each action self.action_count = np.zeros(self.k) # return the index of max-real-reward action self.best_action = np.argmax(self.q_true) # get an action for this bandit def act(self): # np.random.rand() return uniform distribution over [0,1) # explore if np.random.rand() &lt; self.epsilon: # return np.random.choice(self.indices) # I think UCB may choose actions in non-greedy actions if self.UCB_param is not None: # here is a little different from book, Why? # I think he may want to be avoid of zero problem in log and denominator position UCB_estimation = self.q_estimation + \ self.UCB_param * np.sqrt(np.log(self.time + 1) / (self.action_count + 1e-5)) q_best = np.max(UCB_estimation) return np.random.choice([action for action, q in enumerate(UCB_estimation) if q == q_best]) else: return np.random.choice(self.indices) if self.gradient: exp_est = np.exp(self.q_estimation) self.action_prob = exp_est / np.sum(exp_est) # According to the probability to choose action # here use the same method with wiki return np.random.choice(self.indices, p=self.action_prob) # greey-action return np.argmax(self.q_estimation) # take an action, update estimation for this action def step(self, action): # generate the reward under N(real reward, 1) reward = np.random.randn() + self.q_true[action] self.time += 1 # average_reward update uses self.time self.average_reward = (self.time - 1.0) / self.time * self.average_reward + reward / self.time self.action_count[action] += 1 if self.sample_averages: # update estimation using sample averages # q_estimation update uses action_count self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action]) # is different with book elif self.gradient: one_hot = np.zeros(self.k) one_hot[action] = 1 if self.gradient_baseline: baseline = self.average_reward else: baseline = 0 # update method is the same as book, here use q_estimation(a) replaces H_t(a) self.q_estimation = self.q_estimation + self.step_size * (reward - baseline) * (one_hot - self.action_prob) else: # update estimation with constant step size self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action]) return reward 3、训练Bandit123456789101112131415def simulate(runs, time, bandits): best_action_counts = np.zeros((len(bandits), runs, time)) rewards = np.zeros(best_action_counts.shape) for i, bandit in enumerate(bandits): for r in tqdm(range(runs)): bandit.reset() for t in range(time): action = bandit.act() reward = bandit.step(action) rewards[i, r, t] = reward if action == bandit.best_action: best_action_counts[i, r, t] = 1 best_action_counts = best_action_counts.mean(axis=1) rewards = rewards.mean(axis=1) return best_action_counts, rewards 4、结果显示（折线图格式）1、10-Bandit-Testbed value 和reward分布123456def figure_2_1(): plt.violinplot(dataset=np.random.randn(200,10) + np.random.randn(10)) plt.xlabel("Action") plt.ylabel("Reward distribution") plt.show()figure_2_1() 2、different epsilons12345678910111213141516171819202122def figure_2_2(runs=2000, time=1000): epsilons = [0, 0.1, 0.01] bandits = [Bandit(epsilon=eps, sample_averages=True) for eps in epsilons] best_action_counts, rewards = simulate(runs, time, bandits) plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) for eps, rewards in zip(epsilons, rewards): plt.plot(rewards, label='epsilon = %.02f' % (eps)) plt.xlabel('steps') plt.ylabel('average reward') plt.legend() plt.subplot(2, 1, 2) for eps, counts in zip(epsilons, best_action_counts): plt.plot(counts, label='epsilon = %.02f' % (eps)) plt.xlabel('steps') plt.ylabel('% optimal action') plt.legend() plt.show()# figure_2_2() 3、Initial value = 5 VS Initial value = 01234567891011121314def figure_2_3(runs=2000, time=1000): bandits = [] bandits.append(Bandit(epsilon=0, initial=5, step_size=0.1)) bandits.append(Bandit(epsilon=0.1, initial=0, step_size=0.1)) best_action_counts, _ = simulate(runs, time, bandits) plt.plot(best_action_counts[0], label='epsilon = 0, q = 5') plt.plot(best_action_counts[1], label='epsilon = 0.1, q = 0') plt.xlabel('Steps') plt.ylabel('% optimal action') plt.legend() plt.show()#figure_2_3() 4、UCB VS epsilon-greey12345678910111213def figure_2_4(runs=2000, time=1000): bandits = [] bandits.append(Bandit(epsilon=0, UCB_param=2, sample_averages=True)) bandits.append(Bandit(epsilon=0.1, sample_averages=True)) _, average_rewards = simulate(runs, time, bandits) plt.plot(average_rewards[0], label='UCB c = 2') plt.plot(average_rewards[1], label='epsilon greedy epsilon = 0.1') plt.xlabel('Steps') plt.ylabel('Average reward') plt.legend() plt.show()#figure_2_4() 5、softmax baseline VS non-baseline12345678910111213141516171819def figure_2_5(runs=2000, time=1000): bandits = [] bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=True, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=False, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=True, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=False, true_reward=4)) best_action_counts, _ = simulate(runs, time, bandits) labels = ['alpha = 0.1, with baseline', 'alpha = 0.1, without baseline', 'alpha = 0.4, with baseline', 'alpha = 0.4, without baseline'] for i in range(0, len(bandits)): plt.plot(best_action_counts[i], label=labels[i]) plt.xlabel('Steps') plt.ylabel('% Optimal action') plt.legend() plt.show()#figure_2_5() 6、epsilon-greey vs softmax vs UCB vs opt-initial123456789101112131415161718192021222324252627282930def figure_2_6(runs=2000, time=1000): labels = ['epsilon-greedy', 'gradient bandit', 'UCB', 'optimistic initialization'] generators = [lambda epsilon: Bandit(epsilon=epsilon, sample_averages=True), lambda alpha: Bandit(gradient=True, step_size=alpha, gradient_baseline=True), lambda coef: Bandit(epsilon=0, UCB_param=coef, sample_averages=True), lambda initial: Bandit(epsilon=0, initial=initial, step_size=0.1)] parameters = [np.arange(-7, -1, dtype=np.float), np.arange(-5, 2, dtype=np.float), np.arange(-4, 3, dtype=np.float), np.arange(-2, 3, dtype=np.float)] bandits = [] for generator, parameter in zip(generators, parameters): for param in parameter: bandits.append(generator(pow(2, param))) _, average_rewards = simulate(runs, time, bandits) rewards = np.mean(average_rewards, axis=1) i = 0 for label, parameter in zip(labels, parameters): l = len(parameter) plt.plot(parameter, rewards[i:i+l], label=label) i += l plt.xlabel('Parameter(2^x)') plt.ylabel('Average reward') plt.legend() plt.show()figure_2_6()]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter02 softmax-theory]]></title>
    <url>%2F2018%2F11%2F15%2FChapter02-softmax-theory%2F</url>
    <content type="text"><![CDATA[This post is my understanding about P29 deeper insight part:《The Bandit Gradient Algorithm as Stochastic Gradient Ascent》 1. Understand Why it comes from Stochastic Gradient Ascent在我们讨论这个问题之前先看看wiki上关于softmax在reinforcementLearning上的应用吧。 相信你和我一样惊奇，这里本来的H_t(a)变成了q_t(a)/τ。其中τ为温度系数(temperature parameter)，τ-&gt;无穷，所以action具有相同的选择概率，或者说是explore的；如果τ-&gt;0,则具有最大q_t(a)的行为会被选择，且τ越小，选择q_t(a)最大的概率越大并趋于1。 这太有意思了，wiki上直接解释了softmax function选择action的合理性，但是这就是这篇post想要讨论的问题啊@_@。 好了回到标题，书上这里这样写的： In exact gradient ascent, each preference H_t(a) would be incremented proportional to the increment’s effect on performance: 并给出了公式： E[R_t]的定义为： 从梯度上升的角度来解释：如果平均的value:E[R_t]对H_t(a)导数为正，则提高H_t(a)可以提高E[R_t]；如果相反，则减小H_t(a)可以提高E[R_t]。总结起来就是H_t(a)的增量和E[R_t]对H_t(a)的偏导成正比。 计算偏导计算公式如下： 公式的难点主要在第三步B_t的引入。B_t is called the baseline, can be any scalar that does not depend on x. We can include a baseline here without changing the equality because the gradient sums to zero over all the actions: change the equation to the form of expectation 这里的难点是使用R_t替换q_(A_t)，因为E[R_t|A_t] = q_(A_t)，所以可以替换。接着我们处理一下后半部分： 其中1_(a=x) means that if a=x, output 1; else output 0 因为我们通过无数次训练来更新H_t(a)，所以可以去掉期望符号，并在前面加上step-size α进行近似。如果α=1/n，那么就符合大数定律近似了，如果采用常数可能对解决非平稳问题更好一些。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter02 Multi-armed Bandits]]></title>
    <url>%2F2018%2F11%2F14%2FChapter02-Multi-armed-Bandits%2F</url>
    <content type="text"><![CDATA[2.1.1 什么是k-armed Bandits问题You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.大致意思就是，每步有k种action选择，每种选择对应不同的reward，完成的目标就是需要在n次执行后，最大化总共的reward。 In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action: 2.1.2 k-armed Bandits问题的难点这里主要是explore和exploit的权衡。下面的解释摘自周老师的西瓜书强化学习部分： 若仅为知道每个摇臂的期望奖赏，则可采用“仅探索”(exploration-only)法：将所有的尝试机会平均分配给每个摇臂，最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。若仅为执行奖赏最大的动作，则可以用“仅利用”(exploitation-only)法，按下目前最优的摇臂。前者可以很好的估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会，没办法得到最好的收益；后者刚好相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优的摇臂，因此这两种方法都难以使最终的积累奖励最大化。 根据实验情况，exploring-only收敛结果明显劣于exploiting-only，而exploiting-only效果同样很差。 2.2 Action-value Method这是原文标题我直接抄过来了。因为上个标题提到了行为的价值(value)，所以需要给一种量化这种value的算法。 We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: 然后如何选择action？greedy action：选择Qt最大的a作为t时刻的action ε-greedy action: behave greedily most of the time, but every once in a while, say with small probability ε, instead select randomly from among all the actions with equal probability,independently of the action-value estimates. 2.3 The 10-armed Testbed10-armed Testbed是测试k-armed Bandits算法的一个基础模型，后面的模型评估都是基于此的。 What is it?简而言之，Testbed中包含了2000个10-armed Bandits问题，每个问题的10个行为action对应的value q(a)是通过正态分布得到的(mean=0,variance=1)，而实际对每个问题进行训练的时候，得到的实际回报(actual reward)是在action value上附加一个正态分布(mean=q(a),variance=1)。然后用greedy算法和ε-greedy算法对每个问题进行1000step的训练，对2000个问题求平均数得到 target(Average reward or Optimal action) - Steps的折线图。并以此来评价算法性能。 算法对比参照Python代码的博客 2.4 Incremental Implementation这部分讲了一个增量优化算法，这让我联想到了增量式PID和位置式PID，QAQ。其实这个思想和那个也挺相似的。 previous edition: PS:这个公式和第一章的tic-tac-toe的V(s)更新公式太像了。 but，还是有一些区别。第一章的V(s)更新的时候是仅针对greedy-action的，但是这里因为各个action是独立的，如果不对exploring更新的话，就没办法有效的找到最优的action了，即这次action是无意义的。 incremental edition: incremental bandit algorithm: an intresting update ruleThe update rule below is of a form that occurs frequently throughout this book. The general form is: about this rule:1、The expression [Target−OldEstimate] is an error in the estimate. It is reduced by taking a step toward the “Target.” The target is presumed to indicate a desirable direction in which to move, though it may be noisy. In the case above, for example, the target is the nth reward. 2、Note that the step-size parameter (StepSize) used in the incremental method described above changes from time step to time step. In processing the nth reward for action a, the method uses the step-size parameter 1/n . In this book we denote the step-size parameter by α or, more generally, by α_t(a). 2.5 Tracking a Nonstationary(非平稳) Problem上面讨论的Bandits-methods，都是建立在rewards的概率不变的前提下的，如Testbed中规定，reward(a)遵循(mean = q*(a),variance = 1)的概率分布。而在实际的强化学习任务中，经常是与之相对的概率不平稳的(nonstationary)。 考虑上一个标题讨论的update rule的一般形式，处理非平稳问题一个有效方法就是令α为定值(use a constant step-size parameter)，因为这样可以提高最近的reward的比重，降低过去久远的reward的比重，可以参考下面的公式： 因为α&lt;1，所以很明显如果i越小，R_i的weight就会越小。 但是有一个问题，就是如果希望最终的value是收敛的，则Stepsize需要满足如下条件： 第一条保证weight足够大，可以覆盖掉初值的误差以及一些波动，第二条保证最后能够收敛。 当α=1/n条件很明显是满足的，但是如果α=constant第二条就不满足了。但是对于非稳定的情况，这正是我们希望看到的，具体证明也没给出，下次看到补上？ 2.6 Optimistic Initial Values(encouraging exploration)我们在上面看到α=1/n时，Q1被消去了，但是事实上大部分情况下Q1会对学习情况产生影响，比如当α=constant时，Q1就被保留了，不过*了一个相当小的系数。。。 事实上，this kind of bias is usually not a problem and can sometimes be very helpful.举个栗子，在Testbed里，我把训练初值设为5，对于在1附近正态分布的reward，不管选择哪个，在初始阶段都会&lt;5，学习器对他们都很失望：Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being “disappointed” with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time. 但是对于非平稳问题(nonstationary)，因为分布会随时间改变，而这种只在训练开始引入exploring的方法相当于只是encourage exploration for once，故效果不会很好，但是对应sample-average(平稳)的问题就会有一定的效果。 当然，我们不会只采用一种方法，将不同的方法进行组合利用，将会是贯穿全书的思想。 2.7 Upper-Confidence-Bound Action Selection在ε-greedy方法中，通过一个小概率ε来选定non-greedy action进行exploring，但是这种方法是无差别的，如果对那些non-greedy action进行一个事前的评估，根据它们潜能(不确定度)来进行explore的话，那么可以提高explore的广度和效率，效果可能会更好。 One effective way of doing this is to select actions according to(UCB): N_t(a) denotes the number of times that action a has been selected prior to time t.number c &gt; 0 controls the degree of exploration.If N_t(a) = 0, then a is considered to be a maximizing action.公式里的平方根部分是对action a的不确定度的估计，可以从两方面来解释：如果action a经常被选中，则分母N_t就会变大，不确定度就会减小；如果action a不经常被选中，那么随着试验次数t增大，而N_t不变，则不确定度上升，我们应该多考虑一下a啦。 但是也存在一些问题UCB比起ε-greedy方法，相对来说更复杂，而且它在其他强化学习问题中的可扩展性(extend)远不如后者，而且这种方法在非平稳问题中表现的并不好，Another difficulty is dealing with large state spaces, particularly when using function approximation as developed in Part II of this book.In these more advanced settings the idea of UCB action selection is usually not practical. 2.8 Gradient Bandit Algorithms看了这么久，休息一下如何^_^ A new method to choose actionconsider learning a numerical preference for each action a, which we denote H_t(a). The larger the preference, the more often that action is taken: π_t(a) is the probability of taking action a at time t. Initially all preferences are the same(zero)so that all actions have an equal probability of being selected. How to update the H_t(a) in the rule: α&gt;0 is a step-size paramter; (R_t)_average is the baseline with which the reward is compared.It which can be computed incrementally as described in Incremental Implementation about the baselineIf the reward is higher than the baseline, then the probability of taking A_t in the future is increased, and if the reward is below baseline, then probability is decreased. The non-selected actions move in the opposite direction. the theory of softmax methodhere is a seperate blog to explain it. 2.9 Associative Search (Contextual Bandits)上面的算法只处理了一台k-arms-Bandit，接下来考虑这个问题：如果你面对的是10台k-arms-Bandit，每次你随机从这几台机器(k-arms-Bandit comes from slot machine)里抽一个来操作，那么可以认为这仍是在处理同一台slot machine，但是true value是随着一步一步操作剧烈变化的。 说到这里应该就明白了，我的每一步action都是会对接下来的situation和reward引起影响的，而普通的k-arms-Bandit每步之间是独立的。 associative search可以认为是k-arms-Bandit和full reinforcement learning之间的桥梁，现在它一般被称为contextual bandits问题。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter01 Tic-Tac-Toe]]></title>
    <url>%2F2018%2F11%2F14%2FChapter01-Tic-Tac-Toe%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter01/tic_tac_toe.py Tic-Tac-Toe的python代码实现1、引入模块并定义井字棋的常量 12345678import numpy as npimport pickleimport matplotlib.pyplot as plt%matplotlib inline# 将rows和cols调至4，这种算法计算量就会大很多BOARD_ROWS = 3BOARD_COLS = 3BOARD_SIZE = BOARD_ROWS * BOARD_COLS 2、创建环境，State类，表征棋盘上的X和O子的情况。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class State: def __init__(self): # the board is represented by an n * n array, # 1 represents a chessman of the player who moves first, # -1 represents a chessman of another player # 0 represents an empty position self.data = np.zeros((BOARD_ROWS, BOARD_COLS)) self.winner = None self.hash_val = None self.end = None # compute the hash value for one state, it only work at first time def hash(self): if self.hash_val is None: self.hash_val = 0 for i in self.data.reshape(BOARD_ROWS * BOARD_COLS): if i == -1: # shallow copy i = 2 self.hash_val = self.hash_val * 3 + i return int(self.hash_val) # check whether a player has won the game, or it's a tie def is_end(self): if self.end is not None: return self.end results = [] # check row for i in range(0, BOARD_ROWS): results.append(np.sum(self.data[i, :])) # check columns for i in range(0, BOARD_COLS): results.append(np.sum(self.data[:, i])) # check diagonals results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, i] results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, BOARD_ROWS - 1 - i] for result in results: if result == 3: self.winner = 1 self.end = True return self.end if result == -3: self.winner = -1 self.end = True return self.end # whether it's a tie: no one wins, all places are filled. sum = np.sum(np.abs(self.data)) if sum == BOARD_ROWS * BOARD_COLS: self.winner = 0 self.end = True return self.end # game is still going on self.end = False return self.end # @symbol: 1 or -1 # put chessman symbol in position (i, j) def next_state(self, i, j, symbol): new_state = State() # deep copy new_state.data new_state.data = np.copy(self.data) new_state.data[i, j] = symbol return new_state # print the board def print(self): for i in range(0, BOARD_ROWS): print('-------------') out = '| ' for j in range(0, BOARD_COLS): if self.data[i, j] == 1: token = '*' if self.data[i, j] == 0: token = '0' if self.data[i, j] == -1: token = 'x' out += token + ' | ' print(out) print('-------------') 关于这里的hash函数，是为了得到不重复的所有棋盘情况。但是为什么可以通过这种算法那？假设两个不同的棋盘情况A和B，他们的hash值是一致的，我们不妨设他们的最高位不相同，那么我们看到，该位最少相差1，通过等比数列求和可以得到，3^(n+1)&gt;2*(1+3^1+3^2+…+3^n)，即最高位不同是无法通过其余位来弥补的，那么可见他们的最高位不同是不可能hash值相同的，以此递推，则可得A和B值是一样，即一样的棋盘，假设不成立。3、公共函数，用来获取当前state。12345678910111213141516171819202122232425def get_all_states_impl(current_state, current_symbol, all_states): for i in range(0, BOARD_ROWS): for j in range(0, BOARD_COLS): if current_state.data[i][j] == 0: newState = current_state.next_state(i, j, current_symbol) newHash = newState.hash() if newHash not in all_states.keys(): isEnd = newState.is_end() all_states[newHash] = (newState, isEnd) if not isEnd: get_all_states_impl(newState, -current_symbol, all_states)def get_all_states(): current_symbol = 1 current_state = State() all_states = dict() all_states[current_state.hash()] = (current_state, current_state.is_end()) get_all_states_impl(current_state, current_symbol, all_states) return all_states# all possible board configurationsall_states = get_all_states()# for i in all_states.values():# i[0].print()# print(i[1]) 4、Player类，用来模拟AI选手下棋的行为，得到value function1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# AI playerclass Player: # @step_size: the step size to update estimations # @epsilon: the probability to explore def __init__(self, step_size=0.1, epsilon=0.1): self.estimations = dict() self.step_size = step_size self.epsilon = epsilon self.states = [] # greedy means that state is caused by exploiting, otherwise is caused by exploring self.greedy = [] def reset(self): self.states = [] self.greedy = [] def set_state(self, state): self.states.append(state) self.greedy.append(True) # give the player symbol,and update the value table def set_symbol(self, symbol): self.symbol = symbol for hash_val in all_states.keys(): (state, is_end) = all_states[hash_val] if is_end: if state.winner == self.symbol: self.estimations[hash_val] = 1.0 elif state.winner == 0: # we need to distinguish between a tie and a lose self.estimations[hash_val] = 0.5 else: self.estimations[hash_val] = 0 else: self.estimations[hash_val] = 0.5 # update value estimation def backup(self): # for debug # print('player trajectory') # for state in self.states: # state.print() self.states = [state.hash() for state in self.states] for i in reversed(range(len(self.states) - 1)): state = self.states[i] # only udpate the state caused by exploiting td_error = self.greedy[i] * (self.estimations[self.states[i + 1]] - self.estimations[state]) self.estimations[state] += self.step_size * td_error # choose an action based on the state def act(self): state = self.states[-1] next_states = [] next_positions = [] for i in range(BOARD_ROWS): for j in range(BOARD_COLS): if state.data[i, j] == 0: next_positions.append([i, j]) next_states.append(state.next_state(i, j, self.symbol).hash()) # explore if np.random.rand() &lt; self.epsilon: action = next_positions[np.random.randint(len(next_positions))] action.append(self.symbol) self.greedy[-1] = False return action # exploit values = [] for hash, pos in zip(next_states, next_positions): values.append((self.estimations[hash], pos)) np.random.shuffle(values) values.sort(key=lambda x: x[0], reverse=True) # choose the biggest value action action = values[0][1] action.append(self.symbol) return action # policy -&gt; self.estimations def save_policy(self): with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f: pickle.dump(self.estimations, f) def load_policy(self): with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f: self.estimations = pickle.load(f) 这里需要注意一点就是，在backup方法里，只有greedy-action才会更新value，exploring虽然不会更新value，但是其随后的greedy-action会进行更新，个人的理解是：exploring并不能保证最优，它只是提供一种“探索”行为，帮助Learning method更好的学习value，所以只对greedy-action进行update。这地方原文有解释：Exploratory moves do not result in any learning, but each of our other moves does, causing updates as suggested by the curved arrow in which estimated values are moved up the tree from later nodes to earlier as detailed in the text. 5、创建对决类Judger，通过一次对局完成一次value table的update1234567891011121314151617181920212223242526272829303132333435363738394041424344class Judger: # @player1: the player who will move first, its chessman will be 1 # @player2: another player with a chessman -1 # @feedback: if True, both players will receive rewards when game is end def __init__(self, player1, player2): self.p1 = player1 self.p2 = player2 self.current_player = None self.p1_symbol = 1 self.p2_symbol = -1 self.p1.set_symbol(self.p1_symbol) self.p2.set_symbol(self.p2_symbol) self.current_state = State() def reset(self): self.p1.reset() self.p2.reset() # return p1 and p2 in turn(alternately) def alternate(self): while True: yield self.p1 yield self.p2 # @print: if True, print each board during the game def play(self, print=False): alternator = self.alternate() self.reset() current_state = State() self.p1.set_state(current_state) self.p2.set_state(current_state) while True: player = next(alternator) if print: current_state.print() [i, j, symbol] = player.act() next_state_hash = current_state.next_state(i, j, symbol).hash() current_state, is_end = all_states[next_state_hash] self.p1.set_state(current_state) self.p2.set_state(current_state) if is_end: if print: current_state.print() return current_state.winner 6、训练并让两名AI Player进行对局12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def train(epochs): player1 = Player(epsilon=0.01) player2 = Player(epsilon=0.01) judger = Judger(player1, player2) player1_win = 0.0 player2_win = 0.0 player1_a = [] player2_a = [] for i in range(1, epochs + 1): winner = judger.play(print=False) if winner == 1: player1_win += 1 if winner == -1: player2_win += 1 player1_a.append(player1_win/i) player2_a.append(player2_win/i) #print('Epoch %d, player 1 win %.02f, player 2 win %.02f' % (i, player1_win / i, player2_win / i)) player1.backup() player2.backup() judger.reset() player1.save_policy() player2.save_policy() x = range(1,1001,1) x = list(x) plt.plot(x,player1_a) plt.plot(x,player2_a) plt.axis([0,200,0,1]) plt.title('training chart') plt.show() def compete(turns): player1 = Player(epsilon=0) player2 = Player(epsilon=0) judger = Judger(player1, player2) player1.load_policy() player2.load_policy() player1_win = 0.0 player2_win = 0.0 for i in range(0, turns): winner = judger.play() if winner == 1: player1_win += 1 if winner == -1: player2_win += 1 judger.reset() print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))train(int(1e3))]]></content>
      <tags>
        <tag>Reinforcement Learning Jupyter Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter01 Introduction]]></title>
    <url>%2F2018%2F11%2F12%2FChapter01-Introduction%2F</url>
    <content type="text"><![CDATA[希望能以更新博客的方式激励一下自己，目前是准备读一下强化学习的入门书《Reinforcement Learning Introduction》，然后做一下读书笔记。下面是绪论(Introduction)的内容。 What is ReinforcementLearningFirst Impression of RL如果说之前我接触到的机器学习方法，如逻辑回归、svm、决策树、人工神经网络等是数据驱动的，那么强化学习就是“情境”驱动的：行为由当前环境决定，行为导致数值化的reward更新，而reward又影响了环境。这是我对强化学习的初印象。 强化学习有两个特点：1、trial-and-error search：学习算法并未指导行为，但是却必须得出当行为做出相应的reward(近期) 2、delayed reward：每个行为之间不是独立的，当前行为可能会有比当前收益更丰盛的远期收益(value)。 强化学习有三个重要要素，就是学习器(agent，我对学习器的理解就是训练出模型时执行的程序）必须能够感知环境(state)并做出影响它的行为(action)，同时agent must have a goal or goals relating to the state of the environment。 RL vs Supervised learning监督学习就不赘述了，大致来说，监督学习算法通过带标签的训练集学习得到可以对未知样例的判断“能力”。但是如果让监督学习从交互中学习，按照监督学习的思路，获取正确的而且有意义的行为例子是不可取的，所以它没办法从经验中成长，不适合需要和环境交互的算法。 举一个不太恰当的例子，监督学习就像考前刷题的你，而强化学习就像每天认真学习的学霸，如果考题和你刷的吻合度比较高，那你就稳了，但是学霸每天学习，数据已经内化为他的能力了，所以他一点都不慌。 RL vs Unsupervised learning有不少人认为监督学习和非监督学习已经把机器学习进行详尽的划分了，他们将强化学习归入后者，因为它们都没有带标签的数据，但是从目的上来看，强化学习是为了最大化reward的，而非监督学习是为了学习到数据之中的隐藏结构，非监督学习的方法并没有从实质解决强化学习的问题。 exploration and exploitationThe agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future. 这个问题有点像梯度下降中使用的随机梯度法(SGD)或者模拟退火法，但是SGD是一种加速算法，或者说如果我头铁不用，我也是有可能得到还可以的结果，但是强化学习如果不学习未知的部分，结果肯定是有问题的，但是如果一直在试错 explore，不对“经验”进行总结 exploit，也是没办法使算法收敛的。 Tic-Tac-Toe我觉得先分析一个例子可以更好的帮助理解RL的各个属性之间的关系。Tic-Tac-Toe比较像我们小时候玩的井字棋，图我就不画了。 强化学习和传统的classical minimax solution from game theory以及Classical optimization methods for sequential decision problems, such as dynamic programming有区别，前者是按照有强约束的方式运动的（这里是不会走使自己失败的位置），而后者则是需要有每一步的先验概率才能进行优化。 强化学习是基于value functions的学习方法，而前面提到的minimax可以认为是基于evolutionary的学习方法，前者是基于value of state来学习的，后者则是通过结果来更新policy，前者对状态的利用优于后者，而后者则是仅利用了结果来修改policy。 tic-tac-toe的ython代码实现戳这里 Elements of RLBeyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal , a value function, and, optionally, a model of the environment. policy感觉是从state到action的一个映射关系吧，这里就对应了上面提到的explore和exploit的关系。In general, policies may be stochastic. reward每个action导致的短期收益。reward会决定policy，使得行为向高reward的方向发展，所以reward一般是state和action的随机函数：In general, reward signals may be stochastic functions of the state of the environment and the actions taken. value function由reward引起的长期收益。value由reward综合而来，而value同时会使reward朝着高的方向发展。value控制action的发展：Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run. model of the environmentThis is something that mimics(模拟) the behavior of the environment, or more generally, that allows inferences(推断) to be made about how the environment will behave. Summary总体来说强化学习和之前学过的其他机器学习方法还是区别挺大的，但是绪论有几点疑问： 1、action、state、reward、value之间的更新方式还是没搞懂？ 2、如何建立value table？ ——————-(answers in 11.14)————————- 1、value建立在state之上，每次state更新伴随着value的更新；action分为两种，explore和exploit，前者随机，后者基于reward最大；reward认为是从current state -&gt; next state update时，对应的next value or (next value - current value) 2、value table建立基于state，原则视问题而定。比如tic-tac-toe问题，X赢对应的value = 1，O赢对应的value = 0，平局对应value = 0.5，当然我是站在X的角度。]]></content>
      <tags>
        <tag>Reinforcement Learning Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[some config on next theme]]></title>
    <url>%2F2018%2F11%2F10%2Fsome-config-on-next-theme%2F</url>
    <content type="text"><![CDATA[上次配置了hexo+github的个人博客，这次我做了一些偏好的配置。 更换主题为next因为网上很多关于主题配置的博客都是基于next主题的，所以我先将主题换为了next主题： 1、先下载主题到本地 123$ cd ~$ cd GitBlog$ git clone https://github.com/iissnan/hexo-theme-next themes/next 2、更改站点的config文件来使用主题 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 3、更改theme/next的config文件(/theme/next/_config.yml): 123456789101112# 修改主题模式为Gemini# Schemes#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini#修改侧边栏头像# Sidebar Avatar# in theme directory(source/images): /images/avatar.gif# in site directory(source/uploads): /uploads/avatar.gifavatar: https://wx2.sinaimg.cn/large/0070VybLly1fx5b09mly4j30kx0i14kq.jpg 添加搜索功能下载hexo搜索插件hexo-generator-search然后修改站点的config文件启动搜索功能： 12345678910# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next#在插件位置插入搜索模块使能插件search: path: search.xml field: post format: html limit: 10000 添加渐变更换的壁纸效果比较像忧郁的弟弟(额马上就是loli了)背景效果。 1、在/theme/next/source/css/_custom/中修改_custom.styl，添加css3动画特效： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137body &#123;background: #000;background-attachment: fixed;word-wrap: break-word;-webkit-background-size: cover;-moz-background-size: cover;background-size: cover;background-repeat: no-repeat&#125; ul &#123;list-style: none&#125; .cb-slideshow li:nth-child(1) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b098is8j31hc0u0jzz.jpg)&#125; .cb-slideshow li:nth-child(2) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b095t28j31hc11hdph.jpg)&#125; .cb-slideshow li:nth-child(3) span &#123;background-image: url(https://wx2.sinaimg.cn/large/0070VybLly1fx5b09b17xj31hc0z2tim.jpg)&#125; .cb-slideshow li:nth-child(4) span &#123;background-image: url(https://wx4.sinaimg.cn/large/0070VybLly1fx5b09474xj31hc0x6jy5.jpg)&#125; .cb-slideshow li:nth-child(5) span &#123;background-image: url(https://wx2.sinaimg.cn/large/0070VybLly1fx5b08weitj31hc11otga.jpg)&#125; .cb-slideshow li:nth-child(6) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b1eraokj31hc0u0gp9.jpg)&#125; .cb-slideshow,.cb-slideshow:after &#123;position: fixed;width: 100%;height: 100%;top: 0;left: 0;z-index: -2&#125; .cb-slideshow:after &#123;content: ''&#125; .cb-slideshow li span &#123;width: 100%;height: 100%;position: absolute;top: 0;left: 0;color: transparent;background-size: cover;background-position: 50% 50%;background-repeat: none;opacity: 0;z-index: -2;-webkit-backface-visibility: hidden;-webkit-animation: imageAnimation 36s linear infinite 0s;-moz-animation: imageAnimation 36s linear infinite 0s;-o-animation: imageAnimation 36s linear infinite 0s;-ms-animation: imageAnimation 36s linear infinite 0s;animation: imageAnimation 36s linear infinite 0s&#125; .cb-slideshow li:nth-child(2) span &#123;-webkit-animation-delay: 6s;-moz-animation-delay: 6s;-o-animation-delay: 6s;-ms-animation-delay: 6s;animation-delay: 6s&#125; .cb-slideshow li:nth-child(3) span &#123;-webkit-animation-delay: 12s;-moz-animation-delay: 12s;-o-animation-delay: 12s;-ms-animation-delay: 12s;animation-delay: 12s&#125; .cb-slideshow li:nth-child(4) span &#123;-webkit-animation-delay: 18s;-moz-animation-delay: 18s;-o-animation-delay: 18s;-ms-animation-delay: 18s;animation-delay: 18s&#125; .cb-slideshow li:nth-child(5) span &#123;-webkit-animation-delay: 24s;-moz-animation-delay: 24s;-o-animation-delay: 24s;-ms-animation-delay: 24s;animation-delay: 24s&#125; .cb-slideshow li:nth-child(6) span &#123;-webkit-animation-delay: 30s;-moz-animation-delay: 30s;-o-animation-delay: 30s;-ms-animation-delay: 30s;animation-delay: 30s&#125; @-webkit-keyframes imageAnimation &#123;0% &#123;opacity: 0;-webkit-animation-timing-function: ease-in&#125; 8% &#123;opacity: 1;-webkit-transform: scale(1.05);-webkit-animation-timing-function: ease-out&#125; 17% &#123;opacity: 1;-webkit-transform: scale(1.1) rotate(0)&#125; 25% &#123;opacity: 0;-webkit-transform: scale(1.1) rotate(0)&#125; 100% &#123;opacity: 0&#125;&#125; 2、在/theme/next/layout/中修改_layout.swig，在标签中加入下述代码： 1234567891011121314&lt;ul class="cb-slideshow"&gt; &lt;li&gt; &lt;span&gt;1&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;2&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;3&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;4&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;5&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;6&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt;]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first try of hexo+github]]></title>
    <url>%2F2018%2F11%2F09%2Fhow-to-config-the-hexo%2F</url>
    <content type="text"><![CDATA[今天我学习了如何配置Hexo来写博客，并放到github上，但是我还不会markdown，自己还是好菜啊QAQ。 卸载之前安装不成功的hexo，npm和nodejs卸载hexo12$ cd /usr/bin$ npm uninstall hexo -g ###卸载nodejs和npm 123$ sudo apt-get remove nodejs$ sudo apt-get remove node$ sudo apt-get remove npm 可能会没用，我是运行了第一个就把都卸载了，但是不影响哈。 安装nvmnvm是node的包版本控制工具，使用这个我装上了稳定版的hexo，之前我直接apt的hexo在运行 hexo s时会报错： ERROR Local hexo not found。 下载nvm12345$ export NVM_DIR="$HOME/.nvm" &amp;&amp; ( git clone https://github.com/creationix/nvm.git "$NVM_DIR" cd "$NVM_DIR" git checkout `git describe --abbrev=0 --tags --match "v[0-9]*" origin`) &amp;&amp; . "$NVM_DIR/nvm.sh" 配置为nvm自启动在 bash配置文件.bashrc中加入（maybe .zshrc）： 12export NVM_DIR="$HOME/.nvm"[ -s "$NVM_DIR/nvm.sh" ] &amp;&amp; . "$NVM_DIR/nvm.sh" 使用nvm安装nodejs和npm检测远程仓库1$ nvm ls-remote 安装稳定版本nodejs1$ nvm install stable 启动安装好的nodejs1$ nvm use node 设置为nodejs默认版本1$ nvm alias default node 安装hexo使用npm安装hexo1$ npm install -g hexo 初始化hexo本地博客空间1234$ mkdir GitBlog$ cd GitBlog &amp;&amp; hexo init$ npm install$ npm update -g 运行本地服务器查看博客12$ hexo g # 生成静态界面$ hexo s # 开启本地服务 将博客在github上托管配置本机全局git环境假设github使用的邮箱是hehe@qq.com，github用户名是hehe： 12$ git config -global user.email "hehe@qq.com"$ git config -global user.name "hehe" 生成ssh秘钥先检查一下之前有没有生成过ssh的key: 1$ less ~/.ssh/id_rsa.pub 如果有的话会有输出，则不需要生成秘钥，如果没有执行指令： 1$ ssh-keygen -t rsa -C hehe@qq.com 接着会提示输入文件夹位置来放置ssh秘钥，并会让你确认一个验证的密码，如果不需要可以回车跳过。成功的话会在~/.ssh下生成ssh秘钥，即所谓的公钥id_rsa.pub和私钥id_rsa(RSA加密)。 在github创建博客工程在github用户下新建一个仓库，按照上面的假设，那么我生成的这个仓库名应该是hehe@github.io。然后将之前生成的ssh秘钥添加到github上。 在hexo中配置使用git部署博客在站点的配置文件(博客根目录下的_config.yml)中配置你的git: 1234deploy:type: gitrepo: git@github.com:hehe/hehe.github.io.gitbrach: master 将本地文件上传到github上本地修改好了，可以先在本地运行一下看一下效果： 1$ hexo s 如果没什么问题，接下来将本地内容部署到github服务器上： 123$ hexo clean$ hexo g$ hexo d 有的时候可能要清除一下浏览器的缓存才能显示更新后的内容，具体原因我也不知道。部署完成后，可以直接在浏览器输入hehe@github.io就能看到部署完成的hexo blog了。]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F08%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
