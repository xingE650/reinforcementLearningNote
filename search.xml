<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F28%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[infinite_variance]]></title>
    <url>%2F2018%2F11%2F28%2Finfinite-variance%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter05/infinite_variance.py 通过一个例子论证了ordinary importance sampling的不稳定性 问题描述这个程序通过一个简单的例子证明了ordinary importance sampling 的方差经常会发生不收敛的问题。 本例使用了一个只有一个状态s和两个状态left和right，以及一个terminate state的MDP问题，详细的Reward和转移概率如下所示： 选择的target_policy是：π(left|s)=1,π(right|s)=0； 选择生成episode的behavior policy是:b(left|s)=b(right|s)=0.5 满足π cover b的要求，并根据target_policy可以估计出v_π(s)=1，接下来看看代码运行结果，看看通过behavior policy预测出来的可以收敛到什么情况。 引入模块并定义常量，其中action_back=left，action_end=right12345678import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom tqdm import tqdm %matplotlib inlineACTION_BACK = 0ACTION_END = 1 定义behavior-policy和target-policy并开始训练1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# behavior policydef behavior_policy(): return np.random.binomial(1, 0.5)# target policydef target_policy(): return ACTION_BACK# one turn# 返回reward和action trajectory，因为state已知，所以不用指定def play(): # track the action for importance ratio trajectory = [] while True: action = behavior_policy() trajectory.append(action) if action == ACTION_END: return 0, trajectory if np.random.binomial(1, 0.9) == 0: return 1, trajectorydef figure_5_4(): runs = 10 episodes = 100000 for run in tqdm(range(runs)): # 每轮run之间都是独立的 rewards = [] for episode in range(0, episodes): reward, trajectory = play() if trajectory[-1] == ACTION_END: rho = 0 else: rho = 1.0 / pow(0.5, len(trajectory)) rewards.append(rho * reward) rewards = np.add.accumulate(rewards) estimations = np.asarray(rewards) / np.arange(1, episodes + 1) plt.plot(estimations) plt.plot(np.ones(episodes+1)) plt.xlabel('Episodes (log scale)') plt.ylabel('Ordinary Importance Sampling') plt.xscale('log') plt.ylim(0,2) plt.savefig('./figure_5_4.png') plt.show()figure_5_4() 100%|██████████| 10/10 [00:06&lt;00:00, 1.59it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[blackjack]]></title>
    <url>%2F2018%2F11%2F28%2Fblackjack%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter02/blackjack.py 实现了基于Monte Carlo方法的三种算法：1、基于Monte Carlo方法的策略预测，根据给出的策略计算state-value 2、使用exploring starts的训练方法，得出state-action-value，以及对应的优化policy π 3、使用off-policy的重要取样方法预测state-value，并和期望值对比 但是未实现without exploring starts的 on-policy算法，以及off-policy的Monte Carlo Control方法。 问题描述blackjack是一个常见的赌场游戏，规则如下： 牌组和牌值：总的牌堆由7到8组牌去掉大小王组成，所以不用考虑牌数量问题；其中2-10是原值，J-K当做10使用，A分两种情况，可以等于1 or 11。 规则：游戏开始，发牌的兄弟给玩家和庄家各发2张牌，玩家的牌是亮出来的明牌，庄家的是一张明牌一张暗牌，如果玩家当场数值和到21了(natural)，即拿到了一张A和一张10(或者J-K)，如果庄家没有达到21，那么判庄家输(这里不考虑赌金什么的)，反正则平局；如果玩家没有到21，可以选择继续要牌(hit)或者放弃要牌(strick or stand)，如果超过21了就叫做爆牌(go bust)，玩家就被判负；如果玩家strick1了，就轮到庄家发牌了，庄家一般会按照这样的方法来决策(也可以不这样，这里是一种规定吧)：如果牌值&lt;17，就hit，在17-21之间stand，如果庄家goes bust，庄家就被判负；如果庄家stand了，就比较双方的总牌值，大的一方为胜方。 这里使用policy：玩家hit当牌值&lt;20，20-21就stand。 monte carlo算法本身不难理解，但是这个blackjack问题可以说是目前接触到的最复杂的强化学习问题了。首先是state的理解，state=[usable_ace_player, player_sum, dealer_card1]，其中usable_ace_player指的是player是否将A用作11，player_sum指的是Player牌组总值，dealer_card1指的是庄家亮出的明牌。 算法中使用了很多blackjack游戏的游戏技巧，比如上面提到的一些策略，还有state的选取，增加了理解难度。 所以这个问题主要理解monte carlo算法工作原理，具体的技巧可以选择性忽略。 引入模块，并定义action常量，hit=继续抽，stand=停止1234567891011import numpy as npimport matplotlibimport matplotlib.pyplot as pltimport seaborn as snsfrom tqdm import tqdm%matplotlib inline# actions: hit or standACTION_HIT = 0ACTION_STAND = 1 # "strike" in the bookACTIONS = [ACTION_HIT, ACTION_STAND] 为player定义policy，属于游戏技巧，也是本例的一个参考，通过最终学到的policy和这里的POLICY_PLAYER对比来比较算法的优劣123456# policy for playerPOLICY_PLAYER = np.zeros(22)for i in range(12, 20): POLICY_PLAYER[i] = ACTION_HITPOLICY_PLAYER[20] = ACTION_STANDPOLICY_PLAYER[21] = ACTION_STAND 两个待定函数，这里的target_policy_player和behavior_policy_player的工作方式是理解off-policy Monte Carlo算法的关键1234567891011# use for off-policy method# function form of target policy of playerdef target_policy_player(usable_ace_player, player_sum, dealer_card): return POLICY_PLAYER[player_sum]# function form of behavior policy of playerdef behavior_policy_player(usable_ace_player, player_sum, dealer_card): # probality = 0.5 according to binomial distruction if np.random.binomial(1, 0.5) == 1: return ACTION_STAND return ACTION_HIT 为庄家定义policy，属于游戏技巧123456# policy for dealerPOLICY_DEALER = np.zeros(22)for i in range(12, 17): POLICY_DEALER[i] = ACTION_HITfor i in range(17, 22): POLICY_DEALER[i] = ACTION_STAND Monte Carlo方法的关键一步，通过模拟游戏来获得sample123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157# get a new card# 根据游戏规则发牌并定义牌值def get_card(): card = np.random.randint(1, 14) card = min(card, 10) return card# play a game# @policy_player: 为Player制定的policy，也是训练的目标？# @initial_state: 初始状态[whether player has a usable Ace, sum of player's cards, one card of dealer]# @initial_action: 初始行为 the initial action# return:(state,reward,player_trajectory)# 返回变量解释：# state：就是得到的初始状态，如果initial_state=None，就是随机产生的，反正其实就是initial_state的deep copy# reward：是相应的初始状态的最终结果，Player win=1，Player lost=-1，平局=0# player_trajectory：以[state,action]为元素的list，记录整个实验过程中的state和对应的actiondef play(policy_player, initial_state=None, initial_action=None): # player status # sum of player player_sum = 0 # trajectory of player player_trajectory = [] # whether player uses Ace as 11 usable_ace_player = False # dealer status dealer_card1 = 0 dealer_card2 = 0 usable_ace_dealer = False if initial_state is None: # generate a random initial state num_of_ace = 0 # initialize cards of player # 这里是一个理解的难点，主要是因为对游戏不太理解 # 游戏规定是要一开始给玩家和庄家发2张牌，这里没给初始状态，所以随机抽呗 # 但是问题是，这个循环把Player的牌值一直抽到11才跳出，也就是说其中大概率不止抽了2张，怎么回事？ # 因为反正发完牌也是让Player先搞，而且前面按照给的经验玩法，在12到20之间都是无脑hit的，所以直接在这抽到12算了。 while player_sum &lt; 12: # for _ in range(2): # if sum of player is less than 12, always hit card = get_card() # if get an Ace, use it as 11 if card == 1: num_of_ace += 1 card = 11 usable_ace_player = True player_sum += card # if player's sum is larger than 21, he must hold at least one Ace, two Aces are possible # 这里的理解也很有意思，就是如果超过21了，那么考虑前一个while循环，数值肯定是&lt;=11，而最后一次抽最大是11(A)，所以超过21 # 只有一种情况，就是前一次11，这次又抽到了A，所以肯定至少有一个A，当然两个A也是有可能的。 if player_sum &gt; 21: # use the Ace as 1 rather than 11 player_sum -= 10 # if the player only has one Ace, then he doesn't have usable Ace any more if num_of_ace == 1: usable_ace_player = False # initialize cards of dealer, suppose dealer will show the first card he gets # 这里也验证了前面的推测，因为下一次不论到庄家抽卡，所以庄家这里就老老实实抽了2张 # 不过其中card1是可见的，card2是暗牌看不到，所以card1也是会影响决策的(怎么影响就是游戏经验了) dealer_card1 = get_card() dealer_card2 = get_card() else: # use specified initial state usable_ace_player, player_sum, dealer_card1 = initial_state dealer_card2 = get_card() # initial state of the game state = [usable_ace_player, player_sum, dealer_card1] # initialize dealer's sum # 计算得到庄家的牌值总值，当然这个对玩家是不可见的。 dealer_sum = 0 if dealer_card1 == 1 and dealer_card2 != 1: dealer_sum += 11 + dealer_card2 usable_ace_dealer = True elif dealer_card1 != 1 and dealer_card2 == 1: dealer_sum += dealer_card1 + 11 usable_ace_dealer = True elif dealer_card1 == 1 and dealer_card2 == 1: dealer_sum += 1 + 11 usable_ace_dealer = True else: dealer_sum += dealer_card1 + dealer_card2 # game starts! # player's turn while True: # 第一次会对玩家的行为套用初始action，以后不会用了 if initial_action is not None: action = initial_action initial_action = None else: # get action based on current sum # 写了这么变量，其实就是根据state和给定的policy给出action action = policy_player(usable_ace_player, player_sum, dealer_card1) # track player's trajectory for importance sampling # 将state-action对append进player_trajectory player_trajectory.append([(usable_ace_player, player_sum, dealer_card1), action]) if action == ACTION_STAND: break # if hit, get new card player_sum += get_card() # player busts if player_sum &gt; 21: # if player has a usable Ace, use it as 1 to avoid busting and continue if usable_ace_player == True: player_sum -= 10 usable_ace_player = False else: # otherwise player loses return state, -1, player_trajectory # dealer's turn while True: # get action based on current sum action = POLICY_DEALER[dealer_sum] if action == ACTION_STAND: break # if hit, get a new card new_card = get_card() if new_card == 1 and dealer_sum + 11 &lt; 21: dealer_sum += 11 usable_ace_dealer = True else: dealer_sum += new_card # dealer busts if dealer_sum &gt; 21: if usable_ace_dealer == True: # if dealer has a usable Ace, use it as 1 to avoid busting and continue dealer_sum -= 10 usable_ace_dealer = False else: # otherwise dealer loses return state, 1, player_trajectory # compare the sum between player and dealer if player_sum &gt; dealer_sum: return state, 1, player_trajectory elif player_sum == dealer_sum: return state, 0, player_trajectory else: return state, -1, player_trajectory 使用on-policy策略训(yu)练(ce)算法伪代码： 其实给的target_policy才是last boss，这里只是使用Monte Carlo的方法来预测了一下，看看结果是不是符合的。 这里有一个比较有意思的地方感觉可以讨论一下： 算法的最后一部分，也就是循环的最小部分，即对value的更新，因为blackjack问题本身就是state不重复的，所以first-visit和every-visit是一样的；其次就是G，这个G是被累加了，因为这里S_t是按照索引减小的方向移动的，而且注意符号，这里累加了R_{t+1}，这个形式和上一章的MDP问题中的表达式是一致的，关于这里R的索引是n还是n+1，我觉得也有必要提一下。因为在第二章Bandit问题里，R是用的n： 而且这里对value的更新也是从前往后的，state是有限的； 而第4章DP的时候，R使用的就变成n+1了： 而且也变成反向更新了（虽然形式上仍是正向写的code，但是事实上最先确定的是最终的state-value） 仔细看的话会发现，其实这两个问题还是差别挺大的，首先说下bandit问题把。Bandit问题是通过学习找到最优的平均收益，所以n控制的试验次数，所以其实本质上它只计算了一个value值，就是最终目标是为了收敛到最优的action，即找到reward期望最大的action。 MDP问题中的t代表的是state随step出现的时刻，即使一般问题可能不满足MDP的马尔科夫性，但是对于一个普遍的多状态序列决策问题，t时刻的state对应的value肯定是其后时刻states的value的一种和的形式(通常会考虑discount如果问题是continuing task)。所以两个问题本质就是不同的，不能混淆。 再谈谈另一个问题，就是伪算法最里层计算state-value的时候，计算顺序是沿着t减小的方向的，因为这样有利于迭代，其实也是一种DP的思路。但是这个blackjack问题有它的特殊性，就是每一步action的reward是设为0的，只有最后的结果才是有reward的。这样设计是有道理的，避免达到次优点嘛。 看到这里我不禁想到，Monte Carlo方法和MDP其实原理是相通的，都利用的DP的思路来解决问题，我记得在MDP那一章也讲过这个问题，在原书46页： 123456789101112131415161718192021222324252627282930# Monte Carlo Sample with On-Policydef monte_carlo_on_policy(episodes): states_usable_ace = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided states_usable_ace_count = np.ones((10, 10)) states_no_usable_ace = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided states_no_usable_ace_count = np.ones((10, 10)) for i in tqdm(range(0, episodes)): _, reward, player_trajectory = play(target_policy_player) for (usable_ace, player_sum, dealer_card), _ in player_trajectory: # 这里也是一个理解的难点，一点一点分析： # 1、“player_sum-=12”这个操作，通过play函数我发现，append进player_trajectory # 中的state的player_sum一定是&lt;=21的，即state中的action是在player_sum基础上的，player_sum # 的值是在action之前的值。那么可以知道player_sum-12&lt;9 # 接着分析，-12的操作结果是否会向下溢出，根据play函数中的处理，player_sum一定是从&gt;=12的值开始的， # 这一点可以从play函数一开始的初始状态的处理看出来，while循环只有当player_sum&gt;=12才会跳出 # 所以player_sum-12 ~ [0,9]，刚好符合states_usable_ace等4个np.ndarray的范围 # 同理，dealer_card-1也是一样的道理，将[1,10]的范围换算到[0,9] # 那dealer_card就不可能是11吗？根据play函数的分析，dealer_card在计算dealer_sum的时候 # 需要根据另一张暗牌的牌值来计算，所以对于player来说只能认为它是1（暗牌不可见） player_sum -= 12 dealer_card -= 1 if usable_ace: states_usable_ace_count[player_sum, dealer_card] += 1 states_usable_ace[player_sum, dealer_card] += reward else: states_no_usable_ace_count[player_sum, dealer_card] += 1 states_no_usable_ace[player_sum, dealer_card] += reward return states_usable_ace / states_usable_ace_count, states_no_usable_ace / states_no_usable_ace_count 绘制图表1234567891011121314151617181920212223242526272829def figure_5_1(): states_usable_ace_1, states_no_usable_ace_1 = monte_carlo_on_policy(10000) states_usable_ace_2, states_no_usable_ace_2 = monte_carlo_on_policy(500000) states = [states_usable_ace_1, states_usable_ace_2, states_no_usable_ace_1, states_no_usable_ace_2] titles = ['Usable Ace, 10000 Episodes', 'Usable Ace, 500000 Episodes', 'No Usable Ace, 10000 Episodes', 'No Usable Ace, 500000 Episodes'] _, axes = plt.subplots(2, 2, figsize=(40, 30)) plt.subplots_adjust(wspace=0.1, hspace=0.2) axes = axes.flatten() for state, title, axis in zip(states, titles, axes): fig = sns.heatmap(np.flipud(state), cmap="YlGnBu", ax=axis, xticklabels=range(1, 11), yticklabels=list(reversed(range(12, 22)))) fig.set_ylabel('player sum', fontsize=30) fig.set_xlabel('dealer showing', fontsize=30) fig.set_title(title, fontsize=30) plt.savefig('./figure_5_1.png') plt.show() figure_5_1() 100%|██████████| 10000/10000 [00:00&lt;00:00, 51795.15it/s] 100%|██████████| 500000/500000 [00:08&lt;00:00, 55705.87it/s] 使用exploring-start训练算法的伪代码如下： 注意其中几个要点： 在Monte Carlo模拟的时候，即play函数里，使用的player_policy是greey的； 初始状态是explore的，并保证每个初始state和action组合都有概率出现 1234567891011121314151617181920212223242526272829303132333435363738394041# Monte Carlo with Exploring Startsdef monte_carlo_es(episodes): # (playerSum, dealerCard, usableAce, action) state_action_values = np.zeros((10, 10, 2, 2)) # initialze counts to 1 to avoid division by 0 state_action_pair_count = np.ones((10, 10, 2, 2)) # behavior policy is greedy def behavior_policy(usable_ace, player_sum, dealer_card): usable_ace = int(usable_ace) player_sum -= 12 dealer_card -= 1 # get argmax of the average returns(s, a) values_ = state_action_values[player_sum, dealer_card, usable_ace, :] / \ state_action_pair_count[player_sum, dealer_card, usable_ace, :] # np.random.choice 函数通过给定一个list或者np.ndarray或者整数n，以及一个表示选择概率的list来从一系列数中 # 按照概率选出相应的值 # 如果第一个参数是n，其实是相当于np.arange(n)的，第二个概率如果不指定就默认uniform distribution(均匀分布) # 这里是从最大的value的action中选出任一个的 return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) # play for several episodes for episode in tqdm(range(episodes)): # for each episode, use a randomly initialized state and action # 使用exploring start-action对 initial_state = [bool(np.random.choice([0, 1])), np.random.choice(range(12, 22)), np.random.choice(range(1, 11))] initial_action = np.random.choice(ACTIONS) # 这个地方挺有意思的，第一次先使用target_policy，免得state_action_value一直是0，陷入死循环 current_policy = behavior_policy if episode else target_policy_player _, reward, trajectory = play(current_policy, initial_state, initial_action) for (usable_ace, player_sum, dealer_card), action in trajectory: usable_ace = int(usable_ace) player_sum -= 12 dealer_card -= 1 # update values of state-action pairs state_action_values[player_sum, dealer_card, usable_ace, action] += reward state_action_pair_count[player_sum, dealer_card, usable_ace, action] += 1 return state_action_values / state_action_pair_count 绘制图表1234567891011121314151617181920212223242526272829303132333435def figure_5_2(): state_action_values = monte_carlo_es(500000) state_value_no_usable_ace = np.max(state_action_values[:, :, 0, :], axis=-1) state_value_usable_ace = np.max(state_action_values[:, :, 1, :], axis=-1) # get the optimal policy action_no_usable_ace = np.argmax(state_action_values[:, :, 0, :], axis=-1) action_usable_ace = np.argmax(state_action_values[:, :, 1, :], axis=-1) images = [action_usable_ace, state_value_usable_ace, action_no_usable_ace, state_value_no_usable_ace] titles = ['Optimal policy with usable Ace', 'Optimal value with usable Ace', 'Optimal policy without usable Ace', 'Optimal value without usable Ace'] _, axes = plt.subplots(2, 2, figsize=(40, 30)) plt.subplots_adjust(wspace=0.1, hspace=0.2) axes = axes.flatten() for image, title, axis in zip(images, titles, axes): fig = sns.heatmap(np.flipud(image), cmap="YlGnBu", ax=axis, xticklabels=range(1, 11), yticklabels=list(reversed(range(12, 22)))) fig.set_ylabel('player sum', fontsize=30) fig.set_xlabel('dealer showing', fontsize=30) fig.set_title(title, fontsize=30) plt.savefig('./figure_5_2.png') plt.show() figure_5_2() 100%|██████████| 500000/500000 [00:31&lt;00:00, 15708.47it/s] 使用off-policy策略训练(预测)通过计算Ordinary Importance Sampling 和 Weighted Importance Sampling并和target_policy训练的结果比对，分别计算均方误差来测试算法的性能。 重点是两个重要取样的意义和计算： 重要取样(Importance Sampling)的推导是通过MDP得到的，它是一种off-policy方法中通过behavior policy来训练target policy的重要途径，其中一个重要的概念就是重要取样比率(importance-sampling ratio)，它的定义式是target policy下的状态轨迹的π(a|s)和behavior policy下的状态轨迹的b(a|s)的比值。具体推导如下： 首先定义通过behavior policy取样得到的states-chain的联合概率，这里的进一步计算使用了马尔科夫独立性假设： 然后给出重要取样比率的定义： 其中的状态转移概率p被消去了，可以证明这个结论虽然是通过马尔科夫独立性推出的，却具有一般性。 好了，现在我们基本理解了重要采样比率的概念，那么它到底有什么用那？下面的公式解释了这个问题（具体推导不清楚QAQ）： 这个值是联系episode的return和target policy π下的v_π(s)的关键！ 所以很直观的从这个期望公式引出ordinary importance sampling： 但是有一个问题，就是如果重要采样因子有可能是方差无限的，这时我们近似得到的state-value就会发散而无法收敛，这个问题很严重，随后也会通过代码来解释。 所以通过对采样因子做归一化，引出了Weighted importance sampling的概念： 和前者相比，虽然weighted importance sampling是有偏的(bias)，因为它不是直接从上面的期望公式来的，但是经过多次迭代bias会趋于0。但是如果采样因子的方差不是有限的，ordinary importance sampling就很有可能无法收敛，也就是说它的方差(variance)是高于weighted importance sampling的，所以总的来说后者更常用。 12345678910111213141516171819202122232425262728293031323334353637# Monte Carlo Sample with Off-Policydef monte_carlo_off_policy(episodes): # 不用exploring start了，所以可以直接指定起始状态 initial_state = [True, 13, 2] rhos = [] returns = [] for i in range(0, episodes): _, reward, player_trajectory = play(behavior_policy_player, initial_state=initial_state) # get the importance ratio numerator = 1.0 denominator = 1.0 for (usable_ace, player_sum, dealer_card), action in player_trajectory: if action == target_policy_player(usable_ace, player_sum, dealer_card): denominator *= 0.5 else: numerator = 0.0 break rho = numerator / denominator rhos.append(rho) returns.append(reward) rhos = np.asarray(rhos) returns = np.asarray(returns) weighted_returns = rhos * returns weighted_returns = np.add.accumulate(weighted_returns) rhos = np.add.accumulate(rhos) ordinary_sampling = weighted_returns / np.arange(1, episodes + 1) with np.errstate(divide='ignore',invalid='ignore'): weighted_sampling = np.where(rhos != 0, weighted_returns / rhos, 0) return ordinary_sampling, weighted_sampling 绘制图表12345678910111213141516171819202122232425def figure_5_3(): true_value = -0.27726 episodes = 10000 runs = 100 error_ordinary = np.zeros(episodes) error_weighted = np.zeros(episodes) for i in tqdm(range(0, runs)): ordinary_sampling_, weighted_sampling_ = monte_carlo_off_policy(episodes) # get the squared error error_ordinary += np.power(ordinary_sampling_ - true_value, 2) error_weighted += np.power(weighted_sampling_ - true_value, 2) error_ordinary /= runs error_weighted /= runs plt.plot(error_ordinary, label='Ordinary Importance Sampling') plt.plot(error_weighted, label='Weighted Importance Sampling') plt.xlabel('Episodes (log scale)') plt.ylabel('Mean square error') plt.xscale('log') plt.legend() plt.savefig('./figure_5_3.png') plt.show() figure_5_3() 100%|██████████| 100/100 [00:20&lt;00:00, 4.87it/s]]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Monte Carlo method]]></title>
    <url>%2F2018%2F11%2F28%2FMonte-Carlo-method%2F</url>
    <content type="text"><![CDATA[这里需要注意几点： 关于target policy π cover behavior policy b的概念，这里什么才能被称为cover(覆盖)那？ 文中解释是这样的：通过π选取的action一定有概率通过b选取，即通过π(a|s)&gt;0一定可以推出b(a|s)&gt;0，前者是后者的充分条件。 还有就是这里提一下off-policy和on-policy之间的关系： 前者是通过behavior policy来模拟并产生数据(episode)，但是学习得到的是target policy π，即使用的episode is “off” the target policy π； 后者则模拟数据和学习都是同一个policy。一般来说on-policy更简单易用，off-policy则会有更多的参数和更复杂的学习过程，收敛起来也比较慢，但是off-policy可以适的范围更广，可以更好的解决问题。 这里其实也是对已知策略的预测，并通过计算value的均方误差来测试算法性能，并没有使用到target-policy来训练。]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[use-DP-improve-MDP-3-exercises]]></title>
    <url>%2F2018%2F11%2F23%2Fuse-DP-improve-MDP-3-exercises%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter04/gird_world.py、chapter04/car_rental.py、chapter04/gamblers_problem.py 书中第四章3个例子，分别是：4.1 Policy Evaluation : Gird-world problem 4.2 Policy Iteration : Car rental problem 4.3 Value Iteration : Gambler problem 一、grid_world(policy evaluation)问题描述4X4 网格： 左上角和右下角是终止状态(terminal state)，如果action使得state跳转到外面了，就返回上次位置，所有的action造成的reward都是-1。 引入模块并定义常量1234567891011121314import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom matplotlib.table import Table%matplotlib inlineWORLD_SIZE = 4# left, up, right, downACTIONS = [np.array([0, -1]), np.array([-1, 0]), np.array([0, 1]), np.array([1, 0])]ACTION_PROB = 0.25 action控制代码12345678910111213141516# judge whether comes to terminal statedef is_terminal(state): x, y = state return (x == 0 and y == 0) or (x == WORLD_SIZE - 1 and y == WORLD_SIZE - 1)# return the next_state s' and reward rdef step(state, action): state = np.array(state) next_state = (state + action).tolist() x, y = next_state if x &lt; 0 or x &gt;= WORLD_SIZE or y &lt; 0 or y &gt;= WORLD_SIZE: next_state = state.tolist() reward = -1 return next_state, reward 绘制方格图123456789101112131415161718192021222324252627def draw_image(image): fig, ax = plt.subplots() ax.set_axis_off() tb = Table(ax, bbox=[0, 0, 1, 1]) nrows, ncols = image.shape width, height = 1.0 / ncols, 1.0 / nrows # Add cells for (i,j), val in np.ndenumerate(image): # Index either the first or second item of bkg_colors based on # a checker board pattern idx = [j % 2, (j + 1) % 2][i % 2] color = 'white' tb.add_cell(i, j, width, height, text=val, loc='center', facecolor=color) # Row Labels... for i, label in enumerate(range(len(image))): tb.add_cell(i, -1, width, height, text=label+1, loc='right', edgecolor='none', facecolor='none') # Column Labels... for j, label in enumerate(range(len(image))): tb.add_cell(-1, j, width, height/2, text=label+1, loc='center', edgecolor='none', facecolor='none') ax.add_table(tb) 进行policy evaluation，即计算value-state123456789101112131415161718192021222324def compute_state_value(in_place=False): new_state_values = np.zeros((WORLD_SIZE, WORLD_SIZE)) state_values = new_state_values.copy() iteration = 1 while True: # in place algorithm is faster than 2-array edition src = new_state_values if in_place else state_values for i in range(WORLD_SIZE): for j in range(WORLD_SIZE): if is_terminal([i, j]): continue value = 0 for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) value += ACTION_PROB * (reward + src[next_i, next_j]) new_state_values[i, j] = value if np.sum(np.abs(new_state_values - state_values)) &lt; 1e-4: state_values = new_state_values.copy() break state_values = new_state_values.copy() iteration += 1 return state_values, iteration 运行并显示评估结果123456789101112def figure_4_1(): values, sync_iteration = compute_state_value(in_place=False) _, asycn_iteration = compute_state_value(in_place=True) # show the sync DP evaluation draw_image(np.round(values, decimals=2)) print('In-place: %d iterations' % (asycn_iteration)) print('Synchronous: %d iterations' % (sync_iteration)) plt.savefig('./figure_4_1.png') plt.show()figure_4_1() In-place: 142 iterations Synchronous: 218 iterations 二、car rental 问题问题描述这个问题就比较复杂了。。。说是Jack是两个汽车租赁公司的老板，他收入是靠租车出去，租出一辆赚10刀，每次有人还车，那么第二天这车就可以租出去了；每天夜里可以将一个地方的车运到另一个地方，不过每运一辆车要花2刀。关于业务，Jack发现了一些斯巴拉西的规律：每个地方每天汽车租借和归还的数量都遵循泊松分布： 我们就把两个位置称为first和second吧。 first的汽车每天借出去λ=3，归还λ=3； second每天借出去λ=4，归还λ=2； 并且每个地方汽车库存不能超过20辆，超过了总公司就会回收； 夜里从一个地方运到另一个地方的汽车数量不能超过5辆。 这个问题我们把它设计成discount因子=0.9的MDP问题，step是每一天，action是每晚运的车，并设从first运到second为正，从second运到first为负，state是first和second可以租赁车的总数量。同时做一个简化，就是如果泊松分布n&gt;10，就把概率人为截断为0。 引入模块并定义常量123456789101112131415161718192021222324252627282930313233343536373839import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom math import exp, factorialimport seaborn as sns%matplotlib inline# maximum # of cars in each locationMAX_CARS = 20# maximum # of cars to move during nightMAX_MOVE_OF_CARS = 5# expectation for rental requests in first locationRENTAL_REQUEST_FIRST_LOC = 3# expectation for rental requests in second locationRENTAL_REQUEST_SECOND_LOC = 4# expectation for # of cars returned in first locationRETURNS_FIRST_LOC = 3# expectation for # of cars returned in second locationRETURNS_SECOND_LOC = 2DISCOUNT = 0.9# credit earned by a carRENTAL_CREDIT = 10# cost of moving a carMOVE_CAR_COST = 2# all possible actionsactions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)# An up bound for poisson distribution# If n is greater than this value, then the probability of getting n is truncated to 0POISSON_UPPER_BOUND = 11 进行policy evaluation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Probability for poisson distribution# @lam: lambda should be less than 10 for this functionpoisson_cache = dict()def poisson(n, lam): global poisson_cache key = n * 10 + lam if key not in poisson_cache.keys(): poisson_cache[key] = exp(-lam) * pow(lam, n) / factorial(n) return poisson_cache[key]# @state: [# of cars in first location, # of cars in second location]# @action: positive if moving cars from first location to second location,# negative if moving cars from second location to first location# @stateValue: state value matrix# @constant_returned_cars: if set True, model is simplified such that# the # of cars returned in daytime becomes constant# rather than a random value from poisson distribution, which will reduce calculation time# and leave the optimal policy/value state matrix almost the samedef expected_return(state, action, state_value, constant_returned_cars): # initailize total return returns = 0.0 # cost for moving cars returns -= MOVE_CAR_COST * abs(action) # go through all possible rental requests for rental_request_first_loc in range(0, POISSON_UPPER_BOUND): for rental_request_second_loc in range(0, POISSON_UPPER_BOUND): # moving cars num_of_cars_first_loc = int(min(state[0] - action, MAX_CARS)) num_of_cars_second_loc = int(min(state[1] + action, MAX_CARS)) # valid rental requests should be less than actual # of cars real_rental_first_loc = min(num_of_cars_first_loc, rental_request_first_loc) real_rental_second_loc = min(num_of_cars_second_loc, rental_request_second_loc) # get credits for renting reward = (real_rental_first_loc + real_rental_second_loc) * RENTAL_CREDIT num_of_cars_first_loc -= real_rental_first_loc num_of_cars_second_loc -= real_rental_second_loc # probability for current combination of rental requests # possion(n,lam) # P(AB) = P(A)*P(B) prob = poisson(rental_request_first_loc, RENTAL_REQUEST_FIRST_LOC) * \ poisson(rental_request_second_loc, RENTAL_REQUEST_SECOND_LOC) if constant_returned_cars: # get returned cars, those cars can be used for renting tomorrow returned_cars_first_loc = RETURNS_FIRST_LOC returned_cars_second_loc = RETURNS_SECOND_LOC num_of_cars_first_loc = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS) num_of_cars_second_loc = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS) returns += prob * (reward + DISCOUNT * state_value[num_of_cars_first_loc, num_of_cars_second_loc]) else: for returned_cars_first_loc in range(0, POISSON_UPPER_BOUND): for returned_cars_second_loc in range(0, POISSON_UPPER_BOUND): num_of_cars_first_loc_ = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS) num_of_cars_second_loc_ = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS) prob_ = poisson(returned_cars_first_loc, RETURNS_FIRST_LOC) * \ poisson(returned_cars_second_loc, RETURNS_SECOND_LOC) * prob returns += prob_ * (reward + DISCOUNT * state_value[num_of_cars_first_loc_, num_of_cars_second_loc_]) return returns 进行policy iteration123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def figure_4_2(constant_returned_cars=True): value = np.zeros((MAX_CARS + 1, MAX_CARS + 1)) policy = np.zeros(value.shape, dtype=np.int) iterations = 0 _, axes = plt.subplots(2, 3, figsize=(40, 20)) plt.subplots_adjust(wspace=0.1, hspace=0.2) axes = axes.flatten() while True: fig = sns.heatmap(np.flipud(policy), cmap="YlGnBu", ax=axes[iterations]) fig.set_ylabel('# cars at first location', fontsize=30) fig.set_yticks(list(reversed(range(MAX_CARS + 1)))) fig.set_xlabel('# cars at second location', fontsize=30) fig.set_title('policy %d' % (iterations), fontsize=30) # policy evaluation (in-place) while True: new_value = np.copy(value) for i in range(MAX_CARS + 1): for j in range(MAX_CARS + 1): new_value[i, j] = expected_return([i, j], policy[i, j], new_value, constant_returned_cars) value_change = np.abs((new_value - value)).sum() print('value change %f' % (value_change)) value = new_value if value_change &lt; 1e-4: break # policy improvement new_policy = np.copy(policy) for i in range(MAX_CARS + 1): for j in range(MAX_CARS + 1): action_returns = [] for action in actions: if (action &gt;= 0 and i &gt;= action) or (action &lt; 0 and j &gt;= abs(action)): action_returns.append(expected_return([i, j], action, value, constant_returned_cars)) else: action_returns.append(-float('inf')) new_policy[i, j] = actions[np.argmax(action_returns)] policy_change = (new_policy != policy).sum() print('policy changed in %d states' % (policy_change)) policy = new_policy if policy_change == 0: fig = sns.heatmap(np.flipud(value), cmap="YlGnBu", ax=axes[-1]) fig.set_ylabel('# cars at first location', fontsize=30) fig.set_yticks(list(reversed(range(MAX_CARS + 1)))) fig.set_xlabel('# cars at second location', fontsize=30) fig.set_title('optimal value', fontsize=30) break iterations += 1 plt.savefig('./figure_4_2.png') plt.show() # figure_4_2() 注意图片，总共经过4次迭代最终收敛，前5个图片是policy的温度分布图，最后一个是value的分布图。还有一点需要注意，就是这个任务没有终止状态，所以是属于continuing task，discount因子&lt;1，而本章第一个和第三个都是有终止状态的，所以是undiscount的。三、赌徒问题问题描述一个赌徒可以在每轮赌博中决定将自己手里的钱拿来赌硬币的正反，如果硬币向上，则可以获得押金一样的奖励，但是向下的话押金就没了。结束条件是赌徒手里的钱增长到100，或者把钱输光。 这个问题可以定义为state为赌徒手里的钱，action为每次拿去赌的钱，discount=1的MDP问题。 引入模块并定义全局变量12345678910111213import numpy as npimport matplotlibimport matplotlib.pyplot as plt%matplotlib inline# goalGOAL = 100# all states, including state 0 and state 100STATES = np.arange(GOAL + 1)# probability of head, which is the probability win moneyHEAD_PROB = 0.4 Value Iteration需要注意几点： 初始化value-state的时候，除了100的状态为1，其余都为0，可以理解为除了到达100可以获得reward=1，其余action对应reward=0，即利用value-state initialize来实现reward。 训练的时候把action=0去掉，是因为aciton=0会导致agent陷入局部最优，所以需要跳出这个点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def figure_4_3(): # state value initialize state_value = np.zeros(GOAL + 1) state_value[GOAL] = 1.0 # value iteration while True: delta = 0.0 for state in STATES[1:GOAL]: # get possilbe actions for current state actions = np.arange(1,min(state, GOAL - state) + 1) action_returns = [] for action in actions: action_returns.append( HEAD_PROB * state_value[state + action] + (1 - HEAD_PROB) * state_value[state - action]) new_value = np.max(action_returns) delta += np.abs(state_value[state] - new_value) # update state value state_value[state] = new_value if delta &lt; 1e-9: break # compute the optimal policy policy = np.zeros(GOAL + 1) for state in STATES[1:GOAL]: actions = np.arange(1,min(state, GOAL - state) + 1) action_returns = [] for action in actions: action_returns.append( HEAD_PROB * state_value[state + action] + (1 - HEAD_PROB) * state_value[state - action]) # round to resemble the figure in the book, see # https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/issues/83# policy[state] = actions[np.argmax(np.round(action_returns[1:], 5)) + 1] policy[state] = actions[np.argmax(np.round(action_returns,5))] plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) plt.plot(state_value) plt.xlabel('Capital') plt.ylabel('Value estimates') plt.subplot(2, 1, 2) plt.scatter(STATES, policy) plt.xlabel('Capital') plt.ylabel('Final policy (stake)') plt.savefig('./figure_4_3.png') plt.show()figure_4_3() 中奖率=0.4 中奖率=0.1 中奖率=0.8(所以说中奖率太高也不能浪吗。。。)]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[use-DP-improve-MDP]]></title>
    <url>%2F2018%2F11%2F23%2Fuse-DP-improve-MDP%2F</url>
    <content type="text"><![CDATA[上一章讲了马尔科夫决策过程的概念，末尾提出了针对MDP的value-state、value-action-state建立方法，很明显可以看出是使用了动态规划的方法，这一章就在上一章基础上，进一步讲述如何使用动态规划来训练以及优化MDP问题的强化学习算法。 4.1 Policy Evaluation首先理解什么是策略的评估策略的评估也可以认为是一种预测行为，是解决MDP问题的必要环节。通过评估，我们使用已有的policy选择action（大部分问题policy是一个随机函数，即按照一定的概率分布产生action），使用动态规划的方法更新整个state-set的value并达到收敛。评估结果即value-table，是我们决策的重要依据，所以可以为未知的player如何行动提供预测的参考。 评估模型经常使用迭代运算的方式，迭代同时分为原地迭代和旧值迭代，区别仅是在动态规划使用未来状态来更新当前value的时候，是否使用更新后的值，运算通式是： 评估算法的伪代码： 4.2 Policy Improvement一开始看的时候，一直没明白上一步评估的作用何在，在策略提升这部分就讲了，策略评估是为了改进用的，因为如果在某个state s上，我改变了action a-&gt;a’，那么可以认为我采用了一种新的策略π’，即这一步我采取的行为是π’(s)，如果有： 则可以推出新的策略π’一定不差于原来的π，即： 推导过程： 顺势我们推出一种greedy的提升方法，就是直接获得最大reward的action： 这种方法也为下一部分的Policy迭代优化提供一种优化的思路。 4.3 Policy Iteration直接给出迭代优化的伪代码： 可以看出来算法其实就是评估和提升交替进行的，迭代终止条件就是没办法对当前的v_π(s)进一步优化了。 4.4 Value Iteration上面的策略迭代优化的方法很明显有个问题，就是每次提升的前提是需要对策略进行评估，value Iteration提出一种将提升和评估放在一起进行的方法，这个思路比较像4.1中评估的时候使用的in-place方法。 算法伪代码如下： 算法结束条件就是当value-table更新幅度小于阈值Θ时停止。 4.5 4.6 4.7因为后面三部分都很短，也没有给出具体的解释，所以我就放一块写。 4.5讲的是异步动态规划，其实前面讲的value iteration就是异步动态规划的一种，主要是希望可以改进DP算法会遍历整个state set的问题，有的value state没必要多次更新，而有的可以多次更新，具体的算法会在第8章提出。 4.6讲的是统一的策略迭代(GPI)，意指evaluation和improvement是相互竞争合作的，大部分强化学习都是这两者相互作用达到最优的policy和state-value。 4.7讲的是动态规划算法的效率，大致意思就是动规对于large-state的问题计算量仍旧很大，但是可以用异步动规来解决，policy iteration和value iteration现在仍很常用。]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mdp-example-grid-train]]></title>
    <url>%2F2018%2F11%2F22%2Fmdp-example-grid-train%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter03/gird_world.py 使用MDP的强化学习算法解决Grid-world问题任务解释(example 3.5 in chapter 03)grid_world代表了一个网格，网格中每个小格子代表一种状态。其中每个状态可以有4种action:left、right、up、down。对应reward规则如下： 如果action导致agent跑到网格外面去，则reward=-1； 如果agent从A出发，则reward=10，下个状态是固定的A_PRIME_POS；从B出发，reward=5，下个状态时固定的B_PRIME_POS； 其他的state上的action均为0。 示意图如下： 引入模块，定义常量12345678910111213141516171819import numpy as npimport matplotlibimport matplotlib.pyplot as pltfrom matplotlib.table import Table%matplotlib inlineWORLD_SIZE = 5A_POS = [0, 1]A_PRIME_POS = [4, 1]B_POS = [0, 3]B_PRIME_POS = [2, 3]DISCOUNT = 0.9# left, up, right, downACTIONS = [np.array([0, -1]), np.array([-1, 0]), np.array([0, 1]), np.array([1, 0])]ACTION_PROB = 0.25 获取next_state（base on action），和对应的reward123456789101112131415def step(state, action): if state == A_POS: return A_PRIME_POS, 10 if state == B_POS: return B_PRIME_POS, 5 state = np.array(state) next_state = (state + action).tolist() x, y = next_state if x &lt; 0 or x &gt;= WORLD_SIZE or y &lt; 0 or y &gt;= WORLD_SIZE: reward = -1.0 next_state = state else: reward = 0 return next_state, reward 根据给定的np.array数据类型绘制方格图1234567891011121314151617181920212223242526272829303132def draw_image(image): fig, ax = plt.subplots() ax.set_axis_off() tb = Table(ax, bbox=[0, 0, 1, 1]) nrows, ncols = image.shape width, height = 1.0 / ncols, 1.0 / nrows # Add cells # np.ndenumerate() return an iterator yielding pairs of array coordinates and values. for (i,j), val in np.ndenumerate(image): # Index either the first or second item of bkg_colors based on # a checker board pattern idx = [j % 2, (j + 1) % 2][i % 2] color = 'white' tb.add_cell(i, j, width, height, text=val, loc='center', facecolor=color) # Row Labels... for i, label in enumerate(range(len(image))): tb.add_cell(i, -1, width, height, text=label+1, loc='right', edgecolor='none', facecolor='none') # Column Labels... for j, label in enumerate(range(len(image))): tb.add_cell(-1, j, width, height/2, text=label+1, loc='center', edgecolor='none', facecolor='none') ax.add_table(tb) # test draw_imagetest_image = np.array([[1.2,2.1],[3.5,4.7]])draw_image(test_image) Policies and Value Functions123456789101112131415161718192021def figure_3_2(): value = np.zeros((WORLD_SIZE, WORLD_SIZE)) while True: # keep iteration until convergence new_value = np.zeros(value.shape) for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) # bellman equation # each action is random chosen new_value[i, j] += ACTION_PROB * (reward + DISCOUNT * value[next_i, next_j]) if np.sum(np.abs(value - new_value)) &lt; 1e-4: draw_image(np.round(new_value, decimals=2)) plt.savefig('./figure_3_2.png') plt.show() plt.close() break value = new_valuefigure_3_2(); Optimal Policies and Value Functions(Greedy choose action)123456789101112131415161718192021def figure_3_5(): value = np.zeros((WORLD_SIZE, WORLD_SIZE)) while True: # keep iteration until convergence new_value = np.zeros(value.shape) for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): values = [] for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) # value iteration values.append(reward + DISCOUNT * value[next_i, next_j]) new_value[i, j] = np.max(values) if np.sum(np.abs(new_value - value)) &lt; 1e-4: draw_image(np.round(new_value, decimals=2)) plt.savefig('./figure_3_5.png') plt.show() plt.close() break value = new_valuefigure_3_5() 后记这是关于马尔科夫决策过程一个相对简单的过程，可以看到这里一旦确定了previous state和action，current state和对应的reward就确定了。即Pr{s’,r|s,a}=1，事实上MDP的应用包括优化过程都是有相当的局限性的： （1）首先我们需要准确的知道环境的动态变化； （2）我们要有足够的算力来推出所有需要的state和value； （3）我们需要保证问题的马尔科夫性。 然而这里的（2）则很大程度上局限了MDP的推广，因为事实上如果需要在所有的state上做出action并update value，对于state数量庞大的任务几乎不可能完成的。]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mdp-learning]]></title>
    <url>%2F2018%2F11%2F22%2Fmdp-learning%2F</url>
    <content type="text"><![CDATA[马尔可夫决策过程(Markov Decision Process, MDP)，是指具有马尔可夫性的一类强化学习问题，即系统下个状态和当前的状态有关，以及当前采取的动作有关。 some notation in MDP1.state : S_t 2.action: A_t 3.reward: R_t 4.a probability of those values occurring at time t, given particular values of the preceding state and action: 上述概率对s’和r分别累加=1 5.state-transition probability 6.expect reward 7.expected reward for state-action-next-state return of episode tasks and continuing taskswhat is return of task?if we define each step reward as R_i(i=0,1,2,3…), then expect return is defined as some specific function of the reward sequence. what is episode task?Such as plays of a game, trips through a maze, or any sort of repeated interaction. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. And the next episode begins independently of how the previous one ended. what is continuing taks?agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. the return of above two taskepisode task continuing task Unified notationthrough add an absorbing state in episode task(solid square): unified notation: update value function in mdp methodstate-value function for policy π: state-action-value function for policy π: how to update the state in policy π?Bellman equation for v π: Instead of getting average return in some random samples(Monte Carlo method), use some parameters to describle. π(a|s) is the policy π: decide action due to state s p(s’,r|s,a) is the transition probability of previous state s and action a, which follows the Markov property get the optimal policy π and optimal policy value-statechoose the max value in the actions of state s:]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10-armed-Testbed]]></title>
    <url>%2F2018%2F11%2F16%2F10-armed-Testbed%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter02/ten_armed_testbed.py 通过建立10-armed-Testbed来仿真第二章讲的几种Bandit算法 1、引入模块123456import matplotlibimport matplotlib.pyplot as plt%matplotlib inlineimport numpy as np# use for showing process barfrom tqdm import tqdm 2、创建Testbed类，实现基本的action和update value方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class Bandit: # @k_arm: # of arms # @epsilon: probability for exploration in epsilon-greedy algorithm # @initial: initial estimation for each action # @step_size: constant step size for updating estimations # @sample_averages: if True, use sample averages to update estimations instead of constant step size # @UCB_param: if not None, use UCB algorithm to select action # @gradient: if True, use gradient based bandit algorithm # @gradient_baseline: if True, use average reward as baseline for gradient based bandit algorithm def __init__(self, k_arm=10, epsilon=0., initial=0., step_size=0.1, sample_averages=False, UCB_param=None, gradient=False, gradient_baseline=False, true_reward=0.): self.k = k_arm self.step_size = step_size self.sample_averages = sample_averages self.indices = np.arange(self.k) self.time = 0 self.UCB_param = UCB_param self.gradient = gradient self.gradient_baseline = gradient_baseline self.average_reward = 0 self.true_reward = true_reward self.epsilon = epsilon self.initial = initial def reset(self): # real reward for each action # true_reward is baseline can be configed # np.random.randn(_size()) return standard normal array in the size of _size() self.q_true = np.random.randn(self.k) + self.true_reward # estimation for each action # here we can modify the serf.initial to export optimistic initial value self.q_estimation = np.zeros(self.k) + self.initial # of chosen times for each action self.action_count = np.zeros(self.k) # return the index of max-real-reward action self.best_action = np.argmax(self.q_true) # get an action for this bandit def act(self): # np.random.rand() return uniform distribution over [0,1) # explore if np.random.rand() &lt; self.epsilon: # return np.random.choice(self.indices) # I think UCB may choose actions in non-greedy actions if self.UCB_param is not None: # here is a little different from book, Why? # I think he may want to be avoid of zero problem in log and denominator position UCB_estimation = self.q_estimation + \ self.UCB_param * np.sqrt(np.log(self.time + 1) / (self.action_count + 1e-5)) q_best = np.max(UCB_estimation) return np.random.choice([action for action, q in enumerate(UCB_estimation) if q == q_best]) else: return np.random.choice(self.indices) if self.gradient: exp_est = np.exp(self.q_estimation) self.action_prob = exp_est / np.sum(exp_est) # According to the probability to choose action # here use the same method with wiki return np.random.choice(self.indices, p=self.action_prob) # greey-action return np.argmax(self.q_estimation) # take an action, update estimation for this action def step(self, action): # generate the reward under N(real reward, 1) reward = np.random.randn() + self.q_true[action] self.time += 1 # average_reward update uses self.time self.average_reward = (self.time - 1.0) / self.time * self.average_reward + reward / self.time self.action_count[action] += 1 if self.sample_averages: # update estimation using sample averages # q_estimation update uses action_count self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action]) # is different with book elif self.gradient: one_hot = np.zeros(self.k) one_hot[action] = 1 if self.gradient_baseline: baseline = self.average_reward else: baseline = 0 # update method is the same as book, here use q_estimation(a) replaces H_t(a) self.q_estimation = self.q_estimation + self.step_size * (reward - baseline) * (one_hot - self.action_prob) else: # update estimation with constant step size self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action]) return reward 3、训练Bandit123456789101112131415def simulate(runs, time, bandits): best_action_counts = np.zeros((len(bandits), runs, time)) rewards = np.zeros(best_action_counts.shape) for i, bandit in enumerate(bandits): for r in tqdm(range(runs)): bandit.reset() for t in range(time): action = bandit.act() reward = bandit.step(action) rewards[i, r, t] = reward if action == bandit.best_action: best_action_counts[i, r, t] = 1 best_action_counts = best_action_counts.mean(axis=1) rewards = rewards.mean(axis=1) return best_action_counts, rewards 4、结果显示（折线图格式）1、10-Bandit-Testbed value 和reward分布123456def figure_2_1(): plt.violinplot(dataset=np.random.randn(200,10) + np.random.randn(10)) plt.xlabel("Action") plt.ylabel("Reward distribution") plt.show()figure_2_1() 2、different epsilons12345678910111213141516171819202122def figure_2_2(runs=2000, time=1000): epsilons = [0, 0.1, 0.01] bandits = [Bandit(epsilon=eps, sample_averages=True) for eps in epsilons] best_action_counts, rewards = simulate(runs, time, bandits) plt.figure(figsize=(10, 20)) plt.subplot(2, 1, 1) for eps, rewards in zip(epsilons, rewards): plt.plot(rewards, label='epsilon = %.02f' % (eps)) plt.xlabel('steps') plt.ylabel('average reward') plt.legend() plt.subplot(2, 1, 2) for eps, counts in zip(epsilons, best_action_counts): plt.plot(counts, label='epsilon = %.02f' % (eps)) plt.xlabel('steps') plt.ylabel('% optimal action') plt.legend() plt.show()# figure_2_2() 3、Initial value = 5 VS Initial value = 01234567891011121314def figure_2_3(runs=2000, time=1000): bandits = [] bandits.append(Bandit(epsilon=0, initial=5, step_size=0.1)) bandits.append(Bandit(epsilon=0.1, initial=0, step_size=0.1)) best_action_counts, _ = simulate(runs, time, bandits) plt.plot(best_action_counts[0], label='epsilon = 0, q = 5') plt.plot(best_action_counts[1], label='epsilon = 0.1, q = 0') plt.xlabel('Steps') plt.ylabel('% optimal action') plt.legend() plt.show()#figure_2_3() 4、UCB VS epsilon-greey12345678910111213def figure_2_4(runs=2000, time=1000): bandits = [] bandits.append(Bandit(epsilon=0, UCB_param=2, sample_averages=True)) bandits.append(Bandit(epsilon=0.1, sample_averages=True)) _, average_rewards = simulate(runs, time, bandits) plt.plot(average_rewards[0], label='UCB c = 2') plt.plot(average_rewards[1], label='epsilon greedy epsilon = 0.1') plt.xlabel('Steps') plt.ylabel('Average reward') plt.legend() plt.show()#figure_2_4() 5、softmax baseline VS non-baseline12345678910111213141516171819def figure_2_5(runs=2000, time=1000): bandits = [] bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=True, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.1, gradient_baseline=False, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=True, true_reward=4)) bandits.append(Bandit(gradient=True, step_size=0.4, gradient_baseline=False, true_reward=4)) best_action_counts, _ = simulate(runs, time, bandits) labels = ['alpha = 0.1, with baseline', 'alpha = 0.1, without baseline', 'alpha = 0.4, with baseline', 'alpha = 0.4, without baseline'] for i in range(0, len(bandits)): plt.plot(best_action_counts[i], label=labels[i]) plt.xlabel('Steps') plt.ylabel('% Optimal action') plt.legend() plt.show()#figure_2_5() 6、epsilon-greey vs softmax vs UCB vs opt-initial123456789101112131415161718192021222324252627282930def figure_2_6(runs=2000, time=1000): labels = ['epsilon-greedy', 'gradient bandit', 'UCB', 'optimistic initialization'] generators = [lambda epsilon: Bandit(epsilon=epsilon, sample_averages=True), lambda alpha: Bandit(gradient=True, step_size=alpha, gradient_baseline=True), lambda coef: Bandit(epsilon=0, UCB_param=coef, sample_averages=True), lambda initial: Bandit(epsilon=0, initial=initial, step_size=0.1)] parameters = [np.arange(-7, -1, dtype=np.float), np.arange(-5, 2, dtype=np.float), np.arange(-4, 3, dtype=np.float), np.arange(-2, 3, dtype=np.float)] bandits = [] for generator, parameter in zip(generators, parameters): for param in parameter: bandits.append(generator(pow(2, param))) _, average_rewards = simulate(runs, time, bandits) rewards = np.mean(average_rewards, axis=1) i = 0 for label, parameter in zip(labels, parameters): l = len(parameter) plt.plot(parameter, rewards[i:i+l], label=label) i += l plt.xlabel('Parameter(2^x)') plt.ylabel('Average reward') plt.legend() plt.show()figure_2_6()]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[softmax-theory]]></title>
    <url>%2F2018%2F11%2F15%2Fsoftmax-theory%2F</url>
    <content type="text"><![CDATA[This post is my understanding about P29 deeper insight part:《The Bandit Gradient Algorithm as Stochastic Gradient Ascent》 1. Understand Why it comes from Stochastic Gradient Ascent在我们讨论这个问题之前先看看wiki上关于softmax在reinforcementLearning上的应用吧。 相信你和我一样惊奇，这里本来的H_t(a)变成了q_t(a)/τ。其中τ为温度系数(temperature parameter)，τ-&gt;无穷，所以action具有相同的选择概率，或者说是explore的；如果τ-&gt;0,则具有最大q_t(a)的行为会被选择，且τ越小，选择q_t(a)最大的概率越大并趋于1。 这太有意思了，wiki上直接解释了softmax function选择action的合理性，但是这就是这篇post想要讨论的问题啊@_@。 好了回到标题，书上这里这样写的： In exact gradient ascent, each preference H_t(a) would be incremented proportional to the increment’s effect on performance: 并给出了公式： E[R_t]的定义为： 从梯度上升的角度来解释：如果平均的value:E[R_t]对H_t(a)导数为正，则提高H_t(a)可以提高E[R_t]；如果相反，则减小H_t(a)可以提高E[R_t]。总结起来就是H_t(a)的增量和E[R_t]对H_t(a)的偏导成正比。 计算偏导计算公式如下： 公式的难点主要在第三步B_t的引入。B_t is called the baseline, can be any scalar that does not depend on x. We can include a baseline here without changing the equality because the gradient sums to zero over all the actions: change the equation to the form of expectation 这里的难点是使用R_t替换q_(A_t)，因为E[R_t|A_t] = q_(A_t)，所以可以替换。接着我们处理一下后半部分： 其中1_(a=x) means that if a=x, output 1; else output 0 因为我们通过无数次训练来更新H_t(a)，所以可以去掉期望符号，并在前面加上step-size α进行近似。如果α=1/n，那么就符合大数定律近似了，如果采用常数可能对解决非平稳问题更好一些。]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-armed Bandits]]></title>
    <url>%2F2018%2F11%2F14%2Fk-armed-Bandits%2F</url>
    <content type="text"><![CDATA[2.1.1 什么是k-armed Bandits问题You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.大致意思就是，每步有k种action选择，每种选择对应不同的reward，完成的目标就是需要在n次执行后，最大化总共的reward。 In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action: 2.1.2 k-armed Bandits问题的难点这里主要是explore和exploit的权衡。下面的解释摘自周老师的西瓜书强化学习部分： 若仅为知道每个摇臂的期望奖赏，则可采用“仅探索”(exploration-only)法：将所有的尝试机会平均分配给每个摇臂，最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。若仅为执行奖赏最大的动作，则可以用“仅利用”(exploitation-only)法，按下目前最优的摇臂。前者可以很好的估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会，没办法得到最好的收益；后者刚好相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优的摇臂，因此这两种方法都难以使最终的积累奖励最大化。 根据实验情况，exploring-only收敛结果明显劣于exploiting-only，而exploiting-only效果同样很差。 2.2 Action-value Method这是原文标题我直接抄过来了。因为上个标题提到了行为的价值(value)，所以需要给一种量化这种value的算法。 We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: 然后如何选择action？greedy action：选择Qt最大的a作为t时刻的action ε-greedy action: behave greedily most of the time, but every once in a while, say with small probability ε, instead select randomly from among all the actions with equal probability,independently of the action-value estimates. 2.3 The 10-armed Testbed10-armed Testbed是测试k-armed Bandits算法的一个基础模型，后面的模型评估都是基于此的。 What is it?简而言之，Testbed中包含了2000个10-armed Bandits问题，每个问题的10个行为action对应的value q(a)是通过正态分布得到的(mean=0,variance=1)，而实际对每个问题进行训练的时候，得到的实际回报(actual reward)是在action value上附加一个正态分布(mean=q(a),variance=1)。然后用greedy算法和ε-greedy算法对每个问题进行1000step的训练，对2000个问题求平均数得到 target(Average reward or Optimal action) - Steps的折线图。并以此来评价算法性能。 算法对比参照Python代码的博客 2.4 Incremental Implementation这部分讲了一个增量优化算法，这让我联想到了增量式PID和位置式PID，QAQ。其实这个思想和那个也挺相似的。 previous edition: PS:这个公式和第一章的tic-tac-toe的V(s)更新公式太像了。 but，还是有一些区别。第一章的V(s)更新的时候是仅针对greedy-action的，但是这里因为各个action是独立的，如果不对exploring更新的话，就没办法有效的找到最优的action了，即这次action是无意义的。 incremental edition: incremental bandit algorithm: an intresting update ruleThe update rule below is of a form that occurs frequently throughout this book. The general form is: about this rule:1、The expression [Target−OldEstimate] is an error in the estimate. It is reduced by taking a step toward the “Target.” The target is presumed to indicate a desirable direction in which to move, though it may be noisy. In the case above, for example, the target is the nth reward. 2、Note that the step-size parameter (StepSize) used in the incremental method described above changes from time step to time step. In processing the nth reward for action a, the method uses the step-size parameter 1/n . In this book we denote the step-size parameter by α or, more generally, by α_t(a). 2.5 Tracking a Nonstationary(非平稳) Problem上面讨论的Bandits-methods，都是建立在rewards的概率不变的前提下的，如Testbed中规定，reward(a)遵循(mean = q*(a),variance = 1)的概率分布。而在实际的强化学习任务中，经常是与之相对的概率不平稳的(nonstationary)。 考虑上一个标题讨论的update rule的一般形式，处理非平稳问题一个有效方法就是令α为定值(use a constant step-size parameter)，因为这样可以提高最近的reward的比重，降低过去久远的reward的比重，可以参考下面的公式： 因为α&lt;1，所以很明显如果i越小，R_i的weight就会越小。 但是有一个问题，就是如果希望最终的value是收敛的，则Stepsize需要满足如下条件： 第一条保证weight足够大，可以覆盖掉初值的误差以及一些波动，第二条保证最后能够收敛。 当α=1/n条件很明显是满足的，但是如果α=constant第二条就不满足了。但是对于非稳定的情况，这正是我们希望看到的，具体证明也没给出，下次看到补上？ 2.6 Optimistic Initial Values(encouraging exploration)我们在上面看到α=1/n时，Q1被消去了，但是事实上大部分情况下Q1会对学习情况产生影响，比如当α=constant时，Q1就被保留了，不过*了一个相当小的系数。。。 事实上，this kind of bias is usually not a problem and can sometimes be very helpful.举个栗子，在Testbed里，我把训练初值设为5，对于在1附近正态分布的reward，不管选择哪个，在初始阶段都会&lt;5，学习器对他们都很失望：Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being “disappointed” with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time. 但是对于非平稳问题(nonstationary)，因为分布会随时间改变，而这种只在训练开始引入exploring的方法相当于只是encourage exploration for once，故效果不会很好，但是对应sample-average(平稳)的问题就会有一定的效果。 当然，我们不会只采用一种方法，将不同的方法进行组合利用，将会是贯穿全书的思想。 2.7 Upper-Confidence-Bound Action Selection在ε-greedy方法中，通过一个小概率ε来选定non-greedy action进行exploring，但是这种方法是无差别的，如果对那些non-greedy action进行一个事前的评估，根据它们潜能(不确定度)来进行explore的话，那么可以提高explore的广度和效率，效果可能会更好。 One effective way of doing this is to select actions according to(UCB): N_t(a) denotes the number of times that action a has been selected prior to time t.number c &gt; 0 controls the degree of exploration.If N_t(a) = 0, then a is considered to be a maximizing action.公式里的平方根部分是对action a的不确定度的估计，可以从两方面来解释：如果action a经常被选中，则分母N_t就会变大，不确定度就会减小；如果action a不经常被选中，那么随着试验次数t增大，而N_t不变，则不确定度上升，我们应该多考虑一下a啦。 但是也存在一些问题UCB比起ε-greedy方法，相对来说更复杂，而且它在其他强化学习问题中的可扩展性(extend)远不如后者，而且这种方法在非平稳问题中表现的并不好，Another difficulty is dealing with large state spaces, particularly when using function approximation as developed in Part II of this book.In these more advanced settings the idea of UCB action selection is usually not practical. 2.8 Gradient Bandit Algorithms看了这么久，休息一下如何^_^ A new method to choose actionconsider learning a numerical preference for each action a, which we denote H_t(a). The larger the preference, the more often that action is taken: π_t(a) is the probability of taking action a at time t. Initially all preferences are the same(zero)so that all actions have an equal probability of being selected. How to update the H_t(a) in the rule: α&gt;0 is a step-size paramter; (R_t)_average is the baseline with which the reward is compared.It which can be computed incrementally as described in Incremental Implementation about the baselineIf the reward is higher than the baseline, then the probability of taking A_t in the future is increased, and if the reward is below baseline, then probability is decreased. The non-selected actions move in the opposite direction. the theory of softmax methodhere is a seperate blog to explain it. 2.9 Associative Search (Contextual Bandits)上面的算法只处理了一台k-arms-Bandit，接下来考虑这个问题：如果你面对的是10台k-arms-Bandit，每次你随机从这几台机器(k-arms-Bandit comes from slot machine)里抽一个来操作，那么可以认为这仍是在处理同一台slot machine，但是true value是随着一步一步操作剧烈变化的。 说到这里应该就明白了，我的每一步action都是会对接下来的situation和reward引起影响的，而普通的k-arms-Bandit每步之间是独立的。 associative search可以认为是k-arms-Bandit和full reinforcement learning之间的桥梁，现在它一般被称为contextual bandits问题。]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tic-Tac-Toe]]></title>
    <url>%2F2018%2F11%2F14%2FTic-Tac-Toe%2F</url>
    <content type="text"><![CDATA[引用来自ShangtongZhang的代码chapter01/tic_tac_toe.py Tic-Tac-Toe的python代码实现1、引入模块并定义井字棋的常量 12345678import numpy as npimport pickleimport matplotlib.pyplot as plt%matplotlib inline# 将rows和cols调至4，这种算法计算量就会大很多BOARD_ROWS = 3BOARD_COLS = 3BOARD_SIZE = BOARD_ROWS * BOARD_COLS 2、创建环境，State类，表征棋盘上的X和O子的情况。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class State: def __init__(self): # the board is represented by an n * n array, # 1 represents a chessman of the player who moves first, # -1 represents a chessman of another player # 0 represents an empty position self.data = np.zeros((BOARD_ROWS, BOARD_COLS)) self.winner = None self.hash_val = None self.end = None # compute the hash value for one state, it only work at first time def hash(self): if self.hash_val is None: self.hash_val = 0 for i in self.data.reshape(BOARD_ROWS * BOARD_COLS): if i == -1: # shallow copy i = 2 self.hash_val = self.hash_val * 3 + i return int(self.hash_val) # check whether a player has won the game, or it's a tie def is_end(self): if self.end is not None: return self.end results = [] # check row for i in range(0, BOARD_ROWS): results.append(np.sum(self.data[i, :])) # check columns for i in range(0, BOARD_COLS): results.append(np.sum(self.data[:, i])) # check diagonals results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, i] results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, BOARD_ROWS - 1 - i] for result in results: if result == 3: self.winner = 1 self.end = True return self.end if result == -3: self.winner = -1 self.end = True return self.end # whether it's a tie: no one wins, all places are filled. sum = np.sum(np.abs(self.data)) if sum == BOARD_ROWS * BOARD_COLS: self.winner = 0 self.end = True return self.end # game is still going on self.end = False return self.end # @symbol: 1 or -1 # put chessman symbol in position (i, j) def next_state(self, i, j, symbol): new_state = State() # deep copy new_state.data new_state.data = np.copy(self.data) new_state.data[i, j] = symbol return new_state # print the board def print(self): for i in range(0, BOARD_ROWS): print('-------------') out = '| ' for j in range(0, BOARD_COLS): if self.data[i, j] == 1: token = '*' if self.data[i, j] == 0: token = '0' if self.data[i, j] == -1: token = 'x' out += token + ' | ' print(out) print('-------------') 关于这里的hash函数，是为了得到不重复的所有棋盘情况。但是为什么可以通过这种算法那？假设两个不同的棋盘情况A和B，他们的hash值是一致的，我们不妨设他们的最高位不相同，那么我们看到，该位最少相差1，通过等比数列求和可以得到，3^(n+1)&gt;2*(1+3^1+3^2+…+3^n)，即最高位不同是无法通过其余位来弥补的，那么可见他们的最高位不同是不可能hash值相同的，以此递推，则可得A和B值是一样，即一样的棋盘，假设不成立。3、公共函数，用来获取当前state。12345678910111213141516171819202122232425def get_all_states_impl(current_state, current_symbol, all_states): for i in range(0, BOARD_ROWS): for j in range(0, BOARD_COLS): if current_state.data[i][j] == 0: newState = current_state.next_state(i, j, current_symbol) newHash = newState.hash() if newHash not in all_states.keys(): isEnd = newState.is_end() all_states[newHash] = (newState, isEnd) if not isEnd: get_all_states_impl(newState, -current_symbol, all_states)def get_all_states(): current_symbol = 1 current_state = State() all_states = dict() all_states[current_state.hash()] = (current_state, current_state.is_end()) get_all_states_impl(current_state, current_symbol, all_states) return all_states# all possible board configurationsall_states = get_all_states()# for i in all_states.values():# i[0].print()# print(i[1]) 4、Player类，用来模拟AI选手下棋的行为，得到value function1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# AI playerclass Player: # @step_size: the step size to update estimations # @epsilon: the probability to explore def __init__(self, step_size=0.1, epsilon=0.1): self.estimations = dict() self.step_size = step_size self.epsilon = epsilon self.states = [] # greedy means that state is caused by exploiting, otherwise is caused by exploring self.greedy = [] def reset(self): self.states = [] self.greedy = [] def set_state(self, state): self.states.append(state) self.greedy.append(True) # give the player symbol,and update the value table def set_symbol(self, symbol): self.symbol = symbol for hash_val in all_states.keys(): (state, is_end) = all_states[hash_val] if is_end: if state.winner == self.symbol: self.estimations[hash_val] = 1.0 elif state.winner == 0: # we need to distinguish between a tie and a lose self.estimations[hash_val] = 0.5 else: self.estimations[hash_val] = 0 else: self.estimations[hash_val] = 0.5 # update value estimation def backup(self): # for debug # print('player trajectory') # for state in self.states: # state.print() self.states = [state.hash() for state in self.states] for i in reversed(range(len(self.states) - 1)): state = self.states[i] # only udpate the state caused by exploiting td_error = self.greedy[i] * (self.estimations[self.states[i + 1]] - self.estimations[state]) self.estimations[state] += self.step_size * td_error # choose an action based on the state def act(self): state = self.states[-1] next_states = [] next_positions = [] for i in range(BOARD_ROWS): for j in range(BOARD_COLS): if state.data[i, j] == 0: next_positions.append([i, j]) next_states.append(state.next_state(i, j, self.symbol).hash()) # explore if np.random.rand() &lt; self.epsilon: action = next_positions[np.random.randint(len(next_positions))] action.append(self.symbol) self.greedy[-1] = False return action # exploit values = [] for hash, pos in zip(next_states, next_positions): values.append((self.estimations[hash], pos)) np.random.shuffle(values) values.sort(key=lambda x: x[0], reverse=True) # choose the biggest value action action = values[0][1] action.append(self.symbol) return action # policy -&gt; self.estimations def save_policy(self): with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f: pickle.dump(self.estimations, f) def load_policy(self): with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f: self.estimations = pickle.load(f) 这里需要注意一点就是，在backup方法里，只有greedy-action才会更新value，exploring虽然不会更新value，但是其随后的greedy-action会进行更新，个人的理解是：exploring并不能保证最优，它只是提供一种“探索”行为，帮助Learning method更好的学习value，所以只对greedy-action进行update。5、创建对决类Judger，通过一次对局完成一次value table的update1234567891011121314151617181920212223242526272829303132333435363738394041424344class Judger: # @player1: the player who will move first, its chessman will be 1 # @player2: another player with a chessman -1 # @feedback: if True, both players will receive rewards when game is end def __init__(self, player1, player2): self.p1 = player1 self.p2 = player2 self.current_player = None self.p1_symbol = 1 self.p2_symbol = -1 self.p1.set_symbol(self.p1_symbol) self.p2.set_symbol(self.p2_symbol) self.current_state = State() def reset(self): self.p1.reset() self.p2.reset() # return p1 and p2 in turn(alternately) def alternate(self): while True: yield self.p1 yield self.p2 # @print: if True, print each board during the game def play(self, print=False): alternator = self.alternate() self.reset() current_state = State() self.p1.set_state(current_state) self.p2.set_state(current_state) while True: player = next(alternator) if print: current_state.print() [i, j, symbol] = player.act() next_state_hash = current_state.next_state(i, j, symbol).hash() current_state, is_end = all_states[next_state_hash] self.p1.set_state(current_state) self.p2.set_state(current_state) if is_end: if print: current_state.print() return current_state.winner 6、训练并让两名AI Player进行对局12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def train(epochs): player1 = Player(epsilon=0.01) player2 = Player(epsilon=0.01) judger = Judger(player1, player2) player1_win = 0.0 player2_win = 0.0 player1_a = [] player2_a = [] for i in range(1, epochs + 1): winner = judger.play(print=False) if winner == 1: player1_win += 1 if winner == -1: player2_win += 1 player1_a.append(player1_win/i) player2_a.append(player2_win/i) #print('Epoch %d, player 1 win %.02f, player 2 win %.02f' % (i, player1_win / i, player2_win / i)) player1.backup() player2.backup() judger.reset() player1.save_policy() player2.save_policy() x = range(1,1001,1) x = list(x) plt.plot(x,player1_a) plt.plot(x,player2_a) plt.axis([0,200,0,1]) plt.title('training chart') plt.show() def compete(turns): player1 = Player(epsilon=0) player2 = Player(epsilon=0) judger = Judger(player1, player2) player1.load_policy() player2.load_policy() player1_win = 0.0 player2_win = 0.0 for i in range(0, turns): winner = judger.play() if winner == 1: player1_win += 1 if winner == -1: player2_win += 1 judger.reset() print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))train(int(1e3))]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReinforcementLearningChapter1]]></title>
    <url>%2F2018%2F11%2F12%2FReinforcementLearningChapter1%2F</url>
    <content type="text"><![CDATA[希望能以更新博客的方式激励一下自己，目前是准备读一下强化学习的入门书《Reinforcement Learning Introduction》，然后做一下读书笔记。下面是绪论(Introduction)的内容。 What is ReinforcementLearningFirst Impression of RL如果说之前我接触到的机器学习方法，如逻辑回归、svm、决策树、人工神经网络等是数据驱动的，那么强化学习就是“情境”驱动的：行为由当前环境决定，行为导致数值化的reward更新，而reward又影响了环境。这是我对强化学习的初印象。 强化学习有两个特点：1、trial-and-error search：学习算法并未指导行为，但是却必须得出当行为做出相应的reward(近期) 2、delayed reward：每个行为之间不是独立的，当前行为可能会有比当前收益更丰盛的远期收益(value)。 强化学习有三个重要要素，就是学习器(agent，我对学习器的理解就是训练出模型时执行的程序）必须能够感知环境(state)并做出影响它的行为(action)，同时agent must have a goal or goals relating to the state of the environment。 RL vs Supervised learning监督学习就不赘述了，大致来说，监督学习算法通过带标签的训练集学习得到可以对未知样例的判断“能力”。但是如果让监督学习从交互中学习，按照监督学习的思路，获取正确的而且有意义的行为例子是不可取的，所以它没办法从经验中成长，不适合需要和环境交互的算法。 举一个不太恰当的例子，监督学习就像考前刷题的你，而强化学习就像每天认真学习的学霸，如果考题和你刷的吻合度比较高，那你就稳了，但是学霸每天学习，数据已经内化为他的能力了，所以他一点都不慌。 RL vs Unsupervised learning有不少人认为监督学习和非监督学习已经把机器学习进行详尽的划分了，他们将强化学习归入后者，因为它们都没有带标签的数据，但是从目的上来看，强化学习是为了最大化reward的，而非监督学习是为了学习到数据之中的隐藏结构，非监督学习的方法并没有从实质解决强化学习的问题。 exploration and exploitationThe agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future. 这个问题有点像梯度下降中使用的随机梯度法(SGD)或者模拟退火法，但是SGD是一种加速算法，或者说如果我头铁不用，我也是有可能得到还可以的结果，但是强化学习如果不学习未知的部分，结果肯定是有问题的，但是如果一直在试错 explore，不对“经验”进行总结 exploit，也是没办法使算法收敛的。 Tic-Tac-Toe我觉得先分析一个例子可以更好的帮助理解RL的各个属性之间的关系。Tic-Tac-Toe比较像我们小时候玩的井字棋，图我就不画了。 强化学习和传统的classical minimax solution from game theory以及Classical optimization methods for sequential decision problems, such as dynamic programming有区别，前者是按照有强约束的方式运动的（这里是不会走使自己失败的位置），而后者则是需要有每一步的先验概率才能进行优化。 强化学习是基于value functions的学习方法，而前面提到的minimax可以认为是基于evolutionary的学习方法，前者是基于value of state来学习的，后者则是通过结果来更新policy，前者对状态的利用优于后者，而后者则是仅利用了结果来修改policy。 tic-tac-toe的ython代码实现戳这里 Elements of RLBeyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal , a value function, and, optionally, a model of the environment. policy感觉是从state到action的一个映射关系吧，这里就对应了上面提到的explore和exploit的关系。In general, policies may be stochastic. reward每个action导致的短期收益。reward会决定policy，使得行为向高reward的方向发展，所以reward一般是state和action的随机函数：In general, reward signals may be stochastic functions of the state of the environment and the actions taken. value function由reward引起的长期收益。value由reward综合而来，而value同时会使reward朝着高的方向发展。value控制action的发展：Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run. model of the environmentThis is something that mimics(模拟) the behavior of the environment, or more generally, that allows inferences(推断) to be made about how the environment will behave. Summary总体来说强化学习和之前学过的其他机器学习方法还是区别挺大的，但是绪论有几点疑问： 1、action、state、reward、value之间的更新方式还是没搞懂？ 2、如何建立value table？ ——————-(answers in 11.14)————————- 1、value建立在state之上，每次state更新伴随着value的更新；action分为两种，explore和exploit，前者随机，后者基于reward最大；reward认为是从current state -&gt; next state update时，对应的next value or (next value - current value) 2、value table建立基于state，原则视问题而定。比如tic-tac-toe问题，X赢对应的value = 1，O赢对应的value = 0，平局对应value = 0.5，当然我是站在X的角度。]]></content>
      <tags>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[some config on next theme]]></title>
    <url>%2F2018%2F11%2F12%2Fsome-config-on-next-theme%2F</url>
    <content type="text"><![CDATA[上周配置了hexo+github的个人博客，这次我做了一些偏好的配置。 更换主题为next因为网上很多关于主题配置的博客都是基于next主题的，所以我先将主题换为了next主题： 1、先下载主题到本地 123$ cd ~$ cd GitBlog$ git clone https://github.com/iissnan/hexo-theme-next themes/next 2、更改站点的config文件来使用主题 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 3、更改theme/next的config文件(/theme/next/_config.yml): 123456789101112# 修改主题模式为Gemini# Schemes#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini#修改侧边栏头像# Sidebar Avatar# in theme directory(source/images): /images/avatar.gif# in site directory(source/uploads): /uploads/avatar.gifavatar: https://wx2.sinaimg.cn/large/0070VybLly1fx5b09mly4j30kx0i14kq.jpg 添加搜索功能下载hexo搜索插件hexo-generator-search然后修改站点的config文件启动搜索功能： 12345678910# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next#在插件位置插入搜索模块使能插件search: path: search.xml field: post format: html limit: 10000 添加渐变更换的壁纸效果比较像忧郁的弟弟(额马上就是loli了)背景效果。 1、在/theme/next/source/css/_custom/中修改_custom.styl，添加css3动画特效： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137body &#123;background: #000;background-attachment: fixed;word-wrap: break-word;-webkit-background-size: cover;-moz-background-size: cover;background-size: cover;background-repeat: no-repeat&#125; ul &#123;list-style: none&#125; .cb-slideshow li:nth-child(1) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b098is8j31hc0u0jzz.jpg)&#125; .cb-slideshow li:nth-child(2) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b095t28j31hc11hdph.jpg)&#125; .cb-slideshow li:nth-child(3) span &#123;background-image: url(https://wx2.sinaimg.cn/large/0070VybLly1fx5b09b17xj31hc0z2tim.jpg)&#125; .cb-slideshow li:nth-child(4) span &#123;background-image: url(https://wx4.sinaimg.cn/large/0070VybLly1fx5b09474xj31hc0x6jy5.jpg)&#125; .cb-slideshow li:nth-child(5) span &#123;background-image: url(https://wx2.sinaimg.cn/large/0070VybLly1fx5b08weitj31hc11otga.jpg)&#125; .cb-slideshow li:nth-child(6) span &#123;background-image: url(https://wx3.sinaimg.cn/large/0070VybLly1fx5b1eraokj31hc0u0gp9.jpg)&#125; .cb-slideshow,.cb-slideshow:after &#123;position: fixed;width: 100%;height: 100%;top: 0;left: 0;z-index: -2&#125; .cb-slideshow:after &#123;content: ''&#125; .cb-slideshow li span &#123;width: 100%;height: 100%;position: absolute;top: 0;left: 0;color: transparent;background-size: cover;background-position: 50% 50%;background-repeat: none;opacity: 0;z-index: -2;-webkit-backface-visibility: hidden;-webkit-animation: imageAnimation 36s linear infinite 0s;-moz-animation: imageAnimation 36s linear infinite 0s;-o-animation: imageAnimation 36s linear infinite 0s;-ms-animation: imageAnimation 36s linear infinite 0s;animation: imageAnimation 36s linear infinite 0s&#125; .cb-slideshow li:nth-child(2) span &#123;-webkit-animation-delay: 6s;-moz-animation-delay: 6s;-o-animation-delay: 6s;-ms-animation-delay: 6s;animation-delay: 6s&#125; .cb-slideshow li:nth-child(3) span &#123;-webkit-animation-delay: 12s;-moz-animation-delay: 12s;-o-animation-delay: 12s;-ms-animation-delay: 12s;animation-delay: 12s&#125; .cb-slideshow li:nth-child(4) span &#123;-webkit-animation-delay: 18s;-moz-animation-delay: 18s;-o-animation-delay: 18s;-ms-animation-delay: 18s;animation-delay: 18s&#125; .cb-slideshow li:nth-child(5) span &#123;-webkit-animation-delay: 24s;-moz-animation-delay: 24s;-o-animation-delay: 24s;-ms-animation-delay: 24s;animation-delay: 24s&#125; .cb-slideshow li:nth-child(6) span &#123;-webkit-animation-delay: 30s;-moz-animation-delay: 30s;-o-animation-delay: 30s;-ms-animation-delay: 30s;animation-delay: 30s&#125; @-webkit-keyframes imageAnimation &#123;0% &#123;opacity: 0;-webkit-animation-timing-function: ease-in&#125; 8% &#123;opacity: 1;-webkit-transform: scale(1.05);-webkit-animation-timing-function: ease-out&#125; 17% &#123;opacity: 1;-webkit-transform: scale(1.1) rotate(0)&#125; 25% &#123;opacity: 0;-webkit-transform: scale(1.1) rotate(0)&#125; 100% &#123;opacity: 0&#125;&#125; 2、在/theme/next/layout/中修改_layout.swig，在标签中加入下述代码： 1234567891011121314&lt;ul class="cb-slideshow"&gt; &lt;li&gt; &lt;span&gt;1&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;2&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;3&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;4&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;5&lt;/span&gt;&lt;/li&gt; &lt;li&gt; &lt;span&gt;6&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt;]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first try of hexo+github]]></title>
    <url>%2F2018%2F11%2F12%2Fhow-to-config-the-hexo%2F</url>
    <content type="text"><![CDATA[今天我学习了如何配置Hexo来写博客，并放到github上，但是我还不会markdown，自己还是好菜啊QAQ。 卸载之前安装不成功的hexo，npm和nodejs卸载hexo12$ cd /usr/bin$ npm uninstall hexo -g ###卸载nodejs和npm 123$ sudo apt-get remove nodejs$ sudo apt-get remove node$ sudo apt-get remove npm 可能会没用，我是运行了第一个就把都卸载了，但是不影响哈。 安装nvmnvm是node的包版本控制工具，使用这个我装上了稳定版的hexo，之前我直接apt的hexo在运行 hexo s时会报错： ERROR Local hexo not found。 下载nvm12345$ export NVM_DIR="$HOME/.nvm" &amp;&amp; ( git clone https://github.com/creationix/nvm.git "$NVM_DIR" cd "$NVM_DIR" git checkout `git describe --abbrev=0 --tags --match "v[0-9]*" origin`) &amp;&amp; . "$NVM_DIR/nvm.sh" 配置为nvm自启动在 bash配置文件.bashrc中加入（maybe .zshrc）： 12export NVM_DIR="$HOME/.nvm"[ -s "$NVM_DIR/nvm.sh" ] &amp;&amp; . "$NVM_DIR/nvm.sh" 使用nvm安装nodejs和npm检测远程仓库1$ nvm ls-remote 安装稳定版本nodejs1$ nvm install stable 启动安装好的nodejs1$ nvm use node 设置为nodejs默认版本1$ nvm alias default node 安装hexo使用npm安装hexo1$ npm install -g hexo 初始化hexo本地博客空间1234$ mkdir GitBlog$ cd GitBlog &amp;&amp; hexo init$ npm install$ npm update -g 运行本地服务器查看博客12$ hexo g # 生成静态界面$ hexo s # 开启本地服务 将博客在github上托管配置本机全局git环境假设github使用的邮箱是hehe@qq.com，github用户名是hehe： 12$ git config -global user.email "hehe@qq.com"$ git config -global user.name "hehe" 生成ssh秘钥先检查一下之前有没有生成过ssh的key: 1$ less ~/.ssh/id_rsa.pub 如果有的话会有输出，则不需要生成秘钥，如果没有执行指令： 1$ ssh-keygen -t rsa -C hehe@qq.com 接着会提示输入文件夹位置来放置ssh秘钥，并会让你确认一个验证的密码，如果不需要可以回车跳过。成功的话会在~/.ssh下生成ssh秘钥，即所谓的公钥id_rsa.pub和私钥id_rsa(RSA加密)。 在github创建博客工程在github用户下新建一个仓库，按照上面的假设，那么我生成的这个仓库名应该是hehe@github.io。然后将之前生成的ssh秘钥添加到github上。 在hexo中配置使用git部署博客在站点的配置文件(博客根目录下的_config.yml)中配置你的git: 1234deploy:type: gitrepo: git@github.com:hehe/hehe.github.io.gitbrach: master 将本地文件上传到github上本地修改好了，可以先在本地运行一下看一下效果： 1$ hexo s 如果没什么问题，接下来将本地内容部署到github服务器上： 123$ hexo clean$ hexo g$ hexo d 有的时候可能要清除一下浏览器的缓存才能显示更新后的内容，具体原因我也不知道。部署完成后，可以直接在浏览器输入hehe@github.io就能看到部署完成的hexo blog了。]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
