<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Reinforcement Learning Note,">










<meta name="description" content="Monte Carlo MethodUnlike the previous chapter, here we do not assume complete knowledge of the environment. Monte Carlo methods require only experience—sample sequences of states, actions, and rewards">
<meta name="keywords" content="Reinforcement Learning Note">
<meta property="og:type" content="article">
<meta property="og:title" content="Chapter05 Monte Carlo Methods">
<meta property="og:url" content="http://yoursite.com/2018/11/28/Chapter05-Monte-Carlo-Methods/index.html">
<meta property="og:site_name" content="Oppai&gt;&#x2F;&#x2F;&#x2F;&lt;">
<meta property="og:description" content="Monte Carlo MethodUnlike the previous chapter, here we do not assume complete knowledge of the environment. Monte Carlo methods require only experience—sample sequences of states, actions, and rewards">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://wx1.sinaimg.cn/large/0070VybLly1fxo39o53dbj30k509ot9s.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxoodcdvuaj306n06ljrh.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxooef35q4j30n20azmyo.jpg">
<meta property="og:image" content="https://wx1.sinaimg.cn/large/0070VybLly1fxoolcuncpj30no0coabs.jpg">
<meta property="og:image" content="https://wx4.sinaimg.cn/large/0070VybLly1fxoozkoa53j30qb09rwf6.jpg">
<meta property="og:image" content="https://wx4.sinaimg.cn/large/0070VybLly1fxnjxhkwkfj30hi0440sr.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxnk0cdkxhj30f602mjrd.jpg">
<meta property="og:image" content="https://wx4.sinaimg.cn/large/0070VybLly1fxnuz6y9ouj307o01jjr7.jpg">
<meta property="og:image" content="https://wx1.sinaimg.cn/large/0070VybLly1fxnv1wa5y2j30870273yd.jpg">
<meta property="og:image" content="https://wx4.sinaimg.cn/large/0070VybLly1fxnv75ocuij308c02gglh.jpg">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?V_{n}\doteq&space;\frac{\sum_{k=1}^{n-1}W_{k}G_{k}}{\sum_{k=1}^{n-1}W_{k}}\&space;\&space;n\geq&space;2">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?V_{n+1}\doteq&space;V_{n}+\frac{W_{n}}{C_{n}}\left&space;\lfloor&space;G_{n}-V_{n}&space;\right&space;\rfloor\&space;\&space;\&space;n\geq&space;1">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?C_{n}=\sum_{k=1}^{n}W_{k}\&space;\&space;\&space;C_{n+1}\doteq&space;C_{n}+W_{n+1}\&space;\&space;\&space;(C_{0}=0)">
<meta property="og:image" content="https://wx1.sinaimg.cn/large/0070VybLly1fxopsoec1zj30pr0bwabc.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxoq503n3bj30hf0c4myi.jpg">
<meta property="og:updated_time" content="2018-12-02T09:25:47.707Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter05 Monte Carlo Methods">
<meta name="twitter:description" content="Monte Carlo MethodUnlike the previous chapter, here we do not assume complete knowledge of the environment. Monte Carlo methods require only experience—sample sequences of states, actions, and rewards">
<meta name="twitter:image" content="https://wx1.sinaimg.cn/large/0070VybLly1fxo39o53dbj30k509ot9s.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/11/28/Chapter05-Monte-Carlo-Methods/">





  <title>Chapter05 Monte Carlo Methods | Oppai>///<</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Oppai>///<</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


<!-- 图片轮播js文件cdn -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>

<!-- 自定义的js文件 -->
<script type="text/javascript" src="/js/src/custom.js"></script>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/28/Chapter05-Monte-Carlo-Methods/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xingE650">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxoqylcs4uj309708rgoa.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Oppai>///<">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Chapter05 Monte Carlo Methods</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-28T10:03:40+08:00">
                2018-11-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Monte-Carlo-Method"><a href="#Monte-Carlo-Method" class="headerlink" title="Monte Carlo Method"></a>Monte Carlo Method</h1><p>Unlike the previous chapter, here we do not assume complete knowledge of the environment. Monte Carlo methods require only experience—sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Learning from actual experience is striking because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior.</p>
<a id="more"></a>
<h1 id="Monte-Carlo-Prediction"><a href="#Monte-Carlo-Prediction" class="headerlink" title="Monte Carlo Prediction"></a>Monte Carlo Prediction</h1><p>蒙特卡洛预测是利用蒙特卡洛方法的最简单的强化学习算法，通过给定的policy进行模拟，并根据模拟episodes来进行state-value期望计算。</p>
<p>这里要注意的是，预测分为first-visit和every-visit，前者是本章讲述的重点，后面其他的算法也都是first-visit的，它的意思是指模拟过程中只对一次状态s计算return，<br>换句话说episode中只能存在一个s，如果存在多个s的episode这种方法就会直接舍弃；</p>
<p>后者会在后面章节讲述，every-visit会对episode中所有s的return进行计算。</p>
<p>first-visit的Monte Carlo预测伪代码：</p>
<p><img src="https://wx1.sinaimg.cn/large/0070VybLly1fxo39o53dbj30k509ot9s.jpg" alt="png"></p>
<h1 id="Monte-Carlo-Estimation-of-Action-Values"><a href="#Monte-Carlo-Estimation-of-Action-Values" class="headerlink" title="Monte Carlo Estimation of Action Values"></a>Monte Carlo Estimation of Action Values</h1><p>看了网上的一些解释，如果是model-free的强化学习问题，都是要学习Q(s,a)的，只有model-based的方法可以直接学习v(s)。</p>
<p>其实感觉model-free是一种更普适的方法，因为一个一般的决策问题如果知道行为带来的value的话决策起来就会很舒服，直接greedy action嘛。</p>
<p>但是如果我对环境有一些了解，比如像MDP中那样使用model-based的学习方法，知道状态间的转移概率p(s,a)，那么我在训练的过程中不同step对应的state可以通过动态规划联系在一起，即state和following states之间并没有通过action来连接。那么我在学习的时候就不用去考虑不同的action导致的Q(s,a)，具体的算法可以参考前面MDP的post。那我自然只通过建立v(s)就够了。</p>
<p>这里关于如何建立action-value有一点需要注意：在MDP问题中，每个state都会被更新，所以是不需要maintain exploring的；但是对于action来说，如果采用完全greedy的policy的话，所有的action不一定都会选中，这个问题和第2章的Bandit问题一样，所以需要在greedy policy之外保持一种exploring的策略。于是这里便引入了exploring start的方法，即每次episode的初始state-action会保证所有的state-action对都有概率被选中。</p>
<h1 id="Monte-Carlo-Control"><a href="#Monte-Carlo-Control" class="headerlink" title="Monte Carlo Control"></a>Monte Carlo Control</h1><p>蒙特卡洛控制通过上一章提到的GPI(generalized policy iteration)的思路来解决的。就是控制环保有一个近似的value function和一个近似的policy，the value function is repeatedly altered(改变) to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the<br>current value function.GPI的流程可以简单理解为下图：</p>
<p><img src="https://wx2.sinaimg.cn/large/0070VybLly1fxoodcdvuaj306n06ljrh.jpg" alt="png"></p>
<p>这个地方和上一章DP算法提到的policy Iteration和value Iteration原理是一致的就不多赘述了。下面看一下基于ES(exploring start)的Control方法：</p>
<p><img src="https://wx2.sinaimg.cn/large/0070VybLly1fxooef35q4j30n20azmyo.jpg" alt="png"> </p>
<h1 id="Monte-Carlo-Control-without-Exploring-Start"><a href="#Monte-Carlo-Control-without-Exploring-Start" class="headerlink" title="Monte Carlo Control without Exploring Start"></a>Monte Carlo Control without Exploring Start</h1><p>Exploring start的假设在实际问题中不是很常见，所以这一部分提出了两个without exploring start的控制方法：on-policy和off-policy的。</p>
<p>首先需要了解on和off-policy之间的区别和联系：</p>
<p>off-policy是通过behavior policy来模拟并产生数据(episode)，但是学习得到的是target policy π，即使用的episode is “off” the target policy π；</p>
<p>on-policy则在模拟数据和学习时都使用同一个policy。一般来说on-policy更简单易用，off-policy则会有更多的参数和更复杂的学习过程，收敛起来也比较慢，但是off-policy可以适的范围更广，可以更好的解决问题。而且如果off-policy的behavior policy = target policy，二者就一致了。</p>
<p>这一部分主要讨论了on-policy的控制方法，先看一下算法的伪代码：</p>
<p><img src="https://wx1.sinaimg.cn/large/0070VybLly1fxoolcuncpj30no0coabs.jpg" alt="png"></p>
<p>算法提升原理如下所示，这里的q(s,π’(s))指的是在π’策略下的s的state-value。π’是上面伪代码中的ε-greedy策略。</p>
<p><img src="https://wx4.sinaimg.cn/large/0070VybLly1fxoozkoa53j30qb09rwf6.jpg" alt="png"></p>
<h1 id="Off-policy-Prediction-via-Importance-Sampling"><a href="#Off-policy-Prediction-via-Importance-Sampling" class="headerlink" title="Off-policy Prediction via Importance Sampling"></a>Off-policy Prediction via Importance Sampling</h1><h2 id="behavior-policy-vs-target-policy"><a href="#behavior-policy-vs-target-policy" class="headerlink" title="behavior policy vs target policy"></a>behavior policy vs target policy</h2><p>关于target policy π cover behavior policy b的概念，这里什么才能被称为cover(覆盖)那？</p>
<p>文中解释是这样的：通过π选取的action一定有概率通过b选取，即通过π(a|s)&gt;0一定可以推出b(a|s)&gt;0，前者是后者的充分条件。</p>
<p>还有就是这里提一下off-policy和on-policy之间的关系：</p>
<h2 id="what-is-importance-sampling"><a href="#what-is-importance-sampling" class="headerlink" title="what is importance sampling"></a>what is importance sampling</h2><p>重要取样(Importance Sampling)的推导是通过MDP得到的，它是一种off-policy方法中通过behavior policy来训练target policy的重要途径，其中一个重要的概念就是重要取样比率(importance-sampling ratio)，它的定义式是target policy下的状态轨迹的π(a|s)和behavior policy下的状态轨迹的b(a|s)的比值。具体推导如下：</p>
<p>首先定义通过behavior policy取样得到的states-chain的联合概率，这里的进一步计算使用了马尔科夫独立性假设：</p>
<p><img src="https://wx4.sinaimg.cn/large/0070VybLly1fxnjxhkwkfj30hi0440sr.jpg" alt="png"> </p>
<p>然后给出重要取样比率的定义：</p>
<p><img src="https://wx2.sinaimg.cn/large/0070VybLly1fxnk0cdkxhj30f602mjrd.jpg" alt="png"> </p>
<p>其中的状态转移概率p被消去了，可以证明这个结论虽然是通过马尔科夫独立性推出的，却具有一般性。</p>
<p>好了，现在我们基本理解了重要采样比率的概念，那么它到底有什么用那？下面的公式解释了这个问题（具体推导不清楚QAQ）：</p>
<p><img src="https://wx4.sinaimg.cn/large/0070VybLly1fxnuz6y9ouj307o01jjr7.jpg" alt="png"> </p>
<p>这个值是联系episode的return和target policy π下的v_π(s)的关键！</p>
<p>所以很直观的从这个期望公式引出ordinary importance sampling：</p>
<p><img src="https://wx1.sinaimg.cn/large/0070VybLly1fxnv1wa5y2j30870273yd.jpg" alt="png"></p>
<p>但是有一个问题，就是如果重要采样因子有可能是方差无限的，这时我们近似得到的state-value就会发散而无法收敛，这个问题很严重，随后也会通过<a href="https://xinge650.github.io/2018/11/28/infinite-variance/" target="_blank" rel="noopener">代码</a>来解释。</p>
<p>所以通过对采样因子做归一化，引出了Weighted importance sampling的概念：</p>
<p><img src="https://wx4.sinaimg.cn/large/0070VybLly1fxnv75ocuij308c02gglh.jpg" alt="png"></p>
<p>和前者相比，虽然weighted importance sampling是有偏的(bias)，因为它不是直接从上面的期望公式来的，但是经过多次迭代bias会趋于0。但是如果采样因子的方差不是有限的，<br>ordinary importance sampling就很有可能无法收敛，也就是说它的方差(variance)是高于weighted importance sampling的，所以总的来说后者更常用。</p>
<h1 id="Incremental-Implementation-for-Off-policy-Prediction"><a href="#Incremental-Implementation-for-Off-policy-Prediction" class="headerlink" title="Incremental Implementation for Off-policy Prediction"></a>Incremental Implementation for Off-policy Prediction</h1><p>增量算法实现和第2章的增量实现原理类似，先给出简单的证明：</p>
<p>设V_n是模拟过程中第n对(s,a)，G1，G2,…Gn是behavior policy产生的对应第n对(s,a)的return，则有下述表达式成立:</p>
<p>额感觉翻译的不够好，粘一下原文吧：</p>
<p>Suppose we have a sequence of returns G1 , G2 , . . . , Gn−1 , all starting in the same state and each with a corresponding random weight Wi (e.g., Wi = ρ_{t:T(t)−1} ). We wish to form the estimate:</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=V_{n}\doteq&space;\frac{\sum_{k=1}^{n-1}W_{k}G_{k}}{\sum_{k=1}^{n-1}W_{k}}\&space;\&space;n\geq&space;2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_{n}\doteq&space;\frac{\sum_{k=1}^{n-1}W_{k}G_{k}}{\sum_{k=1}^{n-1}W_{k}}\&space;\&space;n\geq&space;2" title="V_{n}\doteq \frac{\sum_{k=1}^{n-1}W_{k}G_{k}}{\sum_{k=1}^{n-1}W_{k}}\ \ n\geq 2"></a></p>
<p>增量形式表达式:</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=V_{n&plus;1}\doteq&space;V_{n}&plus;\frac{W_{n}}{C_{n}}\left&space;\lfloor&space;G_{n}-V_{n}&space;\right&space;\rfloor\&space;\&space;\&space;n\geq&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_{n&plus;1}\doteq&space;V_{n}&plus;\frac{W_{n}}{C_{n}}\left&space;\lfloor&space;G_{n}-V_{n}&space;\right&space;\rfloor\&space;\&space;\&space;n\geq&space;1" title="V_{n+1}\doteq V_{n}+\frac{W_{n}}{C_{n}}\left \lfloor G_{n}-V_{n} \right \rfloor\ \ \ n\geq 1"></a></p>
<p>其中：</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=C_{n}=\sum_{k=1}^{n}W_{k}\&space;\&space;\&space;C_{n&plus;1}\doteq&space;C_{n}&plus;W_{n&plus;1}\&space;\&space;\&space;(C_{0}=0)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C_{n}=\sum_{k=1}^{n}W_{k}\&space;\&space;\&space;C_{n&plus;1}\doteq&space;C_{n}&plus;W_{n&plus;1}\&space;\&space;\&space;(C_{0}=0)" title="C_{n}=\sum_{k=1}^{n}W_{k}\ \ \ C_{n+1}\doteq C_{n}+W_{n+1}\ \ \ (C_{0}=0)"></a></p>
<p>紧接着给出预测的增量形式伪代码：</p>
<p><img src="https://wx1.sinaimg.cn/large/0070VybLly1fxopsoec1zj30pr0bwabc.jpg" alt="png"></p>
<h1 id="Off-policy-Monte-Carlo-Control"><a href="#Off-policy-Monte-Carlo-Control" class="headerlink" title="Off-policy Monte Carlo Control"></a>Off-policy Monte Carlo Control</h1><p>这一章脉络很清晰啊，基本是先阐述预测算法，再讨论控制算法。这种方法在本书中很常见，因为policy的improve，还是control算法都是建立在predict(or evalution)的基础上的，再加上greey的policy提升方法得到的。下面给出off-policy的增量控制算法：</p>
<p><img src="https://wx2.sinaimg.cn/large/0070VybLly1fxoq503n3bj30hf0c4myi.jpg" alt="png"></p>
<p>针对这个算法有几点想讨论的：</p>
<p>1、b是soft-policy的，soft是指b在生成模拟过程的时候对所有的状态动作对的发生概率都不为0，即可以尽可能多的产生不同的过程，相当于一种exploring吧。原文解释是：</p>
<p>policy π is soft, meaning that π(a|s)&gt;0 for all s and a. ε-soft policy is defined as policies for which π(a|s) ≥ ε/|A(s)| for all states and actions, for some ε &gt; 0</p>
<p>2、如果使用π(St)得到的action和episode中的At不同，那么重要采样因子的分子是为0的，此时不对W进行更新。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Reinforcement-Learning-Note/" rel="tag"># Reinforcement Learning Note</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/23/Chapter04-gamblers-problem/" rel="next" title="Chapter04 gamblers_problem">
                <i class="fa fa-chevron-left"></i> Chapter04 gamblers_problem
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/28/Chapter05-blackjack/" rel="prev" title="Chapter05 blackjack">
                Chapter05 blackjack <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://wx2.sinaimg.cn/large/0070VybLly1fxoqylcs4uj309708rgoa.jpg" alt="xingE650">
            
              <p class="site-author-name" itemprop="name">xingE650</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Monte-Carlo-Method"><span class="nav-number">1.</span> <span class="nav-text">Monte Carlo Method</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Monte-Carlo-Prediction"><span class="nav-number">2.</span> <span class="nav-text">Monte Carlo Prediction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Monte-Carlo-Estimation-of-Action-Values"><span class="nav-number">3.</span> <span class="nav-text">Monte Carlo Estimation of Action Values</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Monte-Carlo-Control"><span class="nav-number">4.</span> <span class="nav-text">Monte Carlo Control</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Monte-Carlo-Control-without-Exploring-Start"><span class="nav-number">5.</span> <span class="nav-text">Monte Carlo Control without Exploring Start</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Off-policy-Prediction-via-Importance-Sampling"><span class="nav-number">6.</span> <span class="nav-text">Off-policy Prediction via Importance Sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#behavior-policy-vs-target-policy"><span class="nav-number">6.1.</span> <span class="nav-text">behavior policy vs target policy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#what-is-importance-sampling"><span class="nav-number">6.2.</span> <span class="nav-text">what is importance sampling</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Incremental-Implementation-for-Off-policy-Prediction"><span class="nav-number">7.</span> <span class="nav-text">Incremental Implementation for Off-policy Prediction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Off-policy-Monte-Carlo-Control"><span class="nav-number">8.</span> <span class="nav-text">Off-policy Monte Carlo Control</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xingE650</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

  <ul class="cb-slideshow">
		<li>
		<span>1</span></li>
		<li>
		<span>2</span></li>
		<li>
		<span>3</span></li>
		<li>
		<span>4</span></li>
		<li>
		<span>5</span></li>
		<li>
		<span>6</span></li>
  </ul>
</body>
</html>
