<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Reinforcement Learning Note,">










<meta name="description" content="k-arm-Bandits问题可说是强化学习最简单的任务了，因为他只涉及了1个state下的action选取。通过本章可以对强化学习的目标，评估方法和训练方法有一个初步的认识。">
<meta name="keywords" content="Reinforcement Learning Note">
<meta property="og:type" content="article">
<meta property="og:title" content="Chapter02 Multi-armed Bandits">
<meta property="og:url" content="http://yoursite.com/2018/11/14/Chapter02-Multi-armed-Bandits/index.html">
<meta property="og:site_name" content="Oppai&gt;&#x2F;&#x2F;&#x2F;&lt;">
<meta property="og:description" content="k-arm-Bandits问题可说是强化学习最简单的任务了，因为他只涉及了1个state下的action选取。通过本章可以对强化学习的目标，评估方法和训练方法有一个初步的认识。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?q_{a}&space;\doteq&space;E[R_{t}|A_{t}=a]">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?Q_{t}(a)&space;\doteq&space;\frac{sum\&space;of\&space;rewards\&space;when\&space;a\&space;taken\&space;prior\&space;to\&space;t}{number\&space;of\&space;times\&space;a\&space;taken\&space;prior\&space;to\&space;t}&space;=&space;\frac{\sum_{i=1}^{t-1}R_{i}*1_{A_{i}&space;=&space;a}}{\sum_{i=1}^{t-1}1_{A_{i}&space;=&space;a}}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?Q_{n}\doteq&space;\frac{R_{1}+R_{2}+...+R_{n-1}}{n-1}">
<meta property="og:image" content="https://wx4.sinaimg.cn/large/0070VybLly1fx7x6t06w5j30c70ap0t2.jpg">
<meta property="og:image" content="https://wx3.sinaimg.cn/large/0070VybLly1fx7x973k4bj30pu0823z3.jpg">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?NewEstimate\leftarrow&space;OldEstimate+StepSize[Target-OldEstimate]">
<meta property="og:image" content="https://wx3.sinaimg.cn/large/0070VybLly1fx8v7pj41mj30ez07jwer.jpg">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\sum_{n=1}^{\infty&space;}\alpha&space;_{n}(a)&space;=&space;\infty\&space;\&space;and\&space;\&space;\sum_{n=1}^{\infty&space;}\alpha&space;_{n}^{2}(a)&space;<&space;\infty">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?A_{t}\doteq&space;argmax_{a}[Q_{t}(a)+c\sqrt{\frac{\ln&space;t}{N_{t}(a)}}]">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?P_{r}\left&space;\{&space;A_{t}=a&space;\right&space;\}\doteq&space;\frac{e^{H_{t}(a)}}{\sum_{b=1}^{k}e^{H_{t}(b)}}\doteq&space;\pi&space;_{t}(a)">
<meta property="og:image" content="https://wx4.sinaimg.cn/large/0070VybLly1fx8u7lft01j30ht022mx5.jpg">
<meta property="og:updated_time" content="2018-12-15T01:33:39.384Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter02 Multi-armed Bandits">
<meta name="twitter:description" content="k-arm-Bandits问题可说是强化学习最简单的任务了，因为他只涉及了1个state下的action选取。通过本章可以对强化学习的目标，评估方法和训练方法有一个初步的认识。">
<meta name="twitter:image" content="https://latex.codecogs.com/gif.latex?q_{a}&space;\doteq&space;E[R_{t}|A_{t}=a]">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/11/14/Chapter02-Multi-armed-Bandits/">





  <title>Chapter02 Multi-armed Bandits | Oppai>///<</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Oppai>///<</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


<!-- 图片轮播js文件cdn -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>

<!-- 自定义的js文件 -->
<script type="text/javascript" src="/js/src/custom.js"></script>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/14/Chapter02-Multi-armed-Bandits/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xingE650">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxoqylcs4uj309708rgoa.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Oppai>///<">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Chapter02 Multi-armed Bandits</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-14T19:42:41+08:00">
                2018-11-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>k-arm-Bandits问题可说是强化学习最简单的任务了，因为他只涉及了1个state下的action选取。通过本章可以对强化学习的目标，评估方法和训练方法有一个初步的认识。</p>
<a id="more"></a>
<h2 id="2-1-1-什么是k-armed-Bandits问题"><a href="#2-1-1-什么是k-armed-Bandits问题" class="headerlink" title="2.1.1 什么是k-armed Bandits问题"></a>2.1.1 什么是k-armed Bandits问题</h2><p>You are faced repeatedly with a choice among k different options, or <strong><em>actions</em></strong>. After each choice you receive a <strong><em>numerical reward</em></strong> chosen from a stationary probability distribution that depends on the action you selected. Your objective is to <strong><em>maximize the expected total reward</em></strong> over some time period, for example, over 1000 action selections, or time steps.</p>
<p>大致意思就是，每步有k种action选择，每种选择对应不同的reward，完成的目标就是需要在n次执行后，最大化总共的reward。</p>
<p>In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this <font color="green">the value of that action</font>:<br><!-- 公式 --><br><a href="https://www.codecogs.com/eqnedit.php?latex=q_{a}&space;\doteq&space;E[R_{t}|A_{t}=a]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q_{a}&space;\doteq&space;E[R_{t}|A_{t}=a]" title="q_{a} \doteq E[R_{t}|A_{t}=a]"></a></p>
<h2 id="2-1-2-k-armed-Bandits问题的难点"><a href="#2-1-2-k-armed-Bandits问题的难点" class="headerlink" title="2.1.2 k-armed Bandits问题的难点"></a>2.1.2 k-armed Bandits问题的难点</h2><p>这里主要是explore和exploit的权衡。下面的解释摘自周老师的西瓜书强化学习部分：</p>
<font color="green">若仅为知道每个摇臂的期望奖赏，则可采用“仅探索”(exploration-only)法：将所有的尝试机会平均分配给每个摇臂，最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。若仅为执行奖赏最大的动作，则可以用“仅利用”(exploitation-only)法，按下目前最优的摇臂。前者可以很好的估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会，没办法得到最好的收益；后者刚好相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优的摇臂，因此这两种方法都难以使最终的积累奖励最大化。</font> 

<p>根据实验情况，exploring-only收敛结果明显劣于exploiting-only，而exploiting-only效果同样很差。</p>
<h2 id="2-2-Action-value-Method"><a href="#2-2-Action-value-Method" class="headerlink" title="2.2 Action-value Method"></a>2.2 Action-value Method</h2><p>这是原文标题我直接抄过来了。因为上个标题提到了行为的价值(value)，所以需要给一种量化这种value的算法。</p>
<p>We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. <strong><em>Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received:</em></strong></p>
<p><img src="https://latex.codecogs.com/gif.latex?Q_{t}(a)&space;\doteq&space;\frac{sum\&space;of\&space;rewards\&space;when\&space;a\&space;taken\&space;prior\&space;to\&space;t}{number\&space;of\&space;times\&space;a\&space;taken\&space;prior\&space;to\&space;t}&space;=&space;\frac{\sum_{i=1}^{t-1}R_{i}*1_{A_{i}&space;=&space;a}}{\sum_{i=1}^{t-1}1_{A_{i}&space;=&space;a}}" title="Q_{t}(a) \doteq \frac{sum\ of\ rewards\ when\ a\ taken\ prior\ to\ t}{number\ of\ times\ a\ taken\ prior\ to\ t} = \frac{\sum_{i=1}^{t-1}R_{i}*1_{A_{i} = a}}{\sum_{i=1}^{t-1}1_{A_{i} = a}}"></p>
<h3 id="然后如何选择action？"><a href="#然后如何选择action？" class="headerlink" title="然后如何选择action？"></a>然后如何选择action？</h3><p>greedy action：选择Qt最大的a作为t时刻的action</p>
<p>ε-greedy action: behave greedily most of the time, but every once in a while, say with small probability ε, instead select randomly from among all the actions with equal probability,independently of the action-value estimates.</p>
<h2 id="2-3-The-10-armed-Testbed"><a href="#2-3-The-10-armed-Testbed" class="headerlink" title="2.3 The 10-armed Testbed"></a>2.3 The 10-armed Testbed</h2><p>10-armed Testbed是测试k-armed Bandits算法的一个基础模型，后面的模型评估都是基于此的。</p>
<h3 id="What-is-it"><a href="#What-is-it" class="headerlink" title="What is it?"></a>What is it?</h3><p>简而言之，Testbed中包含了2000个10-armed Bandits问题，每个问题的10个行为action对应的value q<em>(a)是通过正态分布得到的(mean=0,variance=1)，而实际对每个问题进行训练的时候，得到的实际回报(actual reward)是在action value上附加一个正态分布(mean=q</em>(a),variance=1)。然后用greedy算法和ε-greedy算法对每个问题进行1000step的训练，对2000个问题求平均数得到 <strong><em>target(Average reward or Optimal action) - Steps的折线图</em></strong>。并以此来评价算法性能。</p>
<p>算法对比参照Python代码的<a href="https://xinge650.github.io/2018/11/15/10-arm-testBed/" target="_blank" rel="noopener">博客</a></p>
<p><a name="1"></a></p>
<h2 id="2-4-Incremental-Implementation"><a href="#2-4-Incremental-Implementation" class="headerlink" title="2.4 Incremental Implementation"></a>2.4 Incremental Implementation</h2><p>这部分讲了一个增量优化算法，这让我联想到了增量式PID和位置式PID，QAQ。其实这个思想和那个也挺相似的。</p>
<h3 id="previous-edition"><a href="#previous-edition" class="headerlink" title="previous edition:"></a>previous edition:</h3><p><img src="https://latex.codecogs.com/gif.latex?Q_{n}\doteq&space;\frac{R_{1}&plus;R_{2}&plus;...&plus;R_{n-1}}{n-1}" title="Q_{n}\doteq \frac{R_{1}+R_{2}+...+R_{n-1}}{n-1}"></p>
<p>PS:这个公式和第一章的tic-tac-toe的V(s)更新公式太像了。</p>
<p>but，还是有一些区别。第一章的V(s)更新的时候是仅针对greedy-action的，但是这里因为各个action是独立的，如果不对exploring更新的话，就没办法有效的找到最优的action了，即这次action是无意义的。</p>
<h3 id="incremental-edition"><a href="#incremental-edition" class="headerlink" title="incremental edition:"></a>incremental edition:</h3><p><img src="https://wx4.sinaimg.cn/large/0070VybLly1fx7x6t06w5j30c70ap0t2.jpg" alt="png"></p>
<h3 id="incremental-bandit-algorithm"><a href="#incremental-bandit-algorithm" class="headerlink" title="incremental bandit algorithm:"></a>incremental bandit algorithm:</h3><p><img src="https://wx3.sinaimg.cn/large/0070VybLly1fx7x973k4bj30pu0823z3.jpg" alt="png"> </p>
<h3 id="an-intresting-update-rule"><a href="#an-intresting-update-rule" class="headerlink" title="an intresting update rule"></a>an intresting update rule</h3><h3 id="The-update-rule-below-is-of-a-form-that-occurs-frequently-throughout-this-book-The-general-form-is"><a href="#The-update-rule-below-is-of-a-form-that-occurs-frequently-throughout-this-book-The-general-form-is" class="headerlink" title="The update rule below is of a form that occurs frequently throughout this book. The general form is:"></a>The update rule below is of a form that occurs frequently throughout this book. The general form is:</h3><p><img src="https://latex.codecogs.com/gif.latex?NewEstimate\leftarrow&space;OldEstimate&plus;StepSize[Target-OldEstimate]" title="NewEstimate\leftarrow OldEstimate+StepSize[Target-OldEstimate]"></p>
<h3 id="about-this-rule"><a href="#about-this-rule" class="headerlink" title="about this rule:"></a>about this rule:</h3><p>1、The expression [Target−OldEstimate] is an <strong>error</strong> in the estimate. It is reduced by taking a step toward the “Target.” The target is presumed to indicate a desirable direction in which to move, though it may be noisy. In the case above, for example, the target is the <em>n</em>th reward.</p>
<p>2、Note that the <strong>step-size</strong> parameter (StepSize) used in the incremental method described above changes <strong>from time step to time step</strong>. In processing the <em>n</em>th reward for action a, the method uses the step-size parameter <strong>1/n</strong> . In this book we denote the step-size parameter by α or, more generally, by α_t(a).</p>
<h2 id="2-5-Tracking-a-Nonstationary-非平稳-Problem"><a href="#2-5-Tracking-a-Nonstationary-非平稳-Problem" class="headerlink" title="2.5 Tracking a Nonstationary(非平稳) Problem"></a>2.5 Tracking a Nonstationary(非平稳) Problem</h2><p>上面讨论的Bandits-methods，都是建立在rewards的概率不变的前提下的，如Testbed中规定，reward(a)遵循(mean = q*(a),variance = 1)的概率分布。而在实际的强化学习任务中，经常是与之相对的概率不平稳的(nonstationary)。</p>
<p>考虑上一个标题讨论的update rule的一般形式，处理非平稳问题一个有效方法就是令α为定值(use a constant step-size parameter)，因为这样可以提高最近的reward的比重，降低过去久远的reward的比重，可以参考下面的公式：</p>
<p><img src="https://wx3.sinaimg.cn/large/0070VybLly1fx8v7pj41mj30ez07jwer.jpg" alt="png"> </p>
<p>因为α&lt;1，所以很明显如果i越小，R_i的weight就会越小。</p>
<p>但是有一个问题，就是如果希望最终的value是收敛的，则Stepsize需要满足如下条件：</p>
<p><img src="https://latex.codecogs.com/gif.latex?\sum_{n=1}^{\infty&space;}\alpha&space;_{n}(a)&space;=&space;\infty\&space;\&space;and\&space;\&space;\sum_{n=1}^{\infty&space;}\alpha&space;_{n}^{2}(a)&space;<&space;\infty" title="\sum_{n=1}^{\infty }\alpha _{n}(a) = \infty\ \ and\ \ \sum_{n=1}^{\infty }\alpha _{n}^{2}(a) < \infty"></p>
<p>第一条保证weight足够大，可以覆盖掉初值的误差以及一些波动，第二条保证最后能够收敛。</p>
<p>当α=1/n条件很明显是满足的，但是如果α=constant第二条就不满足了。但是对于非稳定的情况，这正是我们希望看到的，具体证明也没给出，下次看到补上？</p>
<h2 id="2-6-Optimistic-Initial-Values-encouraging-exploration"><a href="#2-6-Optimistic-Initial-Values-encouraging-exploration" class="headerlink" title="2.6 Optimistic Initial Values(encouraging exploration)"></a>2.6 Optimistic Initial Values(encouraging exploration)</h2><p>我们在上面看到α=1/n时，Q1被消去了，但是事实上大部分情况下Q1会对学习情况产生影响，比如当α=constant时，Q1就被保留了，不过*了一个相当小的系数。。。</p>
<p>事实上，this kind of bias is usually not a problem and can sometimes be very helpful.举个栗子，在Testbed里，我把训练初值设为5，对于在1附近正态分布的reward，不管选择哪个，在初始阶段都会&lt;5，学习器对他们都很失望：Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being “disappointed” with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of <strong>exploration</strong> even if greedy actions are selected all the time.</p>
<p>但是对于非平稳问题(nonstationary)，因为分布会随时间改变，而这种只在训练开始引入exploring的方法相当于只是encourage exploration for once，故效果不会很好，但是对应sample-average(平稳)的问题就会有一定的效果。</p>
<p>当然，我们不会只采用一种方法，将不同的方法进行组合利用，将会是贯穿全书的思想。</p>
<h2 id="2-7-Upper-Confidence-Bound-Action-Selection"><a href="#2-7-Upper-Confidence-Bound-Action-Selection" class="headerlink" title="2.7 Upper-Confidence-Bound Action Selection"></a>2.7 Upper-Confidence-Bound Action Selection</h2><p>在ε-greedy方法中，通过一个小概率ε来选定non-greedy action进行exploring，但是这种方法是无差别的，如果对那些non-greedy action进行一个事前的评估，根据它们潜能(不确定度)来进行explore的话，那么可以提高explore的广度和效率，效果可能会更好。</p>
<h3 id="One-effective-way-of-doing-this-is-to-select-actions-according-to-UCB"><a href="#One-effective-way-of-doing-this-is-to-select-actions-according-to-UCB" class="headerlink" title="One effective way of doing this is to select actions according to(UCB):"></a>One effective way of doing this is to select actions according to(UCB):</h3><p><img src="https://latex.codecogs.com/gif.latex?A_{t}\doteq&space;argmax_{a}[Q_{t}(a)&plus;c\sqrt{\frac{\ln&space;t}{N_{t}(a)}}]" title="A_{t}\doteq argmax_{a}[Q_{t}(a)+c\sqrt{\frac{\ln t}{N_{t}(a)}}]"></p>
<h4 id="N-t-a-denotes-the-number-of-times-that-action-a-has-been-selected-prior-to-time-t"><a href="#N-t-a-denotes-the-number-of-times-that-action-a-has-been-selected-prior-to-time-t" class="headerlink" title="N_t(a) denotes the number of times that action a has been selected prior to time t."></a>N_t(a) denotes the number of times that action a has been selected prior to time t.</h4><h4 id="number-c-gt-0-controls-the-degree-of-exploration"><a href="#number-c-gt-0-controls-the-degree-of-exploration" class="headerlink" title="number c &gt; 0 controls the degree of exploration."></a>number c &gt; 0 controls the degree of exploration.</h4><h4 id="If-N-t-a-0-then-a-is-considered-to-be-a-maximizing-action"><a href="#If-N-t-a-0-then-a-is-considered-to-be-a-maximizing-action" class="headerlink" title="If N_t(a) = 0, then a is considered to be a maximizing action."></a>If N_t(a) = 0, then a is considered to be a maximizing action.</h4><p>公式里的平方根部分是对action a的不确定度的估计，可以从两方面来解释：如果action a经常被选中，则分母N_t就会变大，不确定度就会减小；如果action a不经常被选中，那么随着试验次数t增大，而N_t不变，则不确定度上升，我们应该多考虑一下a啦。</p>
<h3 id="但是也存在一些问题"><a href="#但是也存在一些问题" class="headerlink" title="但是也存在一些问题"></a>但是也存在一些问题</h3><p>UCB比起ε-greedy方法，相对来说更复杂，而且它在其他强化学习问题中的可扩展性(extend)远不如后者，而且这种方法在非平稳问题中表现的并不好，Another difficulty is dealing with large state spaces, particularly when using function approximation as developed in Part II of this book.In these more advanced settings the idea of UCB action selection is usually not practical.</p>
<h2 id="2-8-Gradient-Bandit-Algorithms"><a href="#2-8-Gradient-Bandit-Algorithms" class="headerlink" title="2.8 Gradient Bandit Algorithms"></a>2.8 Gradient Bandit Algorithms</h2><p><a href="https://www.bilibili.com/" target="_blank" rel="noopener">看了这么久，休息一下如何^_^</a></p>
<h3 id="A-new-method-to-choose-action"><a href="#A-new-method-to-choose-action" class="headerlink" title="A new method to choose action"></a>A new method to choose action</h3><p>consider learning a <strong><em>numerical preference</em></strong> for each action a, which we denote H_t(a). The larger the preference, the more often that action is taken:</p>
<p><img src="https://latex.codecogs.com/gif.latex?P_{r}\left&space;\{&space;A_{t}=a&space;\right&space;\}\doteq&space;\frac{e^{H_{t}(a)}}{\sum_{b=1}^{k}e^{H_{t}(b)}}\doteq&space;\pi&space;_{t}(a)" title="P_{r}\left \{ A_{t}=a \right \}\doteq \frac{e^{H_{t}(a)}}{\sum_{b=1}^{k}e^{H_{t}(b)}}\doteq \pi _{t}(a)"></p>
<p>π_t(a) is the probability of taking action a at time t. Initially all preferences are the same(zero)so that all actions have an equal probability of being selected.</p>
<h3 id="How-to-update-the-H-t-a"><a href="#How-to-update-the-H-t-a" class="headerlink" title="How to update the H_t(a)"></a>How to update the H_t(a)</h3><p><img src="https://wx4.sinaimg.cn/large/0070VybLly1fx8u7lft01j30ht022mx5.jpg" alt="png"></p>
<p>in the rule:</p>
<p>α&gt;0 is a step-size paramter;</p>
<p>(R_t)_average is the baseline with which the reward is compared.<br>It which can be computed incrementally as described in <a href="#1">Incremental Implementation</a></p>
<h4 id="about-the-baseline"><a href="#about-the-baseline" class="headerlink" title="about the baseline"></a>about the baseline</h4><p>If the reward is higher than the baseline, then the probability of taking A_t in the future is increased, and if the reward is below baseline, then probability is decreased. The non-selected actions move in the opposite direction.</p>
<h3 id="the-theory-of-softmax-method"><a href="#the-theory-of-softmax-method" class="headerlink" title="the theory of softmax method"></a>the theory of softmax method</h3><p><a href="https://xinge650.github.io/2018/11/15/softmax-theory/" target="_blank" rel="noopener">here</a> is a seperate blog to explain it.</p>
<h2 id="2-9-Associative-Search-Contextual-Bandits"><a href="#2-9-Associative-Search-Contextual-Bandits" class="headerlink" title="2.9 Associative Search (Contextual Bandits)"></a>2.9 Associative Search (Contextual Bandits)</h2><p>上面的算法只处理了一台k-arms-Bandit，接下来考虑这个问题：如果你面对的是10台k-arms-Bandit，每次你随机从这几台机器(k-arms-Bandit comes from slot machine)里抽一个来操作，那么可以认为这仍是在处理同一台slot machine，但是true value是随着一步一步操作剧烈变化的。</p>
<p>说到这里应该就明白了，我的每一步action都是会对接下来的situation和reward引起影响的，而普通的k-arms-Bandit每步之间是独立的。</p>
<p>associative search可以认为是k-arms-Bandit和full reinforcement learning之间的桥梁，现在它一般被称为contextual bandits问题。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Reinforcement-Learning-Note/" rel="tag"># Reinforcement Learning Note</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/14/Chapter01-Tic-Tac-Toe/" rel="next" title="Chapter01 Tic-Tac-Toe">
                <i class="fa fa-chevron-left"></i> Chapter01 Tic-Tac-Toe
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/15/Chapter02-softmax-theory/" rel="prev" title="Chapter02 softmax-theory">
                Chapter02 softmax-theory <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://wx2.sinaimg.cn/large/0070VybLly1fxoqylcs4uj309708rgoa.jpg" alt="xingE650">
            
              <p class="site-author-name" itemprop="name">xingE650</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">36</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Helpful Link
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://docs.scipy.org/doc/numpy/reference/index.html" title="numpy-reference" target="_blank">numpy-reference</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction" title="RL-book-code" target="_blank">RL-book-code</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-1-什么是k-armed-Bandits问题"><span class="nav-number">1.</span> <span class="nav-text">2.1.1 什么是k-armed Bandits问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-2-k-armed-Bandits问题的难点"><span class="nav-number">2.</span> <span class="nav-text">2.1.2 k-armed Bandits问题的难点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Action-value-Method"><span class="nav-number">3.</span> <span class="nav-text">2.2 Action-value Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#然后如何选择action？"><span class="nav-number">3.1.</span> <span class="nav-text">然后如何选择action？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-The-10-armed-Testbed"><span class="nav-number">4.</span> <span class="nav-text">2.3 The 10-armed Testbed</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-is-it"><span class="nav-number">4.1.</span> <span class="nav-text">What is it?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-Incremental-Implementation"><span class="nav-number">5.</span> <span class="nav-text">2.4 Incremental Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#previous-edition"><span class="nav-number">5.1.</span> <span class="nav-text">previous edition:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#incremental-edition"><span class="nav-number">5.2.</span> <span class="nav-text">incremental edition:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#incremental-bandit-algorithm"><span class="nav-number">5.3.</span> <span class="nav-text">incremental bandit algorithm:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#an-intresting-update-rule"><span class="nav-number">5.4.</span> <span class="nav-text">an intresting update rule</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-update-rule-below-is-of-a-form-that-occurs-frequently-throughout-this-book-The-general-form-is"><span class="nav-number">5.5.</span> <span class="nav-text">The update rule below is of a form that occurs frequently throughout this book. The general form is:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#about-this-rule"><span class="nav-number">5.6.</span> <span class="nav-text">about this rule:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-Tracking-a-Nonstationary-非平稳-Problem"><span class="nav-number">6.</span> <span class="nav-text">2.5 Tracking a Nonstationary(非平稳) Problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-Optimistic-Initial-Values-encouraging-exploration"><span class="nav-number">7.</span> <span class="nav-text">2.6 Optimistic Initial Values(encouraging exploration)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7-Upper-Confidence-Bound-Action-Selection"><span class="nav-number">8.</span> <span class="nav-text">2.7 Upper-Confidence-Bound Action Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#One-effective-way-of-doing-this-is-to-select-actions-according-to-UCB"><span class="nav-number">8.1.</span> <span class="nav-text">One effective way of doing this is to select actions according to(UCB):</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#N-t-a-denotes-the-number-of-times-that-action-a-has-been-selected-prior-to-time-t"><span class="nav-number">8.1.1.</span> <span class="nav-text">N_t(a) denotes the number of times that action a has been selected prior to time t.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#number-c-gt-0-controls-the-degree-of-exploration"><span class="nav-number">8.1.2.</span> <span class="nav-text">number c &gt; 0 controls the degree of exploration.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#If-N-t-a-0-then-a-is-considered-to-be-a-maximizing-action"><span class="nav-number">8.1.3.</span> <span class="nav-text">If N_t(a) = 0, then a is considered to be a maximizing action.</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#但是也存在一些问题"><span class="nav-number">8.2.</span> <span class="nav-text">但是也存在一些问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-8-Gradient-Bandit-Algorithms"><span class="nav-number">9.</span> <span class="nav-text">2.8 Gradient Bandit Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-new-method-to-choose-action"><span class="nav-number">9.1.</span> <span class="nav-text">A new method to choose action</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-to-update-the-H-t-a"><span class="nav-number">9.2.</span> <span class="nav-text">How to update the H_t(a)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#about-the-baseline"><span class="nav-number">9.2.1.</span> <span class="nav-text">about the baseline</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-theory-of-softmax-method"><span class="nav-number">9.3.</span> <span class="nav-text">the theory of softmax method</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-9-Associative-Search-Contextual-Bandits"><span class="nav-number">10.</span> <span class="nav-text">2.9 Associative Search (Contextual Bandits)</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xingE650</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

  <ul class="cb-slideshow">
		<li>
		<span>1</span></li>
		<li>
		<span>2</span></li>
		<li>
		<span>3</span></li>
		<li>
		<span>4</span></li>
		<li>
		<span>5</span></li>
		<li>
		<span>6</span></li>
  </ul>

<body oncopy="alert('be helpful to you(๑•̀ㅂ•́)و✧');return true;">
</body>
</body></html>
