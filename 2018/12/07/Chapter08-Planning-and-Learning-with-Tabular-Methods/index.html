<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Reinforcement Learning Note,">










<meta name="description" content="这章给出了关于model-free方法和model-based方法的一个统一观点，即将两者的思路结合起来解决问题： model-based 方法训练采用基于model进行planning的方法，通过向后迭代(backup)来更新value function；而model-free 方法训练采用从环境抽样(sampling)来进行value function的learning，具体也是使用后续的st">
<meta name="keywords" content="Reinforcement Learning Note">
<meta property="og:type" content="article">
<meta property="og:title" content="Chapter08 Planning and Learning with Tabular Methods">
<meta property="og:url" content="http://yoursite.com/2018/12/07/Chapter08-Planning-and-Learning-with-Tabular-Methods/index.html">
<meta property="og:site_name" content="Oppai&gt;&#x2F;&#x2F;&#x2F;&lt;">
<meta property="og:description" content="这章给出了关于model-free方法和model-based方法的一个统一观点，即将两者的思路结合起来解决问题： model-based 方法训练采用基于model进行planning的方法，通过向后迭代(backup)来更新value function；而model-free 方法训练采用从环境抽样(sampling)来进行value function的learning，具体也是使用后续的st">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxxz8qyaqbj30ma021weg.jpg">
<meta property="og:image" content="https://wx4.sinaimg.cn/large/0070VybLly1fxxzbplpd8j30q405laan.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxxzgj1yuqj30a8084weq.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxxzltwuzvj30fj0bu3z8.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxxzp3ohdfj30pn0970ty.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxy0u87ao2j30pp0d0tan.jpg">
<meta property="og:image" content="https://wx4.sinaimg.cn/large/0070VybLly1fxy6ribiqnj30db0msta3.jpg">
<meta property="og:image" content="https://wx1.sinaimg.cn/large/0070VybLly1fxy6t2p8nlj30c602at8l.jpg">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxy6ubaa7mj30dx01y3yd.jpg">
<meta property="og:updated_time" content="2018-12-15T01:44:01.704Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter08 Planning and Learning with Tabular Methods">
<meta name="twitter:description" content="这章给出了关于model-free方法和model-based方法的一个统一观点，即将两者的思路结合起来解决问题： model-based 方法训练采用基于model进行planning的方法，通过向后迭代(backup)来更新value function；而model-free 方法训练采用从环境抽样(sampling)来进行value function的learning，具体也是使用后续的st">
<meta name="twitter:image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxxz8qyaqbj30ma021weg.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/12/07/Chapter08-Planning-and-Learning-with-Tabular-Methods/">





  <title>Chapter08 Planning and Learning with Tabular Methods | Oppai>///<</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Oppai>///<</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


<!-- 图片轮播js文件cdn -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>

<!-- 自定义的js文件 -->
<script type="text/javascript" src="/js/src/custom.js"></script>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/07/Chapter08-Planning-and-Learning-with-Tabular-Methods/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xingE650">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://wx2.sinaimg.cn/large/0070VybLly1fxoqylcs4uj309708rgoa.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Oppai>///<">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Chapter08 Planning and Learning with Tabular Methods</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-07T09:43:39+08:00">
                2018-12-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>这章给出了关于model-free方法和model-based方法的一个统一观点，即将两者的思路结合起来解决问题：</p>
<p>model-based 方法训练采用基于model进行planning的方法，通过向后迭代(backup)来更新value function；而model-free 方法训练采用从环境抽样(sampling)来进行value function的learning，具体也是使用后续的state(action)-value来backup当前value function。两者之间同时有很多相似点，比如都会update value function，都采用backup的策略update等。可见两者的区别主要在于model的维护与否，当然这就是结合起来两种方法的训练策略。</p>
<a id="more"></a>
<h2 id="Models-and-Planning"><a href="#Models-and-Planning" class="headerlink" title="Models and Planning"></a>Models and Planning</h2><p>我觉得这里理解sample model就够了，sample model也是本章剩下的部分使用的planning方法的基础——从采样得到随机model进行planning。</p>
<p>首先理解distribution model和sample model之间的关系：</p>
<p>Given a state and an action, a <strong><em>model</em></strong> produces a prediction of the resultant next state and next reward.</p>
<p>distribution model: produce a description of all possibilities and probabilities for next state and next reward.</p>
<p>sample model: produce just one next state and action by sampling before.</p>
<p>distribution model就是在<a href="https://xinge650.github.io/2018/11/23/Chapter04-Dynamic-Programming/" target="_blank" rel="noopener">第四章</a>使用的MDP，sample model就是利用采样得到next state，然后利用这些采样经验组成的model。</p>
<p>但是不管哪种模型，更新value function的方法都是一样的，即通过planning的方法来得到优化(optimal)的policy。但是从planning得到policy的具体算法还分为两类：</p>
<p>state-space planning：通过产生模拟的经验，通过模拟action让一个state转换到另一个state，来更新value-function从而得到优化的policy</p>
<p>plan-space planning：通过在plans的空间中寻找优化的plan，通过从一个plan切换到另一个plan，从而更新value-function，得到优化的policy</p>
<p>后者在随机序列决策问题(stochastic sequential decision problems)中应用起来比较困难，所以一般都是采用前者。</p>
<p>state-space planning的优化流程图：</p>
<p><img src="https://wx2.sinaimg.cn/large/0070VybLly1fxxz8qyaqbj30ma021weg.jpg" alt="png"> </p>
<p>接着给出使用sample model更新value function的伪代码，注意我们一般不会只采用模型来更新(Priority Sweeping除外)，所以这个算法会在本章后续部分和Q-Learning等model-free方法一起来完成更新:</p>
<p><img src="https://wx4.sinaimg.cn/large/0070VybLly1fxxzbplpd8j30q405laan.jpg" alt="png"> </p>
<h2 id="Dyna-Integrating-Planning-Acting-and-Learning"><a href="#Dyna-Integrating-Planning-Acting-and-Learning" class="headerlink" title="Dyna: Integrating Planning, Acting, and Learning"></a>Dyna: Integrating Planning, Acting, and Learning</h2><p>这一部分提出一个重要的算法:Dyna-Q。在我们讨论这个算法之前，先来看一下model planning是如何融入之前的model-free算法的，即indirect reinforcement learning方法是如何提升value function的update的：</p>
<p><img src="https://wx2.sinaimg.cn/large/0070VybLly1fxxzgj1yuqj30a8084weq.jpg" alt="png"> </p>
<p>可以看到通过实时的经验(experience)，学习产生了model，然后通过planning也对value function进行了更新，这种方法叫做on-line planning。</p>
<p>好了，我们接着来看一下Dyna算法的通用体系：</p>
<p><img src="https://wx2.sinaimg.cn/large/0070VybLly1fxxzltwuzvj30fj0bu3z8.jpg" alt="png"> </p>
<p>右边部分阐述了model-planning的工作原理，首先需要通过实际的经验来学习得到一个model，然后剩下的和上面提到的state-space plan一致：通过model模拟experience，然后进行backup。</p>
<p>最后给出Dyna-Q的算法伪代码：</p>
<p><img src="https://wx2.sinaimg.cn/large/0070VybLly1fxxzp3ohdfj30pn0970ty.jpg" alt="png"> </p>
<p>这里有一个使用不同的model-planning step的示例：<a href="https://xinge650.github.io/2018/12/06/Chapter08-maze/" target="_blank" rel="noopener">Example 8.1 Dyna Maze</a></p>
<h2 id="When-the-Model-Is-Wrong"><a href="#When-the-Model-Is-Wrong" class="headerlink" title="When the Model Is Wrong"></a>When the Model Is Wrong</h2><p>如果我们正在学习的任务，它所对应的environment是变化的，那么我们学习到的model很有可能和environment之间是有差别，这一部分来探讨一下如果模型出错，Dyna方法的性能。</p>
<p>这里引入一种Dyna-Q的优化算法：Dyna-Q+。说优化，其实主要是Q+方法引入了一种启发式exploration：通过给sample得到的model中的state-action对添加了一个timestamp。当选择action的时候，那些current_time-timestamp值比较大的state-action说明这种state-action已经很久没有选中了，那么我们模型中关于这对state-action的动态特性很有可能已经过时了，即此时模型有可能已经和实际environment有偏差了，所以需要给这些state-action的reward附加补偿：r+k*sqrt(current_time-timestamp)。这种启发式的exploration可以有效的帮助policy的学习过程跳出局部optimal policy，同时对于变化的environment适应性更强。</p>
<p>关于Dyna-Q和Dyna-Q+在变化的environment中的性能，可以参考<a href="https://xinge650.github.io/2018/12/06/Chapter08-maze/" target="_blank" rel="noopener">Example 8.2 Blocking Maze Example 8.3 Shortcut Maze</a></p>
<h2 id="Prioritized-Sweeping"><a href="#Prioritized-Sweeping" class="headerlink" title="Prioritized Sweeping"></a>Prioritized Sweeping</h2><p>可以看到前面建立model的时候，是把episode随机产生的state-action”喂”(feed)进model，同理从model模拟环境进行planning的时候，也是随机产生state-action的。那么我们不禁思考：可不可以优先更新那些“更新效果更好”的state-action？</p>
<p>答案是肯定的。因为有的state-action可能在实际的优化路径中基本不会出现，花费资源来更新这些state-action，对其他有用的state-action或者说整体所有state-action对的更新和策略的优化基本是0收益的。所以这部分提出了一种Prioritized Sweeping的方法，维护一个优先队列，只有那些更新target足够大的state-action才会添加入队列，同时队列也是按照更新概率来排序的，即abs(target) = priority。</p>
<p>同时Prioritized Sweeping算法的value function更新是采用backward focusing方法的。即更新的时候是利用当前Q来更新前导previous-Q。这是一种启发式的思路(P137)：</p>
<p>Only an update along a transition into the state just prior to the goal, or from it, will change any values. So search might be usefully focused by working backward from goal states. Of course, we do not really want to use any methods specific to the idea of “goal state.” In general, we want to work back not just from goal states but from any state whose value has changed.</p>
<p>理解了算法的基本原理，下面给出相应的伪代码：</p>
<p><img src="https://wx2.sinaimg.cn/large/0070VybLly1fxy0u87ao2j30pp0d0tan.jpg" alt="png"> </p>
<p>注意Prioritized Sweeping算法并没有在sample阶段使用Q-Learning或者其他model-free算法来更新，原因上面也讲了，因为更新随机采样得到的state-action会引入不必要的value更新，浪费计算资源。</p>
<p>关于Prioritized Sweeping的例子<a href="https://xinge650.github.io/2018/12/06/Chapter08-maze/" target="_blank" rel="noopener">Example 8.4 Prioritized Sweeping on Mazes</a></p>
<h2 id="Expected-vs-Sample-Updates"><a href="#Expected-vs-Sample-Updates" class="headerlink" title="Expected vs Sample Updates"></a>Expected vs Sample Updates</h2><p>可以通过三个问题将目前我们已经学过的或者提到过的强化学习算法分为7类(理论上应该可以分为8=2^3类，但是有一种分类并没有对应具体的算法)：</p>
<p>(1)更新的action-value还是state-action</p>
<p>(2)估计value使用的是target-policy还是greedy policy</p>
<p>(3)更新使用的是expected还是sample update</p>
<p>对应的具体算法如下:</p>
<p><img src="https://wx4.sinaimg.cn/large/0070VybLly1fxy6ribiqnj30db0msta3.jpg" alt="png"> </p>
<p>expected update使用的是bellman equation：</p>
<p><img src="https://wx1.sinaimg.cn/large/0070VybLly1fxy6t2p8nlj30c602at8l.jpg" alt="png"> </p>
<p>sample update使用的是n-step的更新公式，以Q-Learning为例：</p>
<p><img src="https://wx2.sinaimg.cn/large/0070VybLly1fxy6ubaa7mj30dx01y3yd.jpg" alt="png"> </p>
<p>在value估计过程中，两种方法的误差性能，可以参考<a href="https://xinge650.github.io/2018/12/06/Chapter08-expectation-vs-sample/" target="_blank" rel="noopener">Figure 8.8</a></p>
<h2 id="Trajectory-Sampling"><a href="#Trajectory-Sampling" class="headerlink" title="Trajectory Sampling"></a>Trajectory Sampling</h2><p>在采样更新中，不可避免需要讨论采样方法。可以像<a href="https://xinge650.github.io/2018/11/23/Chapter04-Dynamic-Programming/" target="_blank" rel="noopener">第四章</a>的DP方法那样，从第一个state一直顺序更新到terminal state，即完整的更新整个set of state；也可以像<a href="https://xinge650.github.io/2018/12/01/Chapter06-Temporal-Difference-Learning/" target="_blank" rel="noopener">第六章</a>的Temporal Difference方法那样，以Sarsa方法为例，使用policy来选择并更新action-value。</p>
<p>后者即这部分要讲的trajectory sampling抽样算法。trajectory sampling的思路和优先扫描(Prioritized Sweeping)的一致，都是考虑到均匀更新会浪费大量的计算资源，只按照一定的策略选择更新value，属于启发式的更新策略。</p>
<p>关于均匀抽样和trajectory sampling的算法性能比较，可以参考<a href="https://xinge650.github.io/2018/12/06/Chapter08-trajectory-sampling/" target="_blank" rel="noopener">Figure 8.9</a></p>
<h2 id="Real-time-Dynamic-Programming"><a href="#Real-time-Dynamic-Programming" class="headerlink" title="Real-time Dynamic Programming"></a>Real-time Dynamic Programming</h2><p>real-time DP可以看做DP算法的trajectory sampling升级版：RTDP updates the values of states visited in actual or simulated trajectories by means of expected tabular value-iteration updates.</p>
<p>RTDP是一种典型的异步DP算法，异步DP算法在第四章提到的：</p>
<p>Asynchronous DP algorithms are not organized in terms of systematic sweeps of the state set; they update state values in any order whatsoever, using  whatever values of other states happen to be available. </p>
<p>RTDP算法适用于一类stochastic optimal path problems(随机最优路径问题)，这类问题满足以下4个性质：</p>
<p>(1)每一个goal state的初始value都是0</p>
<p>(2)there exists at least one policy that guarantees that a goal state will be reached with probability one from any start state(懂意思但是翻译不好@_@)</p>
<p>(3)所有从非goal state进行的state转换，得到的reward都是负值</p>
<p>(4)所有的state的初始value都要不小于最终的optimal value，最简单的方法就是将initial value都设置为0</p>
<p>这种问题优化目的不是为了找到最高的reward总和，而是为了得到最少的cost。优化policy，使得达到goal state的负值reward和最小，即最小化cost。</p>
<p>和传统DP相比，RTDP收敛所需的update次数更少，因为RTDP是随着value function达到最优时，选择action的policy也同时达到最优。</p>
<p>但是传统的DP方法需要等待value function更新误差小于一定值才会停止更新，其实在value function停止更新前，policy其实已经达到最优了，但是如果附加程序来检查policy是否达到最优会需要相当多的额外计算。通过这点也可以看出RTDP的收敛是快于传统DP的。</p>
<h2 id="Planning-at-Decision-Time"><a href="#Planning-at-Decision-Time" class="headerlink" title="Planning at Decision Time"></a>Planning at Decision Time</h2><p>这里提出了一种使用planning的新方法，即在给定current_state时，利用当前学习得到的model来预测并实施action，这种方法叫做planning at decision time。</p>
<p>这章前面的内容提到的算法，都是在后台运行planning算法，来提升整体的value function训练性能。关于两种算法，一般根据不同的情形有不同的用法：</p>
<p>如果对时间要求不是很高，比如棋类游戏，每一步都有几秒或者几分钟时间来思考，那么适用planning at decision time可以在这段时间向前plan几十步，选择一个合适的action，而且因为实际的state-action对很多，所以出现重复的概率比较低，相应的资源浪费也就没那么严重；</p>
<p>如果对时间要求比较高，那么最好在后台运行planning算法来优化policy，使得整体的value function得到提升。</p>
<h2 id="Heuristic-Search"><a href="#Heuristic-Search" class="headerlink" title="Heuristic Search"></a>Heuristic Search</h2><p>使用decision time planning的state-space planning方法中，最常见的就属Heuristic Search(启发式搜索)了。</p>
<p>启发式搜索可以看做一个多步的greedy policy，启发式搜索算法的工作流程：</p>
<p>每当遇到一个状态，就以该状态为根节点，所有可能的后续state和action为leaf来创建一棵树，然后使用backup的方法计算每个节点的value，并greedy的选择action。当一个状态的planning结束之后，丢弃所有的备份值。</p>
<p>启发式搜索的优点主要是由decision time带来的，使得policy可以集中资源来考虑当前状态下的决策，比如在棋类游戏中，这种方法可以从当前状态开始存储更多的未来状态情况，可以更加有效的利用内存。</p>
<h2 id="Rollout-Algorithms"><a href="#Rollout-Algorithms" class="headerlink" title="Rollout Algorithms"></a>Rollout Algorithms</h2><p>rollout algorithms是一类针对backgammon的算法，该算法是基于Monte Carlo的decision time planning算法，通过在当前state下进行Monte Carlo模拟采样来进行state的update，当然并不保留backup的值。</p>
<p>Rollout算法的目标是基于当前状态和给定的rollout-policy来进行planning，做出决策之后则抛弃使用的估计值，即计算得到的backup。Rollout算法只是为了在给定的rollout-policy基础上进行提升以及决策，并不是为了学习到optimal policy，所以Rollout算法严格来说并不能算是learning algorithms。</p>
<p>书上对Rollout Algorithms的总结比较合适：</p>
<p>We do not ordinarily think of rollout algorithms as learning algorithms because they do not maintain long-term memories of values or policies.However, these algorithms take advantage of some of the features of reinforcement learning that we have emphasized in this book. As instances of Monte Carlo control, they estimate action values by averaging the returns of a collection of sample trajectories, in this case trajectories of simulated interactions with a sample model of the environment. In this way they are like reinforcement learning algorithms in avoiding the exhaustive sweeps of dynamic programming by trajectory sampling, and in avoiding the need for distribution models by relying on sample, instead of expected, updates. Finally, rollout algorithms take advantage of the policy improvement property by acting greedily with respect to the estimated action values.(我们通常不把rollout算法看作学习算法，因为它们不保持对值或策略的长期记忆。然而，这些算法利用了我们在本书中强调的强化学习的一些特征。作为蒙特卡罗控制的实例，它们通过平均一组样本轨迹的reward来估计动作值，在这种情况下，是模拟与环境样本模型的交互的轨迹。通过这种方式，它们就像强化学习算法一样，通过trajectory sampling避免了进行动态规划的exhaustive sweeps，并且通过依赖样本而不是期望更新来避免对分布模型的需求。最后，展开算法利用策略改进特性，对估计的动作值greedy地操作。)</p>
<h2 id="Monte-Carlo-Tree-Search"><a href="#Monte-Carlo-Tree-Search" class="headerlink" title="Monte Carlo Tree Search"></a>Monte Carlo Tree Search</h2><p>Monte Carlo Tree Search是最近出现的比较成功的decision-planning算法(PS:花了4个小标题来阐述decision time planning算法，为什么不给个例子呢( _ゝ`))。</p>
<p>MCTS算法是基于Rollout算法的，但是通过累计通过Monte Carlo方法仿真得到的value estimate来更好的提升rollout-policy。</p>
<p>算法基本上分为一下四步：</p>
<p>第一步是Selection，就是在树中找到一个最好的值得探索的节点，一般策略是先选择未被探索的子节点，如果都探索过就选择UCB值最大的子节点。第二步是Expansion，就是在前面选中的子节点中走一步创建一个新的子节点，一般策略是随机自行一个操作并且这个操作不能与前面的子节点重复。第三步是Simulation，就是在前面新Expansion出来的节点开始模拟游戏，直到到达游戏结束状态，这样可以收到到这个expansion出来的节点的得分是多少。第四步是Backpropagation，就是把前面expansion出来的节点得分反馈到前面所有父节点中，更新这些节点的quality value和visit times，方便后面计算UCB值。</p>
<p>这里的decision time planning算法只是做了一个简单的介绍，大致介绍了一下基本思路，如果需要深入还需要阅读相关的论文。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Reinforcement-Learning-Note/" rel="tag"># Reinforcement Learning Note</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/06/Chapter08-trajectory-sampling/" rel="next" title="Chapter08 trajectory_sampling">
                <i class="fa fa-chevron-left"></i> Chapter08 trajectory_sampling
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/08/Chapter09-On-policy-Prediction-with-Approximation/" rel="prev" title="Chapter09 On-policy Prediction with Approximation">
                Chapter09 On-policy Prediction with Approximation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://wx2.sinaimg.cn/large/0070VybLly1fxoqylcs4uj309708rgoa.jpg" alt="xingE650">
            
              <p class="site-author-name" itemprop="name">xingE650</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">33</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Helpful Link
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://docs.scipy.org/doc/numpy/reference/index.html" title="numpy-reference" target="_blank">numpy-reference</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction" title="RL-book-code" target="_blank">RL-book-code</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Models-and-Planning"><span class="nav-number">1.</span> <span class="nav-text">Models and Planning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dyna-Integrating-Planning-Acting-and-Learning"><span class="nav-number">2.</span> <span class="nav-text">Dyna: Integrating Planning, Acting, and Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#When-the-Model-Is-Wrong"><span class="nav-number">3.</span> <span class="nav-text">When the Model Is Wrong</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Prioritized-Sweeping"><span class="nav-number">4.</span> <span class="nav-text">Prioritized Sweeping</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Expected-vs-Sample-Updates"><span class="nav-number">5.</span> <span class="nav-text">Expected vs Sample Updates</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Trajectory-Sampling"><span class="nav-number">6.</span> <span class="nav-text">Trajectory Sampling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Real-time-Dynamic-Programming"><span class="nav-number">7.</span> <span class="nav-text">Real-time Dynamic Programming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Planning-at-Decision-Time"><span class="nav-number">8.</span> <span class="nav-text">Planning at Decision Time</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Heuristic-Search"><span class="nav-number">9.</span> <span class="nav-text">Heuristic Search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Rollout-Algorithms"><span class="nav-number">10.</span> <span class="nav-text">Rollout Algorithms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Monte-Carlo-Tree-Search"><span class="nav-number">11.</span> <span class="nav-text">Monte Carlo Tree Search</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xingE650</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

  <ul class="cb-slideshow">
		<li>
		<span>1</span></li>
		<li>
		<span>2</span></li>
		<li>
		<span>3</span></li>
		<li>
		<span>4</span></li>
		<li>
		<span>5</span></li>
		<li>
		<span>6</span></li>
  </ul>

<body oncopy="alert('be helpful to you(๑•̀ㅂ•́)و✧');return true;">
</body>
</body></html>
